# -*- eval: (my/execute-startup-blocks) -*-
# 56
#+title: Mathematics for Computing
#+subtitle: Abridged Lecture Notes @@html:<br>@@ Based on â€œA Logical Approach to Discrete Mathâ€
#+author: Musa Al-hassy
#+options: tags:nil d:nil toc:t
#+PROPERTY: header-args:calccheck :tangle (concat (file-name-sans-extension (buffer-name)) ".calc") :comments nil

#+Description: Abridged Lecture Notes Based on â€œA Logical Approach to Discrete Mathâ€

# Make HTML
# (f-move (org-html-export-to-html) "~/CalcCheck/LectureNotes.html")
#+HTML_HEAD: <link href="https://alhassy.github.io/org-notes-style.css" rel="stylesheet" type="text/css" />
#+HTML_HEAD: <link href="https://alhassy.github.io/floating-toc.css" rel="stylesheet" type="text/css" />
#+HTML_HEAD: <link href="https://alhassy.github.io/blog-banner.css" rel="stylesheet" type="text/css" />
# The last one has the styling for lists.

# One of the first problems was that mathematicians hardly manipulate their formu- lae: instead, they interpret them; and one of the reasons is that the notation they use is not adequate for manipulation.

#+begin_center
#+include: ~/Desktop/colours_palette.html export html
#+end_center

* keywords                                                           :ignore:
  :PROPERTIES:
  :CUSTOM_ID: keywords
  :END:
#+begin_details Keywords

#+begin_documentation template
#+end_documentation
# doc:template

#+begin_documentation Theorem
A /theorem/ is a syntactic object, a string of symbols with a particular property.

A /theorem/ of a calculus is either an axiom or the conclusion of an inference
rule whose premises are theorems.

Different axioms could lead to the same set of theorems, and many texts use
different axioms.
#+end_documentation
doc:Theorem

#+begin_documentation Calculus
A /calculus/ is a method or process of reasoning by calculation
with symbols. A /propositional calculus/ is a method of calculating with Boolean
(or propositional) expressions.
#+end_documentation
doc:Calculus

#+begin_documentation Propositional Calculus
A /calculus/ is a method or process of reasoning by calculation
with symbols. A /propositional calculus/ is a method of calculating with Boolean
(or propositional) expressions.
#+end_documentation
doc:Propositional_Calculus

#+begin_documentation Axiomatic Semantics
Often operations are defined by how they are evaluated (â€œoperationallyâ€), we
take the alternative route of defining operations by how they can be manipulated
(â€œaxiomaticallyâ€); i.e., by what properties they satisfy.

For instance, we may define basic manipulative properties of operators ---i.e.,
/axioms/--- by considering how the operators behave operationally on particular
expressions. That is, one may use an operational, intuitive, approach to obtain
an axiomatic specification (characterisation, interface) of the desired
properties.

More concretely, since $(p â‰¡ q) â‰¡ r$ and $p â‰¡ (q â‰¡ r)$ evaluate to
the same value for any choice of values for $p, q, r$, we may insist that a part
of the definition of equivalence is that it be an associative operation.

Sometimes a single axiom is not enough to â€˜pin downâ€™ a unique operator ---i.e.,
to ensure we actually have a well-defined operation--- and other times this is
cleanly possible; e.g., given an ordering â€˜â‰¤â€™(â€˜â‡’, âŠ†, âŠ‘â€™) we can define minima
â€˜â†“â€™ (â€˜âˆ§, âˆ©, âŠ“â€™) by the axiom: â€œx â†“ y is the greatest lower boundâ€; i.e., $z â‰¤ x
â†“ y \quadâ‰¡\quad z â‰¤ x \,âˆ§\, z â‰¤ y$.
#+end_documentation
doc:Axiomatic_Semantics

#+begin_documentation Operational Semantics
Often operations are defined by how they are evaluated (â€œoperationallyâ€), we
take the alternative route of defining operations by how they can be manipulated
(â€œaxiomaticallyâ€); i.e., by what properties they satisfy.

For instance, we may define basic manipulative properties of operators ---i.e.,
/axioms/--- by considering how the operators behave operationally on particular
expressions. That is, one may use an operational, intuitive, approach to obtain
an axiomatic specification (characterisation, interface) of the desired
properties.

More concretely, since $(p â‰¡ q) â‰¡ r$ and $p â‰¡ (q â‰¡ r)$ evaluate to
the same value for any choice of values for $p, q, r$, we may insist that a part
of the definition of equivalence is that it be an associative operation.

Sometimes a single axiom is not enough to â€˜pin downâ€™ a unique operator ---i.e.,
to ensure we actually have a well-defined operation--- and other times this is
cleanly possible; e.g., given an ordering â€˜â‰¤â€™(â€˜â‡’, âŠ†, âŠ‘â€™) we can define minima
â€˜â†“â€™ (â€˜âˆ§, âˆ©, âŠ“â€™) by the axiom: â€œx â†“ y is the greatest lower boundâ€; i.e., $z â‰¤ x
â†“ y \quadâ‰¡\quad z â‰¤ x \,âˆ§\, z â‰¤ y$.
#+end_documentation
doc:Operational_Semantics

#+begin_documentation Associative
An operation _âŠ•_ is associative when it satisfies $(p âŠ• q) âŠ• r = p âŠ• (q âŠ• r)$.

Associativity allows us to be informal and insert or delete pairs of
parentheses in sequences of âŠ•'s, just as we do with sequences of
additions ---e.g., $a + b + c + d$ is equivalent to $a + (b + c) + d$.

Hence, we can write $p âŠ• q âŠ• r$ instead of $(p âŠ• q) âŠ• r$ or $p âŠ• (q âŠ• r)$.

When an operation is associative, it is best to avoid â€œmaking a choiceâ€ of how
sequences of âŠ• should be read, by using parentheses ---unless to make things
clear or explicit for manipulation.

--------------------------------------------------------------------------------

More generally, for any two operations _âŠ•_ and _âŠ_, the â€œ(left to right) mutual
associativity of âŠ• and âŠâ€ is the property $(x âŠ• y) âŠ z = x âŠ• (y âŠ z)$. It allows
us to omit parentheses in mixed sequences of âŠ• and âŠ. For instance, addition and
subtraction are (left to right) mutually associative.

#+end_documentation
doc:Associative

#+begin_documentation Identity
An operation _âŠ•_ has identity ğ‘° when it satisfies $ğ‘° âŠ• x = x = x âŠ• ğ‘°$.

If it satisfies only the first equation, $ğ‘° âŠ• x = x$, one says
that â€œğ‘° is a left-identity for âŠ•â€. If it satisfies only the second
equation, $x âŠ• ğ‘° = x$, one says that â€œğ‘° is a right-identity for âŠ•â€.

For example, implication only has a left identity, $(false â‡’ x) = x$, and
subtraction only has a right identity, $(x - 0) = x$.

An identity implies that occurrences of â€œâŠ• ğ‘°â€ and â€œğ‘° âŠ•â€ in an expression are
redundant. Thus, $x âŠ• ğ‘°$ may be replaced by $x$ in any expression without
changing the value of the expression. Therefore, we usually eliminate such
occurrences unless something encourages us to leave them in.
#+end_documentation
doc:Identity

#+begin_documentation Distributive
An operation âŠ— distributes over âŠ• when they satisfy
â€œleft-distributivityâ€ $x âŠ— (y âŠ• z) = (x âŠ— y) âŠ• (x âŠ— y)$
and
â€œright-distributivityâ€ $(y âŠ• z) âŠ— x = (y âŠ— x) âŠ• (z âŠ— x)$.

When âŠ• = âŠ—, one says that the operation is â€œself-distributiveâ€.

Distributivity can be viewed in two ways, much like distributivity of
multiplication Ã— over addition +. Replacing the left side by the right side
could be called â€œmultiplying outâ€; replacing the right side by the left side,
â€œfactoringâ€.
#+end_documentation
doc:Identity

#+begin_documentation Metatheorem
A /theorem/ in the technical sense is an expression derived
from axioms using inference rules.

A /metatheorem/ is a general *statement* about a logic that
one argues to be *true*.

For instance, â€œany two theorems are equivalentâ€ is a statement that speaks about
expressions which happen to be theorems. A logic may not have the linguistic
capability to speak of its own expressions and so the statement may not be
expressible as an expression *within* the logic ---and so cannot be a theorem of
the logic.

For instance, the logic ğ’‘ğ‘ has expressions formed from the symbols â€œğ’‘â€, â€œğ’’â€, and
â€œ-â€ (dash). It has the axiom schema $xğ’‘-ğ’’x-$ and the rule â€œIf $xğ’‘yğ’’z$ is a theorem
then so is $x-ğ’‘y-ğ’’z-$â€. Notice that $x, y, z$ are /any/ strings of dashes;
the language of this logic does not have variables and so cannot even speak
of its own expressions, let alone its own theorems!

[Informal] theorems about [technical, logic-specific] theorems are thus termed
â€˜metatheoremsâ€™.
#+end_documentation
doc:Metatheorem

#+begin_documentation Programming
Programming is solving the equation /R â‡’[C] G/ in the unknown /C/; i.e., it is the
activity of finding a â€˜recipeâ€™ that satisfies a given specification. Sometimes
we may write /R â‡’[?] G/ and solve for â€˜?â€™. Programming is a goal-directed activity: From a specification, a program is found by examining the shape of its postcondition.
#+end_documentation
doc:Programming

#+begin_documentation Specification
 A specification is an equation of a certain shape.
 /Programming/ is the activity of solving a specification
 for its unknown. Its unknown is called a /program/.

 See also â€œProgrammingâ€.
#+end_documentation
doc:Specification

#+begin_documentation Expression

A â€˜variableâ€™ or a â€˜function applicationâ€™; i.e., the name of a function along
with a number of existing expressions.

In a sense, an expression is like a sentence with the variables acting as
pronouns and the function applications acting as verb clauses and the argument
to the application are the participants in the action of the verbal clause.
#+end_documentation
doc:Expression

#+begin_documentation Algorithmic Problem Solving
There are two ways to read this phrase.

Algorithmic-problem solving is about solving problems that
involve the construction of an algorithm for their solution.

Algorithmic problem-solving is about problem solving in general,
using the principles of correct-by-construction algorithm-design.

#+end_documentation
doc:Algorithmic_Problem_Solving
# Computing science is all about solving algorithmic problems (or, as some authors pre- fer to say, it is all about instructing computers to solve problems).

#+begin_documentation Calculational Proof
A story whose events have smooth transitions connecting them.

A proof wherein each step is connected to the next step by an explicit
justification.
#+end_documentation
doc:Calculational_Proof

â€¦I'll add more in timeâ€¦
#+end_details
* Notational Setup                                                   :ignore:
  :PROPERTIES:
  :CUSTOM_ID: Notational-Setup
  :END:
:Calc_notation:
\begin{calc}
x
\step[op]{ hint }
y
\end{calc}

:End:

#+name: startup-code
#+begin_src emacs-lisp :exports none
(require 'cl)
  (load-file "~/Desktop/power-blocks.el")

#+end_src

#+BEGIN_export html
<style>

/* Using source blocks â€œmathâ€ as aliaas for haskell */
pre.src-math:before { content: 'Mathematical! Algebraic! Axiomatic!'; }
/* Execute this for alias: (add-to-list 'org-src-lang-modes '("math" . haskell)) */

</style>
#+END_export

# The following snippet let's us export calc clauses in HTML nicely.
#+begin_latex-definitions
\def\BEGINstep{ \left\langle }
\def\ENDstep{ \right\rangle }
\newcommand{\step}[2][=]{ \\ #1 \;\; & \qquad \color{maroon}{\BEGINstep\text{ #2
} \ENDstep} \\ & }

% multi-line step with many lines of text
\newcommand{\line}[1]{ \text{#1}\hfill\\ }
\newcommand{\stepmany}[2][=]{ \\ #1 \;\; & \qquad \color{maroon}{\BEGINstep \large\substack{ #2 } \ENDstep} \\ & }

% multi-line step with 4 lines of text
\newcommand{\stepfour}[5][=]{ \stepmany[#1]{\line{#2} \line{#3} \line{#4}
\line{#5}} }

\newenvironment{calc}{\begin{align*} & }{\end{align*}}

% Inference rules
\def\And{\quad}
\newcommand\Rule[3][]{ {#2 \over #3}\mathsf{#1} }

\def\eq{\,=\,}

\def\true{\mathsf{true}}
\def\false{\mathsf{false}}

\def\even{\mathsf{even}\,}
#+end_latex-definitions

#+html: <p style="display:none">
$$\newcommand\exp[1]{\mathsf{exp}_{#1}\,}
%
% for calc environment
% line breaks with extra whitespace using phantom formula
% \hdashline
\def\NL{\\ \phantom{Î£} \\}
\def\NLtwo{\\ \phantom{\substack{Î£ \\ Î£}} \\}
$$
#+html: </p>


# This snippet let's us, in an org file, do C-c C-x C-l to see the calculation
# rendered prettily. It will not work if you do #+begin_calc â€¦ #+end_calc.
#+begin_src emacs-lisp :exports none
(add-to-list 'org-latex-packages-alist
  '("fleqn, leqno, block" "calculation" t))

(setq org-format-latex-header
      (concat org-format-latex-header
              "\\usepackage{color}
               \\def\\BEGINstep{ \\langle }
               \\def\\ENDstep{ \\rangle }
               \\newcommand{\\step}[2][=]{ \\\\ #1 \\;\\; & \\qquad \\color{maroon}{\\BEGINstep \\text{ #2 } \\ENDstep} \\\\ & }
               \\newenvironment{calc}{\\begin{align*} & }{\\end{align*}}"))
#+end_src

:hide:
 \begin{calc}
  x
\step{nice}
  y
\end{calc}
:end:

# $1 colour eg â€œpinkâ€ or â€œhsl(157 75% 20%)â€ or â€œ#e5f5e5â€; $2 title
#+macro: begin-box @@html: <div style="padding: 1em; background-color: $1; border-radius: 15px; font-size: 0.9em; box-shadow: 0.05em 0.1em 5px 0.01em  #00000057;"> <h3>$2</h3>@@

#+macro: end-box @@html: </div>@@

:Hide:
  {{{begin-box(teal, Salam!)}}}

  {{{end-box}}}
:End:


   #+begin_latex-definitions
\newcommand{Law}[3][]{ #1\;\;\textbf{#2:}\quad #3 }
   #+end_latex-definitions

* subtle colours :ignore:
  :PROPERTIES:
  :CUSTOM_ID: subtle-colours
  :END:

#+name: startup-code
#+begin_src emacs-lisp  :exports none
(defun subtle-colors (c)
  "Names are very rough approximates.

   Translations from: https://www.december.com/html/spec/softhues.html"
  (pcase c
    ("teal"    "#99FFCC") ;; close to aqua
    ("brown"   "#CCCC99") ;; close to moss
    ("gray"    "#CCCCCC")
    ("purple"  "#CCCCFF")
    ("lime"    "#CCFF99") ;; brighter than â€˜greenâ€™
    ("green"   "#CCFFCC")
    ("blue"    "#CCFFFF")
    ("orange"  "#FFCC99")
    ("peach"   "#FFCCCC")
    ("pink"    "#FFCCFF")
    ("yellow"  "#FFFF99")
    ("custard" "#FFFFCC") ;; paler than â€˜yellowâ€™
    (c c)
  ))
#+end_src

#+RESULTS: startup-code
: subtle-colors

# $1 colour eg â€œpinkâ€ or â€œhsl(157 75% 20%)â€ or â€œ#e5f5e5â€; $2 title
#+macro: begin-box (eval (concat "@@html: <div style=\"padding: 1em; background-color: " (subtle-colors $1) "; border-radius: 15px; font-size: 0.9em; box-shadow: 0.05em 0.1em 5px 0.01em  #00000057;\"> <h3>" $2 "</h3>@@"))

#+macro: end-box @@html: </div>@@

:Hide:
â€œSubtle coloursâ€
#+begin_parallelNB

   {{{begin-box(teal,    This is â€œtealâ€!)}}} {{{end-box}}} \\
   {{{begin-box(brown,   This is â€œbrownâ€!)}}} {{{end-box}}} \\
   {{{begin-box(gray,    This is â€œgrayâ€!)}}} {{{end-box}}} \\
   {{{begin-box(purple,  This is â€œpurpleâ€!)}}} {{{end-box}}} \\
   {{{begin-box(lime,    This is â€œlimeâ€!)}}} {{{end-box}}} \\
   {{{begin-box(green,   This is â€œgreenâ€!)}}} {{{end-box}}} \\
   {{{begin-box(blue,    This is â€œblueâ€!)}}} {{{end-box}}} \\
   {{{begin-box(orange,  This is â€œorangeâ€!)}}} {{{end-box}}} \\
   {{{begin-box(peach,   This is â€œpeachâ€!)}}} {{{end-box}}} \\
   {{{begin-box(pink,    This is â€œpinkâ€!)}}} {{{end-box}}} \\
   {{{begin-box(yellow,  This is â€œyellowâ€!)}}} {{{end-box}}} \\
   {{{begin-box(custard, This is â€œcustardâ€!)}}} {{{end-box}}} \\

#+end_parallelNB
:End:
* Introduction to Calculational Reasoning
  :PROPERTIES:
  :CUSTOM_ID: Introduction-to-Calculational-Reasoning
  :END:
** What are Calculational Proofs
   :PROPERTIES:
   :CUSTOM_ID:
   :END:

 We advocate *calculational proofs* in which reasoning is goal directed and
 justified by simple axiomatic laws that can be checked syntactically rather
 than semantically. ---/Program Construction/ by Roland Backhouse

 For example, below are two arguments showing that $\sqrt[n]{k}$ is a rational
 number precisely when /integer/ $k$ is a so-called perfect /n/-th root ---consequently,
 since 2 is not a perfect square, $\sqrt{2}$ is not rational.

{{{begin-box(#e5f5e5, Conventional Proof: Non-perfect powers have irrational
roots)}}}

Suppose that /integer/ $k$ is not a perfect /n/-th power ---i.e., not of the form /ğ“â¿/---
then there is some prime $p$ in the factorisation of $k$ that has its exponent,
say $\exp{p} k$, being not a multiple of $n$.  But if $\sqrt[n]{k} \eq
a/b$ then $\exp{p}{k} \eq \exp{p}{a^n} - \exp{p}{b ^ n} \eq n Â· \exp{p}{a} - n Â·
\exp{p}{b}$ and the difference of multiples of $n$ is a multiple of $n$, and so
we have a contradiction. Hence, no such $a, b$ could exist and so $\sqrt[n]{k}$
is irrational.
{{{end-box}}}

This is an example of an *informal proof*, which is a mixture of natural language,
English, and mathematical calculations.  The English text outline the main steps
of the proof, and the mathematical calculations fill in *some* of the details.

Since they only communicate the key ideas, such proofs are preferred by writers
but they place a large semantic burden on readers who are expected to have such
a good understanding of the problem domain that the details of the outline
proofs can be filled in, and so the writer leaves these as an implicit exercise
to the reader.

However, even worse, such informal outline proofs may skip over important
details and thus can be wrong!

Below is a *calculational proof*. It introduces notation and recalls theorems as
needed, thereby making each step of the argument easy to verify and follow.  As
such, the following argument is more accessible to readers unfamiliar with the
problem domain.

{{{begin-box(#e5f5e5, Calculational Proof)}}}

 \begin{calc}
 \def\BEGINstep{\left[} \def\ENDstep{\right.}
 \sqrt[n]{k} \text{ is a rational number }
 \stepfour{ A rational number is the fraction of two integers.}{
          Let variables $a,\, b$ range over integer numbers.}{}{
  }
  âˆƒ\, a, b â€¢\; \sqrt[n]{k} = {a \over b}
 \step{ Use arithmetic to eliminate the $n$-th root operator.
  }
  âˆƒ\, a, b â€¢\; k Â· a ^n = b ^n
  \stepmany{ \line{Let $\exp{m} x$ be the number of times that $m$ divides $x$.}
   \line{For example, $\exp{2} 48 \eq 4$ and $\exp{2} 49 \eq 0$.}
   \NL
   \line{The numbers $p$ with $âˆ€ m : â„¤âº \,â€¢\, \exp{m}p \,â‰ \, 0 \,â‰¡\, m \,=\, p$ are called $prime$ numbers.}
   \line{Let variable $p$ ranges over prime numbers. }
   \NL
   \line{Fundamental theorem of arithmetic: Numbers are determined by their prime powers.}
   \line{That is, $\big(âˆ€ \,p\, â€¢\; \exp{p} x \eq f(p)\big) \;â‰¡\; x \,=\, \big(Î \, p\, â€¢\; p^{f(p)}\big)$ for any $f$.}
   \line{As such, every number is the product of its prime powers:}
   \line{$\qquad x \eq \big(Î  \,p\, â€¢\; p^{\exp{p} x}\big)$. }
   \line{And so, any two numbers are the same precisely when they have the same primes:}
   \line{$\qquad x \eq y \;â‰¡\; \big(âˆ€ p \,â€¢\, \exp{p} x \eq \exp{p} y\big)$.}
  }
  âˆƒ\, a, b â€¢\; âˆ€\, p â€¢\; \exp{p}(k Â· a ^n) \eq \exp{p}(b ^n )
  \stepmany{\line{When $p$ is prime, $\exp{p}(x Â· y) \eq \exp{p} x \,+\, \exp{p} y$.}
   \line{Aside: In general, $\exp{p}(Î  \,i\, \,â€¢\, x_i) \eq (Î£ \,i\, \,â€¢\, \exp{p} x_i)$.}
  }
  âˆƒ\, a, b â€¢\; âˆ€\, p â€¢\; \exp{p} k + n Â· \exp{p} a \eq n Â· \exp{p} b
  \step{ Use arithmetic to collect similar terms.
  }
  âˆƒ\, a, b â€¢\; âˆ€\, p â€¢\; \exp{p} k \eq  n Â· \Big(\exp{p} b - \exp{p} a\Big)
  \stepmany{ \line{(â‡’) is the definition of multiplicity;}
             \line{(â‡) take $a \,â‰”\, 1$ and define $b$ by its prime powers:}
             \line{ $\qquad âˆ€\, p \,â€¢\, \exp{p} b \,â‰”\, {\exp{p} k \,/\, n}$}
  }
  âˆ€\, p â€¢\; \exp{p} k \text{ is a multiple of } n
  \step{ Fundamental theorem of arithmetic and definition of â€˜perfectâ€™ }
  k \text{ is a perfect $n$-th power; i.e., of the shape } x^n
\end{calc}

{{{end-box}}}

# Go back to the âŸ¨hint notationâŸ©.
#+begin_latex-definitions
\def\BEGINstep{ \left\langle }
\def\ENDstep{ \right\rangle }
#+end_latex-definitions

Observe that the calculational form is *more general*.  The use of a
/formal/ approach let us keep track of when our statements are equivalent (â€œ=â€)
rather than being weakened (â€œâ‡’â€).  That is, the use of English to express the
connection between steps is usually presented naturally using â€œif this, then
thatâ€ statements ---i.e., implication--- rather than stronger notion of equality.
- In contrast, the conventional proof is a â€˜proof by contradictionâ€™;
  a method that is over-used.

- Other features of conventional proofs are the /dot dot dot/ notations, â€œâ‹¯â€, to
  indicate â€œand so on, of the same idea/formâ€ ---leaving readers the burden to
  guess the generic shape of the â€˜idea/formâ€™ that should be repeated.

  Calculational proofs use quantifiers ---and loops--- instead.
- Finally, conventional proofs tend to use prefix notation and thereby
  implicitly forcing a syntactic distinction between equivalent expressions;
  e.g., $\gcd(m, \gcd(n, p))$ and $\gcd(\gcd(m, n), p)$.

The above proof is a generalisation of a proof in Backhouse's text for square
roots, which may be viewed as a [[https://youtu.be/t39wHoFHbvY][Youtube video]] which makes use of kbd:CalcCheck
[[https://alhassy.github.io/CalcCheck/Docs][â‡­]]: A proof checker for the logic of â€œA Logical Approach to Discrete Mathâ€
(â€˜LADMâ€™).
    - It /checks/ your arguments in a notation similar to that of the book.
    - *You can check your work before handing it in.*
    - You can formalise your own theorems from other books and check them
      ---unlimited exercises!

    #+begin_center
    kbd:Control_+_Alt_+_Enter to check a cell.
    #+end_center

     Going forward, instead of defining expressions by how they are evaluated,
     we define expressions in terms of how they can be manipulated.
     # operational versues aximatic method.

     A *<<<calculus>>>* is a method or process of reasoning by calculation with
     symbols.
     A Boolean variable that can denote a proposition is sometimes called a
     /propositional variable/.
     A *<<<propositional calculus>>>* is so named beacuse it is a method
     of calculating with expressions that involve propositional variables.

  {{{begin-box(pink, The propositional calculus of LADM is called â€œequational
  logic ğ‘¬â€)}}}
     One part of ğ‘¬ is a set of /axioms/, which are certain Boolean expressions
     that define basic manipulative properties of Boolean operators.
     For example, the axiom $p âˆ¨ q â‰¡ q âˆ¨ p$ indicates (semantically)
     that the value of a disjunction doesn't depend on the order of its arguments
     and (syntactically) we may swap their order when manipulating expressions.
     The other part of this calculus are the 3 inference rules Substitution,
     Leibniz, and Transitivity.

     A *<<<theorem>>>* of this calculus is either an axiom, the conclusion
     of an inference rule whose premises are theorems, or a Boolean expression
     that, using the inference rules, is proved equal to an axiom or a previously
     proved theorem.
  {{{end-box}}}

#+begin_box Algorithmic Problem Solving ---â€œmath is programmingâ€
Problems may be formulated and solved using, possibly implicitly, the
construction of correct programs:

|   | â€œfor all $x$ satisfying $R(x)$, there is a $y$ such that $G(x, y)$ is trueâ€ |
| â‰ˆ | /âˆ€ x â€¢ R x â‡’ âˆƒ y â€¢ G x y/                                                     |
| â‰ˆ | ~R â‡’[C] G~ for some program command ~C~ with inputs /x/ and outputs /y/             |

This is known as a /constructive proof/ since we have an algorithm ~C~ that actually
shows how to find a particular $y$ to solve the problem, for any given $x$.  In
contrast, non-constructive proofs usually involving some form of counting
followed by a phrase â€œthere is at least one such $y$ â€¦â€, without actually
indicating /how/ to find it!
# p âˆ¨ Â¬ p ...!

More concretely,
|   | Any two consectuive Fibonnaci numbers are coprime    |
| â‰ˆ | /âˆ€ n â€¢ n â‰¥ 1 â‡’ gcd(fib n, fib (n + 1)) = 1/            |
| â‰ˆ | ~a = fib n  âˆ§  b = fib (n + 1)  âˆ§  n â‰¥ 0~              |
|   | ~Â Â Â Â Â Â Â Â Â Â Â Â â‡’[C]~                                     |
|   | ~a = b = gcd(fib n, fib (n + 1)) = 1~, for a program ~C~ |
#+end_box

** What is Discrete Mathematics
   :PROPERTIES:
   :CUSTOM_ID: Discrete-Mathematics
   :END:
 1. *Discrete Mathematics*
    includes logic (calculational reasoning), (data) sets, functions, relations,
    graphs, inductive types, and more.

    Conscious and fluent use of the language of (discrete) mathematics
    is the foundation for precise specification and rigorous reasoning
    in Computer Science and Software Engineering

 2. *Goal*: Understand the mechanics of mathematical expressions and proof.

 3. <<<Propositional>>>: Statements that can be either /true/ or /false/; not numbers.

    <<<Predicate>>>: Propositional statement about some subjects.

 4. <<<Calculus>>>: Formalised reasoning through calculation.

    â€˜Hand wavyâ€™ English arguments tend to favour /case analysis/
       ---considering what could happen in each possible scenario---
       which increases exponentially with each variable; in contrast,
       equality-based calculation is much simpler since it delegates
       intricate case analysis into codifed algebraic laws.

       E.g., Portia's Suitor's Dilemma has 4 unknowns, each being either true or false,
       and so has $2^4$ many possible scenarios to consider. Whereas a
       calculation solving the problem can be formed in less than 10 super simple
       lines.

       #+begin_details Portia's Suitor's Dilemma

 Portia has a gold casket and a silver casket and has placed a picture of herself
 in one of them. On the caskets, she has written the following inscriptions:

 + Gold ::  The portrait is not in here
 + Silver :: Exactly one of these inscriptions is true.

 Portia explains to her suitor that each inscription may be true or false, but
 that she has placed her portrait in one of the caskets in a manner that is
 consistent with the truth or falsity of the inscriptions.

 If the suitor can choose the casket with her portrait, she will marry him.

 -----

 ( This is a â€˜teaserâ€™; you're not expected to know the details in the following
 calculation. )

 Formalisation is the first step towards solution!

 #+begin_src calccheck
Declaration: G, S : ğ”¹

Explanation: G â‰” â€œThe inscription on the gold casket is trueâ€
Explanation: S â‰” â€œThe inscription on the silver casket is trueâ€
 #+end_src

 â€¦ and
 #+begin_src calccheck
Declaration: gc : ğ”¹
Explanation: gc â‰” â€œThe portrait is in the gold casketâ€
 #+end_src

â€¦ we know the portrait is in a casket preciely when
that casket's inscription is true â€¦

 #+begin_src calccheck
Axiom â€œInscription on gold casketâ€: G â‰¡ Â¬ gc
Axiom â€œInscription on silver casketâ€: S â‰¡ (S â‰¡ Â¬ G)
 #+end_src

 â€¦ let us start from what we know about the silver casket:
 #+begin_src calccheck
Calculation:
    S â‰¡ (S â‰¡ Â¬ G)    â€” This is â€œInscription on silver casketâ€
  â‰¡âŸ¨ â€œReflexivity of â‰¡â€ âŸ©
    S â‰¡ S â‰¡ Â¬ G
  â‰¡âŸ¨ â€œSymmetry of â‰¡â€ âŸ©
    Â¬ G
  â‰¡âŸ¨ â€œInscription on gold casketâ€ âŸ©
    Â¬ Â¬ gc
  â‰¡âŸ¨ â€œDouble negationâ€ âŸ©
    gc
 #+end_src

 By just *simplifying*, we calculated that the portrait is in the gold casket!

 # See below [[#shape-of-calculations][The Shape of Calculations]] for more on /exploratory calculations/.

 #+end_details

 #+begin_quote
/Knowledge is software for your brain: The more you know, the more problems you
can solve!/
 #+end_quote

# Time for an upgrade!

** Road-map
   :PROPERTIES:
   :CUSTOM_ID: Road-map
   :END:

 In the previous section, we showed how a calculational argument is more structured
 and may be more accessible. Before getting to *using* such a style, we first pause
 to discuss the *foundations* that legitimatise it as a tool of reasoning.

 In general, proofs are evidence of truth of a claim; by demonstrating that the
 claim follows from some /obvious truth/ using rules of reasoning that /obviously
 preserve truth/. Here are some examples of /clearly obviously true things/.

 | Axiom       | â€œself-evident (obvious) truthâ€                |
 |-------------+-----------------------------------------------|
 | Reflexivity | $X = X$ ---Everything is the same as itself   |
 | Symmetry    | $X = Y$ precisely when $Y = X$ ---Sameness is mutual  |

 #+caption: An inference rule is a syntactic mechansim for deriving â€œtruthsâ€ or â€œtheoremsâ€.
 | Infernece Rule | â€œa reasonable way to derive truthsâ€                                                            |
 |----------------+------------------------------------------------------------------------------------------------|
 | Substitution   | If $E(\vec x)$ is true, then so is $E(\vec F)$ ---where $E(\vec R)$ means $E[\vec x â‰” \vec R]$   |
 |                | E.g., Since $x + y = y + 3$ is true, so is $b + 3 = 3 + b$ ---using $x, y â‰” b, 3$              |
 |----------------+------------------------------------------------------------------------------------------------|
 | Transitivity   | If $X = Y$ and $Y = Z$ then $X = Z$                                                            |
 |                | E.g., since $e^{i Â· Ï€} = -1$ and $-1 = iÂ²$, we must have $e^{i Â· Ï€} = iÂ²$.                     |
 |----------------+------------------------------------------------------------------------------------------------|
 | Leibniz        | If $X = Y$ then $E(X) = E(Y)$ ---â€œsubstituting equals for equalsâ€                              |
 |                | E.g., since $n = 2 Â· m$ we must have $\even n = \even (2 Â· m)$                                 |
 |                | E.g., if /Jim = James/ then /Jim's home address = James' home address/.                            |
 |                |                                                                                                |


 #+begin_details Uses of inference rules ---for Logic ğ‘¬

| /                 | <                                             |
| Inference Rule    | Usage                                         |
|-------------------+-----------------------------------------------|
| Leibniz           | We can apply equalities inside expressions    |
| Transitivity of = | We can chain equalities                       |
| Substitution      | We can use substitution instances of theorems |
| Equipollence      | Things equal to theorems are also theorems    |

Equipollence means if we show something is equal to â€˜trueâ€™ (a particular
theorem), then it is a theorem. Consequently, this means all theorems are
equivalent.
#+end_details

 That's a lot of hand-waving; and a few examples don't scale. In order to discuss
 proof, we need to discuss inference rules, which are ways to derive new claims
 from old claims, and so we need to discuss how claims ---expressions or
 formulae--- are written. So let's start at expressions.

   {{{begin-box(teal, Super terse definition ---to be explained in subsequent
    sections)}}}
    A /logic/ is a set of /symbols/ along with a set of /formulas/ formed from the
    symbols, and a set of /infernece rules/ which allow formulas to be derived
    from other formulas. (The formulas may or may not include a notion of variable.)
    Logics are purely syntactic objects.

    # | Syntax    | Proof theory |
    # | Semantics | Model theory |
   {{{end-box}}}

* Expressions
  :PROPERTIES:
  :CUSTOM_ID: hi
  :END:

** Precedence
   :PROPERTIES:
   :CUSTOM_ID: Precedence
   :END:
# Dot guide
# https://www.graphviz.org/pdf/dotguide.pdf

#+begin_center
How do you â€˜readâ€™ (/parse/) the expression $6 - x + 7$?
#+end_center

#+BEGIN_SRC dot :file images/6-x+7_third_time.png :exports results
digraph structs {
 main [shape=plaintext, label="6 - x + 7"];
 main -> parse1 [style = dashed, label = "means"];
 main -> or [style = invis];
 main -> parse2 [style = dashed, label = "means"];

 parse1 [shape=record,label="+ |{{-|{6|x}}| 7}"];
 or[shape=plaintext];
 parse2 [shape=record,label="- |{6 | {+|{x|7}}}"];

 "???"[shape=plaintext];
  or  -> "???" [style = invis];

}
#     5: struct3 [shape=record,label="hello\nworld |{ b |{c|<here> d|e}| f}| g | h"];
#+END_SRC

#+RESULTS:
[[file:images/6-x+7_third_time.png]]


It can be generated from its parts in two different ways:
1. Both $6$ and $x + 7$ are expressions, so $6 - x + 7$ is an expression.
   #+BEGIN_SRC dot :file images/6-x+7_parse2.png :exports results
   digraph structs {
    "-" -> 6;
    "-" -> "+";
    "+" -> x;
    "+" -> 7;
   }
   #+END_SRC

   #+RESULTS:
   [[file:images/6-x+7_parse2.png]]

2. and also both $6 - x$ and $7$ are expressions, so $6 - x + 7$ is an expression.
   #+BEGIN_SRC dot :file images/6-x+7_parse1.png :exports results :results replace
digraph {
 "-" -> 6;
 "-" -> x;
 "+" -> 7;
 "+" -> "-";
}
#+END_SRC

A *convention* on how a /string/ should be parsed
as a /tree/ is known as a *precedence rule*.

** Grammars
   :PROPERTIES:
   :CUSTOM_ID: Grammars
   :END:

    Expressions are defined by the following /grammar/, but /in practice/ one does
    not write $+(1, 2)$ and instead writes $1 + 2$.  However, the phrase $+(1,
    Â·(2, 3))$ is /unambiguous/, whereas the phrase $1 + 2 Â· 3$ /could be read/ as
    $(1 + 2) Â· 3$ or as $1 + (2 Â· 3)$.

    #+begin_quote
    The grammar defines expressions as *abstract syntax (trees)* whereas strings
    with mixfix notation gives a *concrete syntax* where ambiguity is resolved by
    parentheses, precedence, or association rules.
    #+end_quote
    # Parentheses, precedences, and association rules only serve to disambiguate
    # the encoding of trees in strings.

    #+begin_src math
Expr ::= Constant    -- E.g., 1 or â€œappleâ€
      |  Variable    -- E.g., x or apple (no quotes!)
      |  Application -- E.g., f(xâ‚, xâ‚‚, â€¦, xâ‚™)
    #+end_src

    ( One reads =:== as /becomes/ and so the addition of an extra
    colon results in a â€˜stutterâ€™: One reads
     =::== as /be-becomes/. The symbol =|= is read /or/. )

  {{{begin-box(teal)}}}
Notice that a /constant/ is really just an /application/ with $n = 0$ arguments
and so the first line in the definition above could be omitted.
  {{{end-box}}}

** Textual Substitution ---i.e., [[https://en.wikipedia.org/wiki/Grafting][â€œgrafting treesâ€]]
   :PROPERTIES:
   :CUSTOM_ID: Textual-Substitution-i-e-https-en-wikipedia-org-wiki-Grafting-grafting-trees
   :END:

  The *(simultaneous textual) Substitution operation* $E[\vec x â‰” \vec F]$
  replaces all variables $\vec x$ with parenthesised expressions $\vec F$ in an
  expression $E$. In particular, $E[x â‰” F]$ is just $E$ but with all
  occurrences of $x$ replaced by $â€œ(F)â€$. This is the â€œfind-and-replaceâ€ utility
  you use on your computers.

 {{{begin-box(lime)}}}
  Textual substitution on expressions is known as â€œgraftingâ€ on trees: Evaluate
  $E[x â‰” F]$ by going down the tree $E$ and finding all the â€˜leavesâ€™ labelled
  $x$, cut them out and replace them with the new trees $F$.
 {{{end-box}}}

 {{{begin-box(teal)}}}
  Using the informal English definition of substitution, one quickly notices
  $E[x â‰” x] = E$ and /provided/ $y$ does not occur in $E$:
  $E[x â‰” y][y â‰” x] = E = E[y â‰” F]$.
 {{{end-box}}}

  Since expressions are either variables of functions applications,
  substitution can be defined by the following two clauses ---we will get to
  recursion and induction more formally later on.
  \begin{align*}
     y[x â‰” F]              &=  \mathsf{if}\, x = y \,\mathsf{then}\, F \,\mathsf{else}\, y \,\mathsf{fi}\,
  \\ f(tâ‚, â€¦, tâ‚™)[x â‰” F]  &=  f(tâ‚â€², â€¦, tâ‚™â€²) \; \text{ where } táµ¢â€² = táµ¢[x â‰” F]
  \end{align*}

  {{{begin-box(teal, Sequential â‰  Simultaneous)}}}
  \[
  (x + 2 Â· y)[x â‰” y][y â‰” x]  \quadâ‰ \quad  (x + 2 Â· y)[x, y â‰” y, x]
  \]
  {{{end-box}}}

  [[https://alhassy.github.io/PythonCheatSheet/CheatSheet.pdf][Python]], for example, has simultaneous /assignment/; e.g., ~x, y = y, x~ is
  used to swap the value of two variables.

  Within CalcCheck, to simplify and actually perform the substitution, one uses
  the hint kbd:Substitution; e.g.,
  #+begin_src calccheck
  (x + 2 Â· y)[x, y â‰” 3 Â· y, x + 5]
=âŸ¨ Substitution âŸ©
   3 Â· y + 2 Â· (x + 5)
  #+end_src

** â€œMeta-ğ’³â€: Speaking about the concept of ğ’³ using the notions of ğ’³
   :PROPERTIES:
   :CUSTOM_ID: Meta-ğ’³-Speaking-about-the-concept-of-ğ’³-using-the-notions-of-ğ’³
   :END:

    When we write phrases like =â€œLet E be an expressionâ€=, then the /name/ $E$
    varies and so is a variable, but it is an expression and so may consist of a
    function application or a variable. *That is, $E$ is a variable that may
    stand for variables.* This layered inception is resolved by referring to $E$
    as not just any normal variable, but instead as a *meta-variable*: A variable
    capable of referring to other (simpler) variables.

    Aside: A *variable of type Ï„* is a /name/ denoting a yet unknown /value/ of type Ï„;
    i.e., â€œit is a pronoun (nickname) referring to a person in the collection of people Ï„â€.
    E.g., to say $x$ is an integer variable means that we may treat it
    as if it were a number whose precise value is unknown.
    Then, if we let =Expr Ï„= refer to the expressions denoting /values/ of type Ï„;
    then a *meta-variable* is simply a normal variable of type =Expr Ï„=.

    Likewise, a *theorem* is a Boolean expression that is proved equal to an axiom;
    whereas a *meta-theorem* is a general statement about our logic that we prove
    to be true. That is, if ğ‘¬ is collection of rules that allows us to find
    truths, then a /theorem/ is a truth found using those rules; whereas a
    /meta-theorem/ is property of ğ‘¬ itself, such as what theorems it can have.
    That is, theorems are _in_ ğ‘¬ and meta-theorems are _about_ ğ‘¬.  For example, here
    is a meta-theorem that the equational logic ğ‘¬ has (as do many other theories,
    such as lattices): An /equational/ theorem is true precisely when its â€˜dualâ€™ is
    true. Such metatheorems can be helpful to discover new theorems.
    # A meta-theorem is a theorem about theorems.
    #
    # E.g., p âˆ§ q â‡’ q is not an equation, but it is equivalent to the equation
    # p âˆ§ q â‡’ p â‰¡ true, whose dual is p âˆ¨ q â‡ q â‰¡ false; i.e.,
    # p âˆ¨ q â‡ q.

    #+caption: Being self-reflective using â€œmetaâ€ (Greek for â€˜beyondâ€™)
    | meta-ğ’³           | â€œthe study of ğ’³â€ or â€œğ’³ about ğ’³â€ or â€œbeyond ğ’³â€         |
    |------------------+-------------------------------------------------------|
    | meta-joke        | a joke about jokes                                    |
    | meta-data        | data about data; e.g., publication date               |
    | meta-fiction     | a fictional story that acknowledges itself as fiction |
    | meta-game        | a game in which mini-games happen; e.g., Mario Party  |
    | meta-cognition   | thinking about thinking                               |
    | meta-ethics      | what is the ethical way to study ethics               |
    | meta-physics     | the study of that which is beyond the physical        |
    | meta-mathematics | studying systems of reasoning; aka â€˜proof theoryâ€™     |

* Logics
  :PROPERTIES:
  :CUSTOM_ID: Logics
  :END:

  #+begin_quote
A modern mathematical proof is not very different from a modern machine, or a
modern test setup: the simple fundamental principles are hidden and almost
invisible under a mass of technical details. â€” Hermann Weyl
  #+end_quote

** Syntax vs. Semantics
   :PROPERTIES:
   :CUSTOM_ID: Syntax-vs-Semantics
   :END:

   *Syntax* refers to the structure of expressions, or the rules for putting
     symbols together to form an expression. *Semantics* refers to the meaning
     of expressions or how they are evaluated.

   An expression can contain variables, and evaluating such an expression
   requires knowing what values to use for these variables; i.e., a *state*:
   A list of variables with associated values. E.g., evaluation of $x - y + 2$ in
   the state consisting of $(x, 5)$ and $(y, 6)$ is performed by replacing $x$
   and $y$  by their values to yield $5 - 6 + 2$ and then evaluating that to
   yield $1$.

   A Boolean expression $P$ is *<<<satisfied>>>* in a state if its value is /true/
   in that state; $P$ is *<<<satisfiable>>>* if there is a state in which it is
   satisfied; and $P$ is *<<<valid>>>* (or is a *<<<tautology>>>*) if it is
   satisfied in every state.

 --------------------------------------------------------------------------------

     All theorems of the propositional calculus ğ‘¬ are valid. This can be checked by checking
     that each axiom with a truth table and arguing for each inference rule that
     if its premises are valid then so is its conclusion.

     For example, let's show that the Substitution rule preserves validity.  Let
     us write $s(E)$ to denote the value of expression $E$ in state $s$.  If $E$
     is valid, then it is true in any state, let's argue that $E[x â‰” F]$ is also
     true in any state. So, given a state $s$, let $sâ€²$ be the â€˜updatedâ€™ state
     that assigns the same values to all the variables as does $s$ /except/ that
     the variable $x$ is assigned the value $s(F)$.  Then, since $E$ is valid,
     $sâ€²(E)$ is true but $sâ€²(E)$ is just $s\big(E[x â‰” F]\big)$ and so the
     resulting substitution is also valid.

     In programming, if we want the /assignment/ $x â‰” F$ to ensure a property $R$
     holds, then we need $R[x â‰” F]$ to hold /before/ the assignment.
     That is, if the state $s$ of our program variables satisfies $R[x â‰” F]$
     then the updated state $sâ€²$ ---having /sâ€²(x) = s(F)/--- will satisfy $R$.

     Not only are all theorems valid, but all valid expressions are theorems of
     our calculus (although we do not prove this fact). Theoremhood and validity
     are one and the same.

 --------------------------------------------------------------------------------

   Evaluation of the expression $X = Y$ in a state yields the value /true/ if
   expressions $X$ and $Y$ have the same value and yields /false/ if they have
   different values.

   This characterisation of equality is in terms of expression evaluation.

   For reasoning about expressions, a more useful characterisation
   would be a set of laws that can be used to show that two expressions
   are equal, *without* calculating their values.
   # c.f., static analysis versues running a program

   For example, you know that $x = y$ equals $y = x$, regardless
   of the values of $x$ and $y$.

   A collection of such laws can be regarded as a definition
   of equality, *provided* two expressions have the same value
   in all states precisely when one expression can be translated into
   the other according to the laws.

   Later we see that theorems correspond to expressions that are true in all states.

** Inference Rules
   :PROPERTIES:
   :CUSTOM_ID: Logics-and-Inference-Rules
   :END:

   Formally, a â€œproofâ€ is obtained by applying a number of â€œrulesâ€ to known
   results to obtain new results; a â€œtheoremâ€ is the conclusion of a â€œproofâ€.
   An â€œaxiomâ€ is a rule that does not need to be applied to any existing
   results: It's just a known result.

   That is, a *rule* $R$ is a tuple $Pâ‚, â€¦, Pâ‚™, C$ that is thought of as â€˜taking
   *premises* (instances of known results) $Páµ¢$â€™ and acting as a â€˜natural,
   reasonable justificationâ€™ to obtain *conclusion* $C$.  A *proof system* is a
   collection of rules. At first sight, this all sounds very abstract and rather
   useless, however it is a /game/: *Starting from rules, what can you obtain?* Some
   games can be very fun! Another way to see these ideas is from the view of
   programming:

   #+caption: Proofs-are-programs
   | /           | <                                     |
   | Mathematics | Programming                           |
   |-------------+---------------------------------------|
   | logic       | trees (algebraic data types, ğ’²-types) |
   | rules       | constructors                          |
   |-------------+---------------------------------------|
   | proof       | an application of constructors        |
   | axiom       | a constructor with no arguments       |

   For example, recall from elementary school that the addition â€˜+â€™
   of a number 12 and a number 7 to obtain a number 19 is written as
   \begin{align*}
    & 12 \\
   + & \;\;7 \\ \hline
    & 19
   \end{align*}
   This familiar notation is also used for proof rules as well:
   A rule $R = (Pâ‚, â€¦, Pâ‚™, C)$ is traditionally presented in the shape
   \[{Pâ‚ \; Pâ‚‚ \; â€¦ \; Pâ‚™ \over C}R\]

   {{{begin-box(lime, ğ‘°ğ‘­ I have ingredients and a recipe for a cake ğ‘»ğ‘¯ğ‘¬ğ‘µ I can
   make a cake)}}}

   Here are two familiar and eerily similar rules ;-)

   $$\Rule[Function Application]{a : A \And f : A â†’ B}{f(a) : B}$$

   $$\Rule[Modus Ponens]{p \And p â‡’ q}{q}$$

   For instance, the first rule says â€œif you have a road between two cities, /A/ and /B/, then you
   can travel from address /a/ in city /A/ to get to address /f(a)/ in city $B$â€.  The
   second rule says the same thing, but *forgets/ignores* the precise
   locations. Sometimes it's okay for something â€œto existâ€, but other times
   that's not enough and you â€œactually want to get (construct) it somehowâ€;
   e.g., as the title begs: It's /possible/ to make a cake, but /how/? /Which/ recipe
   you use makes a difference!

   # The second rule is also known as /Impication Elimination/
   # as it is â€œthe way an implication can be usedâ€.

   {{{end-box}}}

 --------------------------------------------------------------------------------

     Just as there are meta-variables and meta-theorems, there is â€˜meta-syntaxâ€™:
     - The use of a fraction to delimit premises from conclusion is a form of â€˜implicationâ€™.
     - The use of a comma, or white space, to separate premises is a form of â€˜conjunctionâ€™.

     If our expressions actually have an implication and conjunction operation,
     then inference rules $\Rule[R]{Pâ‚ \And â‹¯ \And Pâ‚™}{C}$ can be presented as
     axioms $Pâ‚ \,âˆ§\, â‹¯ \,âˆ§\, Pâ‚™ \,â‡’\, C$.

     The inference rule says â€œif the $Páµ¢$ are all valid, i.e., true in /all
     states/, then so is $C$â€; the axiom, on the other hand, says â€œif the $Páµ¢$
     are true in /a state/, then $C$ is true in /that state/.â€ Thus the rule and
     the axiom are not quite the same.

     Moreover, the rule is not a Boolean expression.  Rules are thus more
     general, allowing us to construct systems of reasoning that have no
     concrete notions of â€˜truthâ€™ ---see the logic ğ‘¾ğ‘© below.

     Finally, the rule asserts that $C$ follows from $Pâ‚, â€¦, Pâ‚™$.
     The formula $Pâ‚ \,âˆ§\, â‹¯ \,âˆ§\, Pâ‚™ \,â‡’\, C$, on the other hand, is a Boolean
     expression (but it need not be a theorem).

     An example of this relationship between rules and operators
     may be observed by comparing the logics ğ‘¾ğ‘© and ğ‘´ğ‘ºğ‘¯, below.
     One could read â€œâ—‡â€ as â€œandâ€, and â€œâŸ¶â€ as â€œimpliesâ€.

#  --------------------------------------------------------------------------------

#   Let's look at a few simpler rules; the next 3 rules
#   are part of the *Logic E* system used in the LADM text book
#   ---see â€œ[[http://www.cse.yorku.ca/~logicE/misc/logicE_intro.pdf][Equational Propositional Logic]]â€ by Gries & Schneider.


A â€œtheoremâ€ is a syntactic concept: Can we play the game of moving symbols to
get this? Not â€œis the meaning of this trueâ€!
â€˜Semantic conceptsâ€™ rely on â€˜statesâ€™, assignments of values to variables
so that we can â€˜evaluate, simplifyâ€™ statements to deduce if they are true.

Syntax is like static analysis; semantics is like actually running the program
(on some, or all possible inputs).


** [Optional] Strange Logics
   :PROPERTIES:
   :CUSTOM_ID: water-bucket-logics
   :END:

     Here is an example logic, call it <<<ğ‘¾ğ‘©>>>:
     - The symbols are the usual numbers, along with =+= and =-= and
       =,= (comma).
     - A formula is term of the shape =x, y=, where $x$ and $y$ are terms formed
       from numbers, +, and -.
       + Notice that comma is a binary /operator/.
       + Notice that there are /no variables/ (as terms).
     - There are 7 inference rules ---including one axiom.

 #    Let's construct a logic to that models two bukects of water,
 #    one containing 3 liters and the other containing 5 liters, and
 #    an unlimited water supply.

 \[\Rule[Empty]{}{0,0}\]
 \[
 \Rule[ZeroLeft]{x,y}{0, y} \quad
 \Rule[ZeroRight]{x,y}{x, 0}
 \]\[
 \Rule[RefreshLeft]{x, y}{3, y} \quad
 \Rule[RefreshRight]{x, y}{x, 5}
 \]
 \[ \Rule[ShiftLeft_d \quad\text{(provided $y - d = 0$ or $x + d = 3$)}]{x, y}{x + d, y -
 d} \]
 \[
 \Rule[ShiftRight_d \quad\text{(provided $x - d = 0$ or $y + d = 5$)}]{x, y}{x - d, y + d}
 \]

 *Exercise [[#water-bucket-logics]].1*: Using this logic, prove the theorem =0, 4=.
 - Notice that the theorem has nothing to do with â€˜truthâ€™! ---At least not
   explicitly, or intuitively.
 #+begin_details Solution
 \[
 \Rule[ZeroLeft]{\normalsize\Rule[ShiftLeft_1]{\LARGE\Rule[RefreshLeft]{\LARGE\Rule[ShiftLeft_2]{\Rule[ZeroLeft]{\LARGE
 \Rule[ShiftLeft_3]{\LARGE \Rule[RefreshRight]{\LARGE\Rule[Empty]{}{0,0}}{0,
 5}}{3, 2}}{0,2}}{2,0}}{2, 5}}{3, 4}}{0, 4}
 \]
 #+end_details

 *Exercise [[#water-bucket-logics]].2:*
 A logic models reasoning, can you /interpret/ the terms =x, y= in such
 a way that makes the inference rules true?
 #+begin_details Solution

 The logic ğ‘¾ğ‘© /could be/ interpreted as modelling two â€˜water bucketsâ€™, the first
 can contain 3 litres while the second can contain 5 litres, along with an
 unlimited water supply.

 1. The axiom says we start out with empty buckets.
 2. The zero rules says we can empty out buckets.
 3. The refresh rules say we can fill up buckets to being full.
 4. The shift rules say we can pour out water from one bucket to
    the other, such that the first is emptied *or* the second is filled.
    (In particular, we cannot pour an arbitrary /chosen/ amount of water. )

 Then the theorem says we can measure 4 litres of water ---using only a 3 and 5
 litre buckets and an unlimited water supply.
 #+end_details

 --------------------------------------------------------------------------------

 Here is another example logic, call it <<<ğ‘´ğ‘ºğ‘¯>>>:
 + The symbols are the usual numbers, along with =+, -, â—‡, âŸ¶=.
 + A formula is of the form $x â—‡ y âŸ¶ xâ€² â—‡ yâ€²$ where â—‡ binds tightest
   and $x, y, xâ€², yâ€²$ are terms formed from numbers, =+=, and =-=.
 + In contrast to ğ‘¾ğ‘©, this logic has only 1 non-axiom inference rule!

   \[\Rule[Reflexivity]{}{x â—‡ y âŸ¶ x â—‡ y}\]
   \[\Rule[Transitivity]{x â—‡ y âŸ¶ xâ€² â—‡ yâ€² \And xâ€² â—‡ yâ€² âŸ¶ xâ€³ â—‡ yâ€³}{x â—‡ y âŸ¶ xâ€³ â—‡ yâ€³}\]

   \[\Rule[ZeroLeft]{}{x â—‡ y âŸ¶ 0 â—‡ y} \quad \Rule[ZeroRight]{}{x â—‡ y âŸ¶ x â—‡ 0}\]
   \[\Rule[RefreshLeft]{}{x â—‡ y âŸ¶ 3 â—‡ y} \quad \Rule[RefreshRight]{}{x â—‡ y âŸ¶ x â—‡ 5} \]
   \[\Rule[ShiftLeft_d]{\text{(provided $y - d = 0$ or $x + d = 3$)}}{x â—‡ y âŸ¶ (x+d) â—‡ (y-d)} \]
   \[\Rule[ShiftRight_d]{\text{(provided $x - d = 0$ or $y + d = 5$)}}{x â—‡ y âŸ¶ (x - d) â—‡ (y + d)}\]

 *Exercise [[#water-bucket-logics]].3:* Finish reading this section, then come back and
 prove the theorem =0 â—‡ 0 âŸ¶ 0 â—‡ 4= using a /calculational proof/.
 #+begin_details Solution

 As discussed in Â§[[#Rules-of-Equality-and-Proof-Trees-vs-Calculational-Proofs]], we
 form calculational proofs using a transitive relation in the left-most column of
 a calculation.  The transitvity of the relation ensures that the first term is
 related, via the relation, to the last term.

 \begin{calc}
   0â—‡ 0 \step[âŸ¶]{refresh left}
   3â—‡ 0 \step[âŸ¶]{ shift right}
   0â—‡ 3 \step[âŸ¶]{ refresh left }
   3â—‡ 3 \step[âŸ¶]{ shift right, then zero right}
   1â—‡ 0 \step[âŸ¶]{ shift right }
   0â—‡ 1 \step[âŸ¶]{ refresh left, then shift right }
   0â—‡ 4
 \end{calc}

 :AnotherProof:
 #+begin_src C
  0,0
â†’âŸ¨ refresh 2 âŸ©
  0, 5
â†’âŸ¨ draw from 2 âŸ©
  3, 2
â†’âŸ¨ dump 1 âŸ©
  0, 2
â†’âŸ¨ draw from 2 âŸ©
  2, 0
â†’âŸ¨ refresh 1 âŸ©
  2, 5
â†’âŸ¨ draw from 2 âŸ©
  3, 4
â†’âŸ¨ dump 1 âŸ©
  0, 4
     #+end_src
 :End:
 #+end_details

 *Exercise [[#water-bucket-logics]].4:* Provide an interpretation of this logic.
 #+begin_details Solution

 We /may/ think of ğ‘´ğ‘ºğ‘¯ as a â€˜machineâ€™ with two memory banks: A computer with memory
 state $x$ and $y$ is executed and it terminates in memory state $xâ€²$ and $yâ€²$.
 That is, $x â—‡ y âŸ¶ xâ€² â—‡ yâ€²$ is â€œstarting from $(x, y)$, the computer finishes
 with $(xâ€², yâ€²)$â€.

 The theorem then says that it is possible for the computer to start at $(0, 0)$
 and finish with memory store $(0, 4)$.

 The idea to use *inference rules as computation*
 is witnessed by the [[https://alhassy.github.io/PrologCheatSheet/CheatSheet.pdf][Prolog]] programming language.

 Of-course, we could also re-use the water buckets interpretation of ğ‘¾ğ‘©.
 #+end_details

** Rules of Equality and Proof Trees vs. Calculational Proofs
   :PROPERTIES:
   :CUSTOM_ID: Rules-of-Equality-and-Proof-Trees-vs-Calculational-Proofs
   :END:

 # ** Defining equality by how it can be used, manipulated


 # E.g., 4 laws that characterise equality are reflexitivitry, symmetry,
 #   transitvity, and Leibniz.

 Before we can showcase an example of a proof tree ---let alone
 compare them with calculational proofs--- we need a few
 example inference rules that can be used in the construction of the proofs.

 The following rules define equality by how it can be used, manipulated.

 1. Equality is:
    - *reflexive:* $X = Y$;
    - *symmetric:* $X = Y$ implies $Y = X$; and
    - *transitive*: $X = Z$ follows from having both $X = Y$ and $Y = Z$, for any
      $Y$

 2. The *Substitution inference rule*
    says that a substitution $E[\vec x â‰” \vec F]$ is
    a theorem /whenever/ $E$ is a theorem.

    Within CalcCheck, this rule is realised as the kbd:with clause: The phrase =E
    with `xâ‚, xâ‚‚, â€¦, xâ‚™ â‰” Fâ‚, Fâ‚‚, â€¦, Fâ‚™`= is tantamount to invoking the theorem
    $E[\vec x â‰” \vec F]$. The rule is applied /implicitly/, unless =rigid matching=
    is activated ---e.g., to get students *thinking correctly about applying
    theorems* instead of just putting random theorems that look similar and hoping
    the system sees a justification from a mixture of them.

 3. The *Leibniz inference rule* says that $E[z â‰” X] = E[z â‰” Y]$ whenever $X = Y$;
    i.e., it justifies substituting â€œequals for equalsâ€.

    Leibniz allows us to use an equation to rewrite a part of an expression; and
    so, it justifies the use of â€˜calculation hintsâ€™.

    Leibniz says: Two expressions are equal (in all states) precisely when
    replacing one by the other in any expression $E$ does not change the value of
    $E$ (in any state).

      {{{begin-box(blue)}}}
    A /function/ $f$ is a rule for computing a value from another value.

    If we define $f\, x = E$ using an expression, then /function application/ can
    be defined using textual substitution: $f \, X = E[x â‰” X]$. That is,
    expressions can be considered functions of their variables
    ---but it is still expressions that are the primitive idea, the building blocks.

    Using functions, Leibniz says /if X = Y then f X = f Y, for any function f/.
    That is, if two things are actually the same, then any (/f-/)value extracted
    from one must be the same when extracted from the other.
    {{{end-box}}}

    Again: Unlike the Substitution rule, which allows us to instantiate /any/
    theorem, the Leibniz rule is meant for *applying equational theorems deeper
    within expressions*. Later on, we will look at â€˜monotonicity rulesâ€™ which will
    let us apply inclusion (â‰¤, â‡’, âŠ‘) theorems deep within expressions.

    The kbd:with syntax is overloaded for this rule as well.

 ------

 In addition to these rules, suppose that we have
    $2 Â· a = a + a$ (â€œTwiceâ€) and $-1 Â· a = - a$ (15.20) as axioms;
    then we can form the following proof.

 \[
 \Rule[Transitivity\; of\; =]
 {\large
   \Rule[\small Substitution]
   {\Large \Rule{âœ“}{-1 Â· a \,=\, - a} }
   { (- 1) Â· 2 Â· (x + y) \,=\, - (2 Â· (x + y)) }
   \And
   \Rule[\small Leibniz]
   {\Large \Rule{âœ“}{2 Â· a = a + a} }
   { - (2 Â· (x + y)) \,=\,    -((x + y) + (x + y)) }
 }{(- 1) Â· 2 Â· (x + y) \,=\, -((x + y) + (x + y))}
 \]

 This is known as a /natural deduction proof tree/; one begins â€˜readingâ€™ such a
 proof from the very *bottom*: Each line is an application of a rule of reasoning,
 whose assumptions are above the line; so read upward.
 The *benefit* of this approach is that *rules guide proof construction*; i.e., it is
 goal-directed.

 However the *downsides are numerous*:
 - So much horizontal space for such a simple proof!
 - One has to *repeat* common subexpressions, such as the
   $-(2 Â· (x + y))$.
 - For comparison with other proof notations, such as Hilbert style,
   see â€œ[[http://www.cse.yorku.ca/~logicE/misc/logicE_intro.pdf][Equational Propositional Logic]]â€ or LADM-Â§6.

 Instead, we may use a more â€˜linearâ€™ proof format:
 \begin{calc}
 (- 1) Â· 2 Â· (x + y)
 \step{ 15.20) $- a \,=\, - 1 Â· a$
       â”€ Using implicit substitution rule }
 - (2 Â· (x + y))
 \step{ â€œTwiceâ€
       â”€ Using implicit Leibniz with $a â‰” x + y$ }
 -((x + y) + (x + y))
 \end{calc}

 In this equational style, instead of a *tree* (on the left)
 we use a *sequential chain of equalities* (on the right):

 #+begin_parallel org
 $$\Rule[Leibniz]{X \,=\, Y}{E[z â‰” X] \,=\, E[z â‰” Y]}$$

 #+html: <br>

 \begin{calc}
     E[z â‰” X]
 \step{ X = Y }
     E[z â‰” Y]
 \end{calc}
 #+end_parallel

 In this way, we may use the Substitution rule to create theorems that can be
 used with the Leibniz rule and then use the Transitivity rule to conclude
 that the first expression of an equational proof is equivalent to the last one.
 {{{begin-box(orange, )}}}
 To show that $L = R$, transform $L$ into $R$ by a series of substitutions
 of equals for equals. (If $R$ has more â€˜structureâ€™, then begin at $R$ and
 transform to $L$.)
 {{{end-box}}}

 --------------------------------------------------------------------------------

 + Transitivity allows us to conclude the first expression in a calculation
    is equal to the last expression in the calculation.
 + Reflexivity allows us to have â€˜emptyâ€™ calculations and â€œno (expression) changeâ€
      calculation steps
 + Symmetry allows us to use an equation $LHS = RHS$
      â€œin the other directionâ€ to replace an instance of $RHS$ by $LHS$.

 Equational proofs thus have this shape:

 \begin{calc}
   P
 \step{ $P = Q[z â‰” X]$ }
   Q[z â‰” X]
 \stepmany{ \line{make a â€œremarkâ€ about $Q[z â‰” X]$}
            \line{or the direction of the proof}
            \line{or â€œremove superflous parenthesesâ€}
            \line{or â€œinsert parentheses for clairtyâ€} }
   Q[z â‰” X]
 \step{ $X = Y$ }
   Q[z â‰” Y]
 \step{ $R = Q[z â‰” Y]$ â”€â”€note the change in â€˜directionâ€™ }
   R
 \end{calc}

 Which is far *easier to read and write* than:
 \[
 \Rule[Transitivity]{
  P = Q[z â‰” X]
  \And
  \Rule[Transitivity]{
    \Rule[\large Transitivity]{ \LARGE
      \Rule[Reflexivity]{}{Q[z â‰” X] \eq Q[z â‰” X]}
      \And
      \Rule[Leibniz]{X \eq Y}{Q[z â‰” X] \eq Q[z â‰” Y]}
      }{\LARGE Q[z â‰” X] \eq Q[z â‰” Y]}
    \And
    {\LARGE \Rule[\large Symmetry]{R \eq Q[z â‰” Y]}{Q[z â‰” Y] \eq R}
    }}
 {\large \text{$Q[z â‰” X] \eq R$}}}
 {P = R}
 \]

    *The structure of equational proofs allows implicit use of infernece rules
    Leibniz, Transitvitity & Symmetry & Reflexivity of equality, and Substitution.* In contrast, the
    structure of proof trees is no help in this regard, and so all uses of
    inference rules must be mentioned explicitly.
    # In fact, more suitable inference rules for proof trees are those of /natural
    # deduction/ (ğ‘µğ‘«): Each propositional operator âŠ• has two rules, one to show
    # how to introduce it into a theorem (i.e., prove a theorem involving it) and
    # one to show how to use it (eliminate it) to derive new truths; as such, ğ‘µğ‘«
    # has no axioms and the â‡’-elimination inference rule is known as â€œmodus
    # ponensâ€, a theorem in ğ‘¬.

 --------------------------------------------------------------------------------

    Leibniz is often used with Substitution, as follows
    ---supposing we know the theorem =â€œHalfâ€= $2 Â· x / 2 = x$:

    \begin{calc}
      2 Â· j / 2 = 2 Â· (j - 1)
    \step{ Half, with $x â‰” j$ }
      j = 2 Â· (j - 1)
    \end{calc}

    We are using Leibniz with the premise $2 Â· j / 2 = j$.
    We can use this premise only if it is a theorem. It is, because
    $2 Â· x / 2 = x$ is a theorem and, therefore, by Substitution,
    $(2 Â· x / 2 = x)[x â‰” j]$ is a theorem.

    If a use of Substitution is simple enough, as in this case, we may leave
    off the indication â€œwith $x â‰” j$â€.

* Â Propositional Calculus
  :PROPERTIES:
  :CUSTOM_ID: Propositional-Calculus
  :END:

** Preliminaries :ignore:
   :PROPERTIES:
   :CUSTOM_ID: Preliminaries-ignore
   :END:

Often operations are defined by how they are evaluated (â€œoperationallyâ€), we
take the alternative route of defining operations by how they can be manipulated
(â€œaxiomaticallyâ€); i.e., by what properties they satisfy.  For instance, we may
define basic manipulative properties of operators ---i.e., /axioms/--- by
considering how the operators behave operationally on particular
expressions. That is, one may use an operational, intuitive, approach to obtain
an axiomatic specification (characterisation, interface) of the desired
properties. More concretely, since $(p â‰¡ q) â‰¡ r$ and $p â‰¡ (q â‰¡ r)$ evaluate to
the same value for any choice of values for $p, q, r$, we may insist that a part
of the definition of equivalence is that it be an doc:Associative operation.
Sometimes a single axiom is not enough to â€˜pin downâ€™ a unique operator ---i.e.,
to ensure we actually have a well-defined operation--- and other times this is
cleanly possible; e.g., given an ordering â€˜â‰¤â€™(â€˜â‡’, âŠ†, âŠ‘â€™) we can define minima
â€˜â†“â€™ (â€˜âˆ§, âˆ©, âŠ“â€™) by the axiom: â€œx â†“ y is the greatest lower boundâ€; i.e., $z â‰¤ x
â†“ y \quadâ‰¡\quad z â‰¤ x \,âˆ§\, z â‰¤ y$.

A /calculus/ is a method or process of reasoning by calculation with symbols. A
/propositional calculus/ is a method of calculating with Boolean (or
propositional) expressions.

A /theorem/ is a syntactic object, a string of symbols with a particular property.
A /theorem/ of a calculus is either an axiom or the conclusion of an inference
rule whose premises are theorems. Different axioms could lead to the same
set of theorems, and many texts use different axioms.

** Boolean Expressions and Laws
   :PROPERTIES:
   :CUSTOM_ID: Boolean-Expressions-and-Laws
   :END:

  The type of propositions is known as the *Booleans* and denoted ğ”¹.
     #+begin_src math
ğ”¹ ::= true | false
     #+end_src

*** Equality: â€œ=â€ and â€œâ‰¡â€
    :PROPERTIES:
    :CUSTOM_ID: Equality-and
    :END:

    For instance, the notion of equality on any type Ï„ is
    typed ~_=_ : Ï„ â†’ Ï„ â†’ ğ”¹~; i.e., equality takes two values of a type Ï„
    and returns a propositional value.

    #+begin_quote
    In general, the â€œcontinued equalityâ€ $x = y = z$
    is *read conjunctively*: Both $x = y$ /and/ $y = z$.
    However, for the special case Ï„ being ğ”¹, the expression
    $x = y = z$ could be *read associativity*: $(x = y) = z$.

    These two ways to read (parse) a continued equality
    give different operators on ğ”¹. The associative equality
    is popularly written as â€˜â‡”â€™ but, unfortunately, not usually treated
    as an equality at all! In this class, we write the associative equality
    as â€˜â‰¡â€™ and read it as â€œequivalesâ€.

    See [[https://www.researchgate.net/publication/220113201_The_associativity_of_equivalence_and_the_Towers_of_Hanoi_problem][The associativity of equivalence and the Towers of Hanoi problem]].
    #+end_quote

 The phrase $p â‰¡ q$ may be read as
    - /p is equivalent to q/, or
    - /p exactly when q/,
    - /p if-and-only-if q/,

    This operator is just equality on the Booleans:
    | Definition of â‰¡ |   | ~(p â‰¡ q) = (p = q)~ |

    The need for a new name for an existing concept is that they have different
    *notational conventions*: Firstly, â€œâ‰¡â€ has lower precedence than â€œ=â€ and
    secondly,
    - = is conjunctive :: $\big(p = q = r\big) \quad=\quad \big( (p = q)
      \;\land\; (q = r)\big)$
    - â‰¡ is associative :: $\big(p â‰¡ q â‰¡ r\big) \quad=\quad \big((p â‰¡ q) â‰¡ r\big) \quad=\quad \big(p â‰¡ (q â‰¡ r)\big)$

    For example, $\false â‰¡ \true â‰¡ \false$ is $\true$, whereas
    $\false = \true = \false$ is $\false$.

    #+begin_quote
 For the Booleans, equality is equal to equivalence:
 | /(p = q) = (p â‰¡ q)/ for /p, q : ğ”¹/ |

 For the Booleans, equality is equivalent to equivalence:
 | /(p = q) â‰¡ (p â‰¡ q)/ for /p, q : ğ”¹/  |
    #+end_quote

*** Useful Operators
    :PROPERTIES:
    :CUSTOM_ID: Useful-Operators
    :END:
 The Booleans have a number of useful operators that model reasoning,
    such as:
    #+caption: Boolean operators and similar numeric operators
    | Operator    | Booleans (ğ”¹)    | Numbers (â„¤)                 |
    | /           | >               |                             |
    |-------------+-----------------+-----------------------------|
    | â€œandâ€       | =_âˆ§_ : ğ”¹ â†’ ğ”¹ â†’ ğ”¹= | â€œminimumâ€ =_â†“_ : â„¤ â†’ â„¤ â†’ â„¤=   |
    | â€œorâ€        | =_âˆ¨_ : ğ”¹ â†’ ğ”¹ â†’ ğ”¹= | â€œmaximumâ€ =_â†‘_ : â„¤ â†’ â„¤ â†’ â„¤=   |
    | â€œnotâ€       | =Â¬_ : ğ”¹ â†’ ğ”¹=      | â€œnegationâ€ =-_ : â„¤ â†’ â„¤ â†’ â„¤=   |
    | â€œimpliesâ€   | =_â‡’_ : ğ”¹ â†’ ğ”¹ â†’ ğ”¹= | â€œinclusionâ€ =_â‰¤_ : â„¤ â†’ â„¤ â†’ ğ”¹= |
    | [[https://www.researchgate.net/publication/220113201_The_associativity_of_equivalence_and_the_Towers_of_Hanoi_problem][â€œEquivalesâ€]]  | =_â‰¡_ : ğ”¹ â†’ ğ”¹ â†’ ğ”¹= | â€œequalityâ€ =_=_ : â„¤ â†’ â„¤ â†’ ğ”¹=  |

    These operators can be defined /informally/, as done below, but we shall follow
    an /axiomatic/ definition as done in LADM by providing an /interface/ of
    properties that they satisfy instead of any particular /implementation/. Later
    in the class when we get to the =if_then_else_fi= construct, we may provide
    explicit implementations and prove them to be equal to the operations
    specified axiomatically.

    #+caption: Example explicit definitions ---not used in this class
    | â€œp âˆ§ qâ€ is â€œtrueâ€ whenever both â€œpâ€ and â€œqâ€ are â€œtrueâ€, otherwise it is â€œfalseâ€ |
    | â€œm â†“ nâ€ is â€œmâ€ whenever â€œm â‰¤ nâ€, otherwise it is â€œnâ€                            |

    #+caption: Meanings of Boolean operators
    | Expression | Pronounced       | is $\true$ if                                        |
    |------------+------------------+------------------------------------------------------|
    | $p â‰¡ q$    | /p equivales q/    | exactly an even number of arguments is $\false$, (â‹†) |
    | $p â‰¢ q$    | /p differs from q/ | exactly an odd number of its arguments are $\true$   |
    |------------+------------------+------------------------------------------------------|
    | $x = y$    | /x equals y/       | exactly $x$ and $y$ simplify to the same expression  |
    | $x â‰  y$    | /x differs from y/ | $x$ and $y$ do not simplify to the same expression   |
    |------------+------------------+------------------------------------------------------|
    | $p âˆ§ q$    | /p and q/          | all of its arguments are $\true$                     |
    | $p âˆ¨ q$    | /p or q/           | at least one of its arguments is $\true$             |
    |------------+------------------+------------------------------------------------------|
    | $p â‡’ q$    | /p implies q/      | either /q/ is $\true$ or /p/ is $\false$                 |
    |            | /if p, then q/     |                                                      |
    | $p â‡ q$    | /p follows from q/ | either /p/ is $\true$ or /q/ is $\false$                 |
    |            | /p if q/           |                                                      |
    |------------+------------------+------------------------------------------------------|
    | $Â¬ p$      | /not p/            | /p/ is $\false$; read â€œit is not the case that $p$â€    |

    For example,
    |   | â€œp, even if qâ€                               |
    | â‰ˆ | $p âˆ§ (q â‡’ p)$                                |
    |   | ( This is provably equivalent to just $p$. ) |

    (â‹†) Note that if an even number of arguments is /false/, then the /false/'s
    cancel out and only /true/ remains. Note that since /true/ is the identity of
    â€˜â‰¡â€™, we can simply cancel them out of a chain of equivalences.  When there
    are /2 Â· n/ many elements in the chain, then if there are an even number of
    /true/'s, say /k/-many, then there must be an even number of /false/'s: $\even (2 Â·
    n - k) = (\even (2 Â· n) â‰¡ \even (-k)) = \even k = \true$.

    Also,
    | $p â‰¡ q â‰¡ r$         | â‰ˆ | /One or all of p,q, and r are true/     |
    |---------------------+---+---------------------------------------|
    | $p â‰¡ q$             | â‰ˆ | /None or both of p and q is true/       |
    |---------------------+---+---------------------------------------|
    | $p â‰¢ q$             | â‰ˆ | /Exactly one of p and q is true/        |
    |                     |   | /Either p or q, but not both/           |
    |---------------------+---+---------------------------------------|
    | $pâ‚€ â‰¡ pâ‚ â‰¡ â‹¯ â‰¡ pâ‚‚â‚™$ | â‰ˆ | /An even number of the páµ¢ are true/ (â‹†) |
    |                     |   |                                       |

 #+begin_details (â‹†) When is <em>pâ‚€ â‰¡ pâ‚ â‰¡ â‹¯ â‰¡ pâ‚™</em> true?
 When is /pâ‚€ â‰¡ pâ‚ â‰¡ â‹¯ â‰¡ pâ‚™/ true?

 Since /true/ is the identity of â€˜â‰¡â€™; any $páµ¢$ equal to $\true$ can be â€˜cancelled
 outâ€™. Hence, we are left with only $\false$'s. Since $(\false â‰¡ \false) â‰¡
 \true$, we can cancel out any pair of $\false$'s and so if there are an even
 number of $\false$'s the resulting expression is $\true$.

 However, if the number, say $k$, of $\false$'s is even, then
 \begin{calc}
 \text{the parity of trues}
 \step{ Formalise: The trues are the non-falses }
 \even(n - k)
 \step{ Even distributes over sums, subtractions }
 \even n â‰¡ \even (-k)
 \step{ Even is invariant under unary minus }
 \even n â‰¡ \even k
 \step{ By assumption, there are $k$-many falses}
 \even n â‰¡ \true
 \step{ Identity of equivalence }
 \even n
 \end{calc}

 Hence,
 |   | $pâ‚€ â‰¡ pâ‚ â‰¡ â‹¯ â‰¡ pâ‚™$                                                        |
 | â‰ˆ | An even number of the arguments is false.                                 |
 | â‰ˆ | The parity of trues is the same as the parity of the number of arguments. |

 For example,
 |   | $p â‰¡ q$                                     |
 | â‰ˆ | None or both of $p$ and $q$ is true         |
 |---+---------------------------------------------|
 |   | $p â‰¢ q$                                     |
 | â‰ˆ | Exactly one of $p$ and $q$ is true          |
 |---+---------------------------------------------|
 |   | $p â‰¡ q â‰¡ r$                                 |
 | â‰ˆ | One or all of $p, q, r$ are true            |
 |---+---------------------------------------------|
 |   | $p â‰¡ q â‰¡ r â‰¡ s$                             |
 | â‰ˆ | Zero, two, or four of $p, q, r, s$ are true |
 |---+---------------------------------------------|
 |   | $Â¬ (p â‰¡ q â‰¡ r â‰¡ s)$                         |
 | â‰ˆ | One or three of $p, q, r, s$ are true.      |

 The second and last examples rely on the fact that â€œnot an even number are trueâ€
 equivales â€œan odd number are trueâ€.

 #+end_details

*** Boolean Laws and Numeric Laws
    :PROPERTIES:
    :CUSTOM_ID: Boolean-Laws-and-Numeric-Laws
    :END:
 To better understand the ğ”¹ooleans, it can be useful to compare their laws
     with those on numbers. For instance, the =Definition of â‡’= at first glance is
     tremendously cryptic: Why in the world would anyone define implication in
     this way $p â‡’ q \,â‰¡\, p âˆ§ q â‰¡ p$?  However, when compared to the similar law
     for numbers that defines inclusion $m â‰¤ n \,â‰¡\, m â†“ n = m$, the definition
     becomes *â€œobviousâ€*: /p is included in (implies) q precisely when having both p
     and q is the same as just having p/; i.e., /m is at-most n precisely when m is
     the minimum of m and n./

     #+caption: Properties of propositional operators and similar (familiar) numeric laws
     | Law                       | Booleans (ğ”¹)                      | Numbers (â„¤ with Â±âˆ)               |
     | /                         | >                                 |                                   |
     |---------------------------+-----------------------------------+-----------------------------------|
     | Symmetry of âˆ§             | $p âˆ§ q â‰¡ q âˆ§ p$                   | $m â†“ n = n â†“ m$                   |
     | Associativity of âˆ§        | $(p âˆ§ q) âˆ§ r â‰¡ p âˆ§ (q âˆ§ r)$       | $m â†“ n = n â†“ m$                   |
     | Idempotency of âˆ§          | $p âˆ§ p â‰¡ p$                       | $n â†“ n = n$                       |
     | Identity of âˆ§             | $p âˆ§ \true â‰¡ p$                   | $n â†“ +âˆ = n$                      |
     | Zero of âˆ§                 | $p âˆ§ \false â‰¡ \false$             | $n â†“ -âˆ = -âˆ$                     |
     | Contradiction             | $p âˆ§ Â¬ p â‰¡ \false$                | â”€nopeâ”€                            |
     |---------------------------+-----------------------------------+-----------------------------------|
     | Symmetry of âˆ¨             | $p âˆ¨ q â‰¡ q âˆ¨ p$                   | $m â†‘ n = n â†‘ m$                   |
     | Associativity of âˆ¨        | $(p âˆ¨ q) âˆ¨ r â‰¡ p âˆ¨ (q âˆ¨ r)$       | $m â†‘ n = n â†‘ m$                   |
     | Idempotency of âˆ¨          | $p âˆ¨ p â‰¡ p$                       | $n â†‘ n = n$                       |
     | Identity of âˆ¨             | $p âˆ¨ \false â‰¡ p$                  | $n â†‘ -âˆ = n$                      |
     | Zero of âˆ¨                 | $p âˆ¨ \true â‰¡ p$                   | $n â†‘ +âˆ = +âˆ$                     |
     | Excluded Middle           | $p âˆ¨ Â¬ p â‰¡ \false$                | â”€nopeâ”€                            |
     |---------------------------+-----------------------------------+-----------------------------------|
     | Golden Rule               | $p âˆ§ q â‰¡ p â‰¡ q â‰¡ p âˆ¨ q$           | $m â†“ n = m \,â‰¡\, n = m â†‘ n$       |
     | âˆ§/âˆ¨ Distributivity        | $p âˆ§ (q âˆ¨ r) â‰¡ (p âˆ§ q) âˆ¨ (p âˆ§ r)$ | $m â†‘ (n â†“ r) = (m â†‘ n) â†“ (m â†‘ r)$ |
     | âˆ¨/âˆ§ Distributivity        | $p âˆ¨ (q âˆ§ r) â‰¡ (p âˆ¨ q) âˆ§ (p âˆ¨ r)$ | $m â†‘ (n â†“ r) = (m â†‘ n) â†“ (m â†‘ r)$ |
     |---------------------------+-----------------------------------+-----------------------------------|
     | Absorption                | $(p âˆ¨ q) âˆ§ Â¬ p â‰¡ Â¬ p âˆ§ q$         | â”€nopeâ”€                            |
     |                           | $(p âˆ§ q) âˆ¨ Â¬ p â‰¡ Â¬ p âˆ¨ q$         | â”€nopeâ”€                            |
     |                           | $(p âˆ¨ q) âˆ§ p â‰¡ p$                 | $(m â†‘ n) â†“ m = m$                 |
     |                           | $(p âˆ§ q) âˆ¨ p â‰¡ p$                 | $(m â†“ n) â†‘ m = m$                 |
     |---------------------------+-----------------------------------+-----------------------------------|
     | Double negation           | $Â¬ Â¬ p â‰¡ p$                       | $- - n = n$                       |
     | Definition of $\false$    | $\false â‰¡ Â¬ \true$                | $-âˆ \,=\, - (+âˆ)$                 |
     | Negation of $\false$      | $Â¬ \false = \true$                | $- (-âˆ) = +âˆ$                     |
     | De Morgan                 | $Â¬(p âˆ§ q) = Â¬ p âˆ¨ Â¬ q$            | $-(m â†“ n) = -m â†‘ -n$              |
     |                           | $Â¬(p âˆ¨ q) = Â¬ p âˆ§ Â¬ q$            | $-(m â†‘ n) = -m â†“ -n$              |
     |---------------------------+-----------------------------------+-----------------------------------|
     | Definition of â‡’           | $p â‡’ q â‰¡ p âˆ§ q â‰¡ p$               | $m â‰¤ n \,â‰¡\, m â†“ n = m$           |
     |                           | $p â‡’ q â‰¡ p âˆ¨ q â‰¡ q$               | $m â‰¤ n \,â‰¡\, m â†‘ n = n$           |
     | Consequence               | $p â‡ q â‰¡ q â‡’ p$                   | $m â‰¥ n \,â‰¡\, n â‰¤ m$               |
     | ex falso quodlibet        | $\false â‡’ p â‰¡ \true$              | $-âˆ â‰¤ n \,â‰¡\, \true$              |
     | Left-identity of â‡’        | $\true â‡’ p â‰¡ p$                   | $+âˆ â‰¤ n \,â‰¡\, n = +âˆ$             |
     | Right-zero of â‡’           | $p â‡’ \true â‰¡ \true$               | $n â‰¤ +âˆ \,â‰¡\, \true$              |
     | Definition of Â¬           | $p â‡’ \false â‰¡ Â¬ p$                | â”€nopeâ”€                            |
     | Contrapositive            | $p â‡’ q \,â‰¡\, Â¬ q â‡’ Â¬ p$           | $m â‰¤ n \,â‰¡\, -n â‰¤ -m$             |
     |---------------------------+-----------------------------------+-----------------------------------|
     | Weakening / Strengthening | $p âˆ§ q â‡’ p$                       | $m â†“ n â‰¤ m$                       |
     |                           | $p â‡’ p âˆ¨ q$                       | $m â‰¤ m â†‘ n$                       |
     |                           | $p â‡’ q \,â‰¡\, p â‡’ p âˆ§ q$           | $m â‰¤ n \,â‰¡\, m â‰¤ m â†“ n$           |
     |---------------------------+-----------------------------------+-----------------------------------|

     An *instance of absorption $(p âˆ¨ q) âˆ§ Â¬ p  \;â‰¡\; Â¬ p âˆ§ q$*:
     /If I say â€œI have a dog or a cat, and I don't have a dogâ€, then I might as
     well be saying â€œI don't have a dog but I have a catâ€./

** â€˜trueâ€™
   :PROPERTIES:
   :CUSTOM_ID: Equivalence-and-true
   :END:

   The symmetry of equivalence could be read as $(p â‰¡ p) â‰¡ (q â‰¡ q)$ and so
   â€˜self-applications of â‰¡â€™ are indistinguishable.  That is, the value of $p â‰¡
   p$ does not depend on the value of $q$ and so we introduce the constant
   symbol /true/ is an abbreviation for $p â‰¡ p$.

  \[\Law[(3.4)]{Axiom, Identity of â‰¡}{\true â‰¡ p â‰¡ p}\]

  When this definition is read as $(\true â‰¡ p) = p$, and by symmetry of â‰¡ as $(p
  â‰¡ \true) = p$, we see that this new constant is an doc:Identity of â‰¡.

  Since â‰¡ is associative, a formula can be read in multiple ways.
  - $p â‰¡ p â‰¡ true$ can be read as the reflexitivty of â‰¡ or the definition
    of true ---both being $(p â‰¡ p) â‰¡ true$ ---
    or as an identity law --- $p â‰¡ (p â‰¡ true)$.
  - The Golden Rule can also be read a way to define âˆ§ in-terms of â‰¡ and âˆ¨,
    or to define âˆ¨ in terms of â‰¡ and âˆ¨, or to phrase â‰¡ in terms of â‰¡, âˆ§, and âˆ¨;
    or to absorb an expression containing â‰¡,âˆ¨, âˆ§ down to a single subexpression:
    $p â‰¡ (q â‰¡ p âˆ¨ q â‰¡ p âˆ§ q)$.

  #+begin_box (3.56) Parsing Heuristic ---Page 56
  Exploit the ability to parse theorems like the â€˜Golden Ruleâ€™
  and the â€˜Definition of trueâ€™ in many different ways.

  For instance, in chains of equivalences, the use symmetry and associativity
  of equivalence increases the number of parses.
  #+end_box

  Using its definition, we can quickly show that
  $\true = \big(\true â‰¡ q â‰¡ q\big)$ and so by equanimity,
  since the right side is a theorem, then the left side is also a theorem.
  Hence,

  \[\Law[3.4]{True is a theorem}{\true}\]

  What is the benefit of this theorem?

  By equanimity, this means that to prove $P$ is a theorem, it is
  enough to show that $P â‰¡ \true$! This is an â€˜expectedâ€™ result :-)

  ( We can phrase this observation as a theorem itself as $(P â‰¡ \true) â‰¡ P$, but
  this is essentially the definition of true, above! )

  Here is an impressive benefit of this theorem.  Suppose we want to prove an
  equation $L = R$ is true; if our proof only alters $L$ to make it the same as
  $R$ so that we obtain $R = R$, then we may the definition of $\true$ to
  obtain, well, $\true$, but since this is a theorem then so too is $L = R$.
  That is,
  \begin{calc}
  L = R
  \step{ Perform a number of steps ... }
  ...
  \step{ ... to transform L to R }
  R = R
  \step{ Definition of identity }
  \true
  \end{calc}
  Since the right side of the equation â€œ= Râ€ is not altered, we can
  /abbreviate/ such calculations, by omitting the final step and avoiding
  the repetitious â€œ= Râ€ on each line, as follows.
  \begin{calc}
  L
  \step{ Perform a number of steps ... }
  ...
  \step{ ... to transform L to R }
  R
  \end{calc}

  That is, (3.4) gives us a new proof method ---which is always a bonus result
  from a theorem.

  #+begin_box (3.6) Simplifiction Proof Method ---Page 45
  To prove $L = R$ is a theorem, transform $L$ to $R$ or $R$ to $L$
  using Leibniz (equals for equals reasoning).

  Usually, you start with the more â€˜complicatedâ€™ (more structured) side of the
  equation and transform that to the â€˜simplerâ€™ side. The (additional) structure
  then narrows the number of applicable laws and thus guides the proof.
  #+end_box

  #+begin_box (3.34) Rabbit Avoidance ---Page 51
  A â€œrabbit pulled out of hatâ€ is a step in a proof that has little or no
  motivation; e.g., it introduces more structure and it's not clear why that is
  the case ---for instance, replacing $true$ with $p âˆ¨ p â‰¡ p âˆ¨ p$.

  Structure proofs to minimise the number of rabbits pulled out of a hat ---make
  each step seem obvious, based on the structure of the expression and the goal
  of the manipulation.

  E.g., when the driving goal of a proof is to simplify; then there should not
  be any rabbits,
  #+end_box

  Finally, (3.4) gives us the following doc:Metatheorem.

  \[\Law[(3.7)]{Metatheorem}{\text{Any two theorems are equivalent}}\]

  Indeed, if $P$ is a theorem and $Q$ is a theorem, then by (3.4) we have $P â‰¡
  \true$ and $\true = Q$ and so by transitivity of â‰¡, we have $P â‰¡ Q$.

  With true in-hand, one can now define false:
  \[\Law[3.10]{Definition of false}{\false â‰¡ Â¬ true}\]

  Since â€˜â‰¡â€™ = â€˜=â€™ on the Booleans, we can phrase this as $false â‰  true$,
  which is a useful thing to know.

  Moreover, we can then show that a Boolean expression not equal to true is
  equal to false: ~(p â‰¢ true) â‰¡ (p â‰¡ false)~.

** Double Negation Example.
   :PROPERTIES:
   :CUSTOM_ID: Double-Negation-Example
   :END:

 \[\Law[(3.12)]{Double negation}{Â¬ Â¬ p â‰¡ p}\]

 Double negation asserts that negation is its own inverse.

 Double negation is used in English occasionally.
 For example, one might say â€œThat was not done unintentionallyâ€
 instead of â€œThat was done intentionallyâ€.

** A remark on Axiom (3.9) â€œCommutativity of Â¬ with â‰¡â€: Â¬ (p â‰¡ q) â‰¡ (Â¬ p â‰¡ q)
   :PROPERTIES:
   :CUSTOM_ID: A-remark-on-Axiom-3-9-Commutativity-of-with-p-q-p-q
   :END:

   \[\Law[(3.9)]{Commutativity of Â¬ with â‰¡}{ Â¬ (p â‰¡ q) â‰¡ (Â¬ p â‰¡ q)}\]

   The left side says that /p/ and /q/ are different; but there are only two Boolean
   values and so for /p/ and /q/ to be different, one must be the â€˜flipâ€™ (negation)
   of the other.

   Moreover, this rule says â€œdiffers fromâ€ (â‰ ) on the Booleans can be expressed
   directly in terms of equality (=) instead of a negation of an equality
   ---which is the case in general.

   The following laws uniquely define negation.

   \[\Law[(3.8)]{Axiom, Definition of false}{\false â‰¡ Â¬ \true}\]
   \[\Law[(3.9)]{Axiom, Commutativity of Â¬ with â‰¡}{Â¬ (p â‰¡ q) â‰¡ Â¬ p â‰¡ q}\]

   Indeed, suppose $f : ğ”¹ â†’ ğ”¹$ also satisfies these laws, then we can
   show $f(p) â‰¡ Â¬ p$ ---in particular, $f(\true) = \false$ and $f(\false) = \true$.

   That is, of the 4 possibly unary functions on the Booleans, only negation
   satisfies these two properties.

** TODO COMMENT 3.21 on p47; 3.22&23 and preceeding paragraph on p48; and footnote 10 on p58
   :PROPERTIES:
   :CUSTOM_ID: COMMENT-3-21-on-p47-3-22-23-and-preceeding-paragraph-on-p48-and-footnote-10-on-p58
   :END:

** Alternative definitions of â‰¡ and â‰¢
   :PROPERTIES:
   :CUSTOM_ID: Alternative-definitions-of-and
   :END:

  The following theorems are sometimes used to define â‰¡ and â‰¢.
  The first theorem indicates that $p â‰¡ q$ holds exactly when $p$
  and $q$ are both /true/ or both /false/. The second theorem indicates that
  $p â‰¢ q$ holds exactly when one of them is /true/ and the other is /false/.

  \[\Law[(3.52)]{Definition of â‰¡}{p â‰¡ q â‰¡ (p âˆ§ q) âˆ¨ (Â¬ p âˆ§ Â¬ q)}\]
  \[\Law[(3.53)]{Exclusive or}{p â‰¢ q â‰¡ (Â¬ p âˆ§ q) âˆ¨ (p âˆ§ Â¬ q)}\]

# (3.53b) ?
#+begin_src math
Theorem â€œxorâ€ â€œâ‰¢ is one or the other, but not bothâ€: (p â‰¢ q) â‰¡ (p âˆ¨ q) âˆ§ Â¬ (p âˆ§ q)
Proof:
    (p âˆ¨ q) âˆ§ Â¬ (p âˆ§ q)
  =âŸ¨ â€œDe Morganâ€ âŸ©
    (p âˆ¨ q) âˆ§ (Â¬ p âˆ¨ Â¬ q)
  =âŸ¨ â€œDistributivity of âˆ§ over âˆ¨â€ âŸ©
     ((p âˆ¨ q) âˆ§ Â¬ p) âˆ¨ ((p âˆ¨ q) âˆ§ Â¬ q)
  =âŸ¨ â€œAbsorptionâ€ âŸ©
     (Â¬ p âˆ§ q) âˆ¨ (Â¬ q âˆ§ p)
  =âŸ¨ â€œAlternative definition of â‰¢â€ âŸ©
     p â‰¢ q
#+end_src

  In most propositional calculi equivalence is the last operator to be defined
  and is defined as â€œmutual implicationâ€.  Thus, (3.80) below typically is made
  an axiom.  We down-play implication in our calculus because, as an unsymmetric
  operator (by 3.72 and 3.73), it is harder to manipulate. Sometimes
  (3.80) would be read as â€œ(strong) antisymmetry of â‡’â€.

  \[\Law[(3.72)]{Right Zero of â‡’}{p â‡’ \true â‰¡ \true}\]
  \[\Law[(3.73)]{Left Identity of â‡’}{\true â‡’ p â‰¡ p}\]

  \[\Law[((3.80))]{(Mutual Implication)}{(p â‡’ q) âˆ§ (q â‡’ p) â‰¡ p â‰¡ q}\]

** Contextual Rules ---Leibniz and Substitution
   :PROPERTIES:
   :CUSTOM_ID: Contextual-Rules-Leibniz-and-Substitution
   :END:

   With the implication operator available, the Leibniz inference
   rule can be re-cast as an axiom.

   \[\Law{Abbreviation}{E^z_F \;=\; E[z â‰” F]}\]
   \[\Law[(3.83)]{Axiom, Leibniz}{(e = f) â‡’ E_e^z = E_f^z}\]
   \[\Law[(3.84a)]{Substitution}{(e = f) âˆ§ E_e^z \quadâ‰¡\quad (e = f) âˆ§ E_f^z}\]
   \[\Law[(4.84c)]{Substitution}{q âˆ§ (e = f) â‡’ E_e^z \quadâ‰¡\quad q âˆ§ (e = f) â‡’ E_f^z}\]

   Replacing variables by Boolean constants.

   \[\Law[(3.85)]{Replace by true}{q âˆ§ p â‡’ E^z_p \quadâ‰¡\quad q âˆ§ p â‡’ E^z_\true}\]
   \[\Law[(3.86)]{Replace by false}{E^z_p â‡’ p âˆ¨ q \quadâ‰¡\quad E^z_\false â‡’ p âˆ¨ q}\]
   \[\Law[(3.87)]{Replace by true}{p âˆ§ E_p^z \quadâ‰¡\quad p âˆ§ E_\true^z}\]
   \[\Law[(3.88)]{Replace by false}{p âˆ¨ E_p^z \quadâ‰¡\quad p âˆ¨ E_\false^z}\]
   \[\Law[(3.89)]{Shannon, Case analysis}{E_p^z \quadâ‰¡\quad (p âˆ§ E_\true^z) âˆ¨ (Â¬  p âˆ§ E_\false^z)}\]

** Disjunction
   :PROPERTIES:
   :CUSTOM_ID: Disjunction
   :END:

   The axioms (3.24)-(3.28) uniquely determine disjunction.

   That is, of the 16 possibly binary functions on the Booleans, only
   disjunction satisfies these properties.

* COMMENT Induction ---Chapter 12
  :PROPERTIES:
  :CUSTOM_ID: Induction
  :END:

\[Law[(12.4)]{Induction}{(âˆ€ n : â„• â€¢ (âˆ€ m : â„• â€¢ m < n â‡’ P\, m) â‡’ P\, n) \quadâ‰¡\quad (âˆ€ n : â„• â€¢ P\, n)}\]

This says that to prove $P\, n$ for all natural numbers $n$ (the right side), we
prove (the left side) that for any $n$, if $P\, 0, P\, 1, â€¦, P\, (n - 1)$ holds,
then so does $P\, n$. This is because, in principle ---given enough time and
space--- we can prove $P\, N$ for any given $N$ by proving, in turn, $P\, 0, P\,
1, â€¦,$ and finally $P\, N$:
+ When $n = 0$, the left side simplifies to $P\, 0$, and so we conclude $P\, 0$.
+ From $P\, 0$ and $P\,0 â‡’ P\, 1$ (the left side with $n â‰” 1$), by Modus ponens
  (3.77)
  we conclude $P\, 1$.
+ ...
+ From $P\, 0 âˆ§ â‹¯ âˆ§ P\, (N - 1)$ and $P\, 0 âˆ§ â‹¯ âˆ§ P\, (N - 1) â‡’ P\, N$ (the left
  side with $n â‰” N$) by Modus ponens (3.77) we conclude $P\, N$.


#+begin_box Exposing the Hypothesis Heuristic
In the proof of $P\, n$, one aims to â€˜split off a termâ€™
so as to expose $P\, i$, for $i < n$, and use that hypothesis
to make progress in the proof of $P\, n$.
#+end_box

* Â Program Correctness
   :PROPERTIES:
   :CUSTOM_ID: The-Assignment-Statement
   :END:

   #+begin_latex-definitions
   \def\If#1{\,\;â‡\!\![#1]\;\;\,} % goal â‡[command] provisos
   \def\Then#1{\,\;â‡’\!\![#1]\;\;\,} % provisos â‡’[command] goal
   #+end_latex-definitions

    # (load-file "~/Desktop/power-blocks.el")

    Textual substitution is inextricably intertwined with equality.

    Likewise, assignment statements in programming can be reasoned
      about using textual substitution.

      The coincidence of notations is deliberate.

      Rather than understanding how a program is /executed/,
      we can also understand a program in terms of /syntactic substitution/.

   #+begin_center
   Focus is on goal-directed and calculational construction of algorithms as
   opposed to the traditional guess-and-verify methodology.
   #+end_center

** From Comments to Hoare Triples
   :PROPERTIES:
   :CUSTOM_ID: More
   :END:

 #+begin_box Commenting Your Code :background-color cyan
 When writing computer programs, it is very good practice to comment them
 thoroughly in order to explain what is going on.  It helps the programmer to
 avoid errors by enforcing greater clarity, and it helps others who need to
 modify the program at a later date (including the one who wrote the program in
 the first place!)

 It is a good discipline, for example, to comment every variable declaration with
 a statement about the variable's function in the program.  This has the
 additional benefit of disciplining the programmer to use distinct variables for
 distinct functions, rather than overloading a variable with several different
 functions.

 Good comments supplement  the program text with explanations of the program's
 function and why the code that is used achieves that function.
 #+end_box

   The comments we write state formal properties of the program variables at a
   particular point in the execution of the program.

   Sometimes comments are written within braces, as in ~{ 0 < i } i := i - 1 { 0 â‰¤
   i }~ which documents that before the assignment we know $0 < i$ and after the
   assignment we know $0 â‰¤ i$. Such /machine checkable comments/ are also known as
   *assertions* and many languages have src_plantuml[:exports code]{assert }
   commands ---e.g., [[https://alhassy.github.io/PythonCheatSheet/CheatSheet.pdf][Python]] has them.

   An assertion is a claim that a particular property about the program variables
   is true at that point in the program's execution.

   An expression of the form ~{ R } C { G }~, where $R, G$ are properties of the program
   variables and $C$ is a program command, is called a <<<Hoare triple>>>.

   Such expressions are *commented programs*, but they are also *Boolean
     expressions*: Triples ~{ R } C { G }~ denote the claim that, if the program
     variables satisfy property $R$ before execution of command =C=, then execution
     of =C= is guaranteed to terminate and, afterwards, the program variables will
     satisfy property $G$. ( /Specifications are theorems!/ )

 What can be said of the following *very interesting* triples?
 | ~{ true  } i := 1 { i = 1 }~ |
 | ~{ i = 1 } i := 0 { true  }~ |
 | ~{ false } i := 1 { i = 0 }~ |
 #+begin_details Solutions

 The solutions below will generalise the exercises.

 #+begin_box No assumptions needed!

 | ~{ true } x := K { x = K }~  for a /constant/ $K$ |

 A $\true$ precondition describes all states of the program variables; the claim
 is thus that /whatever the initial value of the program variables/ (in particular
 the variable /x/) after execution of the assignment ~x := K~ the property $x = K$
 will hold ---you can /prove/ this using the assignment rule below.
 #+end_box

 #+begin_box Termination!

 | ~{ R } C { true }~        |

 The triple says nothing about the command because Since all states satisfy
 postcondition $\true$, the triple communicates that ~C~ /terminates/ ---see the
 informal definition of the Hoare triples.

 Compare this with the right-zero property of implication: $p â‡’ \true$; â€œtrue is
 always true, no matter what you have in handâ€. Also similar to $n â‰¤ +âˆ$.
 #+end_box

 #+begin_box Impossible Assumptions ---â€œThe Law of the Excluded Miracleâ€

 | ~{ false } C { G }~          |

 The claim is vacuously true because the assumption is that the execution of the
 assignment is begun in a state satisfying $\false$, which can never be the case.

 Compare this property with ex falso quodlibet: $\false â‡’ p$; â€œstarting from
 false, anything can be derivedâ€. Also similar to $-âˆ â‰¤ n$.
 #+end_box

 #+end_details

 Consider the swap program ~x, y := y, x~, it swaps the /values/ of the /program
 variables/. To formalise such a claim, we introduce variables $X$ and $Y$
 to denote the /values/ of the program variables ~x, y~. Then, we can
 formalise the claim as
 \[
 x = X \;âˆ§\; y = Y \;\Then{x,y := y, x}\; x = Y \;âˆ§\; y = Y
 \]
 We refer to variables that never appear in program text
 as _<<<ghost variables>>>_; their function is to relate the final values of the
 program variables to their initial values.

 #+begin_box Definition of Hoare Triples
 In general, an expression ~{ R } C { G }~, with $R, G$ predicates on a collection
 of program variables and ghost variables, means that, /forall possible values of
 the ghost variables,/ if the program variables satisfy property $R$ before
 execution of the command ~C~, execution of ~C~ is guaranteed to terminate, and,
 afterwards, the program variables will satisfy property $G$.
 #+end_box

 So the claim about swapping variables above is that, for all values of $X$ and
 $Y$, if $x = X \;âˆ§\; y = Y$, before executing the simultaneous assignment ~x,
 y := y, x~, then, afterwards, $x = Y \;âˆ§\; y = X$.
** â€˜Dynamic Logicâ€™ Notation
   :PROPERTIES:
   :CUSTOM_ID: Dynamic-Logic-Notation
   :END:
   States may be represented by predicates on variables and so *imperative
   commands are relations on predicates*: /Given two propositions $G, R$ making/
   /use of program variables, we write $R \Then{C} G$ to mean â€œin a state $R$, the
   execution of command $C$ terminates in state $G$â€/.
   ( This is also known as a /Hoare Triple/ and written ={R} C {G}=. )

   For  example,
   | ~v = 5 âˆ§ w = 4 âˆ§ x = 8 Â Â â‡’[ v := v + w ]Â Â  v = 9 âˆ§ w = 4 âˆ§ x = 8~ |
   |-----------------------------------------------------------------|
   | ~x = 0 Â Â â‡’[ x := x + 1 ]Â Â x > 0~                                  |
   | ~x > 5 Â Â â‡’[ x := x + 1]Â Â x > 0~                                   |
   |-----------------------------------------------------------------|
   | ~(x = 5 Â Â â‡’[ x := x + 1 ]Â Â x = 7) Â Â â‰¡Â Â  false~                    |
   |-----------------------------------------------------------------|
   | ~5 â‰  5 Â Â â‡’[ x := 5 ]Â Â  x â‰  5~                                     |

   In practice, one begins with *the goal G* (also known as the
   â€˜postconditionâ€™) and /forms/ a suitable *command C* that ensures $G$ but may
   require some provisos to be given, which are conjuctively known the *required R*
   (also known as the â€˜preconditionâ€™).

   | $R \Then{C} G$       | â‰ˆ | â€œGet goal $G$ using command $C$, by requiring $R$.â€ |

   This is only /reasonable/: We have some desired goal $G$ and we wish to form an
   imperative program =C= whose execution will ensure the goal $G$, but the
   construction of =C= may require some necessary provisos $R$.

** Example Specifications
   :PROPERTIES:
   :CUSTOM_ID: Hi
   :END:

 A /specification is an equation of a certain shape.
 /Programming/ is the activity of solving a specification
 for its unknown. Its unknown is called a /program/.

 One says â€œprogram =C= is specified by precondition $R$ and postcondition $G$â€
 whenever $R \Then{C} G$. One also says this is a /specification of =C=./

 #+begin_box C. A. R. Hoare
 Tony Hoare's 1969 landmark paper /An axiomatic basis for computer programming/
 proposed to define the meaning of programs by how they transform state
 (predicates on the program variables; i.e., stores). It defined $R \,\{C\}\, G$
 to mean /partial correctness/; whereas the modern notations $\{ R\}\, C\, \{G\}$
 and $R \Then{C} G$ denote /total correctness/, which has the additional
 requirement of the /termination/ of $C$.
 #+end_box

 *Programming* is solving the equation $R \Then{C} G$ in the unknown $C$;
 i.e., it is the activity of finding a â€˜recipeâ€™ that satisfies a given
 specification.  Sometimes we may write $R \Then{?} G$ and solve for â€˜?â€™.
 *Programming is a goal-directed activity: From a specification, a program is
 found by examining the /shape/ of its postcondition.*

 The notation $x : E$ is intended to communicate that we are looking at the
 expression $E$ with /unknown/ $x$ ---i.e., $x$ is the variable we are focusing on.
 For instance, is $xÂ² + b Â· x = 0$ a linear equation? Yes, if $b$ is the variable
 and $x$ is considered constant! In such a case, we are speaking of the equation
 $b : xÂ² + b Â· x = 0$. With this convention, the notation
 $R \Then{C} \vec{x} : G$ means that /only/ the names $\vec{x}$ should be
 considered â€˜program variablesâ€™ and all other variables should be treated
 as â€˜fixedâ€™ or â€˜constantâ€™ and so cannot appear in the program command $C$.

 However, this convention only reduces ambiguity about what variables can be
 meddled with; and so ghost variables are still required.  For instance, consider
 the example specification â€œset $z$ to its own absolute valueâ€, it is formalised
 with the help of a ghost variable: $z = Z \Then{?} z = |Z|$.

 #+begin_details Set q and r to the quotient and remainder of integer division of x â‰¥ 0 by y > 0
 \[ x â‰¥ 0 âˆ§ y > 0 \Then{?} q Â· y + r = x âˆ§ 0 â‡ r âˆ§ r < y \]

 The first conjunct states that the quotient times the denominator plus the
 remainder equals the numerator. The last two conjuncts bound the remainder: The
 remainder is at least 0 and is less than the denominator.
 #+end_details

 #+begin_details Set x to the integer square-root of N â‰¥ 0
 The integer square-root of $N$ is the largest integer whose square is at most
 $N$, and so the answer is \[ N â‰¥ 0 \Then{?} xÂ² â‰¤ N âˆ§ N < (x + 1)Â²\]

 Note that the postcondition is equivalent to $x â‰¤ \sqrt{N} < x + 1$.
 #+end_details

 #+begin_details Set x to the largest integer that is a power of 2 and is at most N
 \[N â‰¥ 0 \Then{?} (âˆƒ i : â„• â€¢ x = 2^i) âˆ§ x â‰¤ N âˆ§ N < 2 Â· x\]

 The first conjunct states that $x$ is a power of 2. The second states that $x4
 is at most $N4, while the third states that the next power of 2 exceeds $N$.

 This is /the integer-logarithm base-2/ problem; the postcondition is equivalent to
 $âˆƒ i : â„• \,â€¢\, i = \lg_2 x \,âˆ§\, i â‰¤ \lg_2 N < 1 + i$.
 #+end_details

** What is the definition of $R \Then{C} G$?
   :PROPERTIES:
   :CUSTOM_ID: What-is-the-definition-of-R-C-G
   :END:

   It is defined by the /shape/ of the possible commands =C=.

   For example, the *<<<sequential>>> command* =Câ‚ â® Câ‚‚= is executed by first
   executing =Câ‚= then by executing =Câ‚‚=. As such, to obtain a goal state $G$, we
   may construct a partial program $Câ‚‚$ which in-turn requires an intermediary
   state $I$; then to establish state $I$, we may construct a program =Câ‚= which
   requires a state $R$. This is *divide and conquer*.

   \[
   \Rule[Sequencing]{R \;â‡’[Câ‚]\;\; I \And I \;â‡’[Câ‚‚]\;\; G}{R \;â‡’[Câ‚â®Câ‚‚]\;\; G}
   \]

   Notice that this is similar to $\Rule{x â‰¤ y \And y â‰¤ z}{x â‰¤ z}$ but unlike
   inclusions â€˜â‰¤â€™ which are either true or false, the relationships â€˜$â‡’[C]$â€™ are
   /parameterised/ by commands $C$ and so it's important to *remember* which
   commands /witnessed/ the relationship. To see the similarity even closely, let
   us write $m â‰¤â‚–\, n \;â‰¡\; m + k = n$, so that we have â€œa witness to the
   inclusionâ€; then the â‰¤-transitivity becomes suspiciously similar to the
   sequencing rule â€¦  \[ \Rule[Addition]{x â‰¤_a y \And y â‰¤_b z}{x â‰¤_{a+b} z} \]
   The reason that we usually use â€˜â‰¤â€™ instead of â€˜â‰¤â‚–â€™, because if $x â‰¤â‚–\, y$
   then there can only be one such $k$, namely $k = y - x$, and so the simpler
   â€˜â‰¤â€™ simply marks whether such a (unique) $k$ exists or not. In contrast,
   infinitely many programs =C= can be used to establish relationships $R â‡’[C] G$
   and this is what makes programming interesting!

** We define assignment using textual substitution
   :PROPERTIES:
   :CUSTOM_ID: We-define-assignment-using-textual-substitution
   :END:

   The <<<assignment statement>>>
   ~x := E~ evaluates expression =E= (which we assume â€œnever crashesâ€)
   and stores the result in variable =x=.
   The statement is read â€œ ~x~ becomes ~E~ â€.

   When your goal $G$ mentions a variable $x$,
   then your goal state $G$ could be established
   by using the command ~x := E~ for some choice of expression $E$
   provided you know that $G[x â‰” E]$ is true to begin with.

   For example, suppose we want to get the goal $i < 10$ /after/ executing ~i := 2
   Â· i~. Then, this could only happen if /beforehand/ we had $(i < 10)[i â‰” 2 Â· i]$;
   i.e., $2 Â· i < 10$; i.e., $i < 5$. Hence, starting in a state in which $i <
   5$, the execution of ~i := 2 Â· i~ is guaranteed to terminate in a state with $i
   < 10$.

   This is summarised as follows.
   \[
   \mathsf{Definition\, of\, Assignment:}\quad G[x:= E] \Then{x:= E} G
   \]

   The â€˜backwardsâ€™ nature of this rule ---working backward from the
   postcondition--- is most easily understood via examples of the video lectures
   that start with the goal and work back to the precondition.  See the first 5
   minutes of this [[https://youtu.be/JxRZC2UMJb0][this video lecture]] for a taste of â€˜working backwardsâ€™.  The
   examples below also aim to demonstrate this idea.

   This definition is also used when we allow /multiple assignments/
   ~xâ‚, xâ‚‚, â€¦, xâ‚™ := Eâ‚, Eâ‚‚, â€¦, Eâ‚™~ ---which, for distinct variables $xáµ¢$,
   is executed by first evlauating all the expressions $Eáµ¢$
   to yield values, say, $váµ¢$; then assigning $vâ‚$ to $xâ‚$, â€¦, $vâ‚™$ to $xâ‚™$.
   - Note that all expressions are evaluated before any assignments are
     performed. E.g., ~x, y := y, x~ is a program that *swaps* the values of two
     variables, ~x~ and ~y~:
     | ~x = X âˆ§ y = Y Â Â â‡’[x, y := y, x]Â Â x = Y âˆ§ y = X~ |

   - Python, for example, allows multiple assignments.

** Let's show a calculation!
   :PROPERTIES:
   :CUSTOM_ID: Let's-show-a-calculation
   :END:

   First, since *programming is a goal-directed activity*, let us begin with the
   goal and use that to arrive at a required precondition.
   How? Just as $y â‰¥ x \;â‰¡\; x â‰¤ y$, we define
   \[G \If{C} R \quadâ‰¡\quad R \Then{C} G \]
   Using this â€˜turned aroundâ€™ (/converse/) notation, we may begin with the goal.

   :Swap:
   Here's a common problem: We want to have $x = Y âˆ§ y = X$, how do we achieve this?
   \begin{calc}
   x = Y âˆ§ y = X
   \step[\If{x, y := y, x}]{ Assignment rule }
   (x = Y âˆ§ y = X)[x, y â‰” y, x]
   \step[=\quad\qquad\qquad\;\;]{ Substitution }
    y = Y âˆ§ x = X
   \end{calc}
   :End:

   # LADM exercise 1.11.b
   Let's use the assignment rule to *find a necessary precondition*:
   When will the assignment ~x := x - 1~ ensure the goal state $xÂ² + 2 Â· x = 3$?
   \begin{calc}
     xÂ² + 2 Â· x = 3
   \step[\If{x := x - 1}]{ Assignment rule }
     (xÂ² + 2 Â· x = 3)[x â‰” x - 1]
   \step[=\qquad\qquad\quad]{ Substitution }
     (x - 1)Â² + 2 Â· (x - 1) = 3
   \step[=\qquad\qquad\quad]{ Arithmetic: Perform multiplications }
     (xÂ² - 2 Â· x + 1) + (2 Â· x - 2) = 3
   \step[=\qquad\qquad\quad]{ Arithmetic: Collect like terms }
     xÂ² - 1 = 3
   \step[=\qquad\qquad\quad]{ Arithmetic: Square roots }
     x = Â±2
   \end{calc}
   Hence, if $x$ is positive or negative 2, then the assignment
   ~x := x - 1~ will ensure that $x$ satisfies the predicate state $xÂ² + 2 Â· x =
   3$
   ---incidentally, after the assignment $x$ will have its value being
   positive 1 or negative 3, which are both solutions to the equation
    $xÂ² + 2 Â· x = 3$.

    kbd:Begin_Warning [[color:red][*At the moment, in CalcCheck notebooks, to keep things
   clear, we are using =R â‡’[C] G= and /not (yet)/ using ~G â‡[C] R~.*]] See the first 5
   minutes of this [[https://youtu.be/JxRZC2UMJb0][this video lecture]] for a taste of â€˜working backwardsâ€™.
   kbd:End_Warning

 When $I \;â‡’[C]\;\; I$, one says that â€œstate $I$ is *<<<invariant>>>* for
 command $C$â€ or that â€œ$C$ /maintains/ $I$â€.

** Calculating Assignments
   :PROPERTIES:
   :CUSTOM_ID: Calculating-Assignments
   :END:

   It is /often/ possible to *calculate* an assignment statement that satisfies a
   given precondition-postcondition specification.
   Many examples involve a property that is to be maintained invariant whilst
   progress is made by incrementing (or decrementing) a counter.

   E.g., suppose we want to increment =k= while maintain the value of the sum =j +
   k=. How should we alter =j=? Rather than guess and check, we can /calculate/! Let
   =S= be the value of the sum (whatever it may) and let =ğ’³= be the unknown
   assignment to =j=; then our goal is to â€œsolve for ğ’³â€ in
   \[j + k = S \Then{j,k := ğ’³, k+1} j+k = S\] Let's begin calculating!
   \begin{calc}
       j + k = S
   \step[\If{j, k := ğ’³, k + 1}]{Assignment rule}
       (j + k = S)[j, k â‰” ğ’³, k + 1]
   \step[=\hspace{7em}]{ Substitution }
       ğ’³ + k + 1 = S
   \step[â‡\hspace{7em}]{ Strengthening }
       ğ’³ + k + 1 = S = j + k
   \step[=\hspace{7em}]{ Arithmetic: Substitution and +-cancellation }
       ğ’³ + 1 = j \;âˆ§\; j + k = S
   \step[=\hspace{7em}]{ Subtraction }
       ğ’³ = j - 1 \;âˆ§\; j + k = S
   \end{calc}
   Hence, we have found a solution for the unknown assignment ğ’³,
   and so have /calculated/:
   \[ j + k = S \Then{j,k := j - 1, k+1} j+k = S \]

   Within the above calculation, we have /silently/ used the following law:
   \[
   \Rule[Strengthening]{Râ€² â‡’ R \And R \Then{C} G}{Râ€² \Then{C} G}
   \]

   Here is a more complicated example: Suppose we are computing the square $s$
   of a number $n$ *without* using multiplication or squaring ---only using
   addition!---, and we have just incremented $n$, how should we alter $s$ so as
   to /maintain/ its relationship to $n$? Exercise: Solve for the unknown
   assignment ğ’³ in \[ s = nÂ² \Then{s,n := ğ’³, n + 1} s = nÂ² \]
   # Solution: ğ’³ = s + n + n + 1

** Calculating expressions in assignments :ignore:
   :PROPERTIES:
   :CUSTOM_ID: Calculating-expressions-in-assignments
   :END:

 + Exercise: Solve for ğ’³ in $\true \Then{x := ğ’³} x = 4$.
   - The answer is â€˜obviousâ€™, but actually do the calculation
     to see that unknown expressions in an assignment can be found by
     calculation.
 + Exercise: Solve for ğ’³ in $0 â‰¤ x âˆ§ 0 < y \Then{q, r := ğ’³, x} 0 â‰¤ r âˆ§ q Â· y + r
   = x$
 + Exercise: Solve for ğ’³ in $q = a Â· c âˆ§ w  = cÂ² \Then{a, q := a + c, ğ’³} q = a *
   c$

 #+begin_box  Heuristic
 To determine an unknown expression in an assignment, calculate.
 #+end_box
** The Shapes of Programs
   :PROPERTIES:
   :CUSTOM_ID: The-Shapes-of-Programs
   :END:

 The activity of programming is the activity of solving a specification for its
 unknown, where this unknown is called a /program/. Programs are
 formulae of a certain shape.

 The simplest program is called =skip=, then we may â€˜sequentially composeâ€™ two
 programs =Câ‚= and =Câ‚‚= to obtain the program =Câ‚â® Câ‚‚=, and finally we have the
 /(multiple) assignment/ program ~x := E~ consisting of a list of distinct variables
 =x= and a corresponding list of expressions =E=. There are other shapes of programs
 and we will get to those in due time.

 #+begin_src math
Program ::=  skip
         |   Câ‚ â® Câ‚‚
         |   x := E
 #+end_src

 :Hide:
 Thus far we have defined $R \Then{C} G$ informally, we now turn to defining
 it /algebraically/ thereby placing the notion of program correctness firmly
 within predicate calculus: We shall have a formal method to calculate
 whether a program satisfies its specification.
 :End:

 #+begin_box Program Correctness Laws
 $$\begin{align*}
 \mathsf{Axiom,\; The\; Law\; of\; the\; Exluded\; Miracle} & & R \Then{C} \false \quadâ‰¡\quad Â¬ R \\
 \mathsf{Axiom,\; Conjunctivity} && R \Then{C} (Gâ‚ âˆ§ Gâ‚‚) \quadâ‰¡\quad \big(R \Then{C} Gâ‚\big) âˆ§ \big(R \Then{C} Gâ‚‚\big) \\
 \mathsf{Axiom,\; Skip\; Rule} && R \Then{\mathsf{skip}} G \quadâ‰¡\quad R â‡’ G \\
 \mathsf{Axiom,\; Sequence\; Rule} && R \Then{Câ‚} I \Then{Câ‚‚} G \quadâ‡’\quad R \Then{Câ‚â®Câ‚‚} G \\
 \mathsf{Axiom,\; Assignment\; Rule} && R \Then{x := E} G \quadâ‰¡\quad R[x â‰” E] â‡’ G \\ \hdashline
 \mathsf{Postcondition\; Weakening} && \big(R \Then{C} G) âˆ§ (G â‡’ Gâ€²) \quadâ‡’\quad R \Then{C} Gâ€² \\
 \mathsf{Precondition\; Strengthening} && (Râ€² â‡’ R) âˆ§ \big(R \Then{C} G) \quadâ‡’\quad Râ€² \Then{C} G \\  \hdashline
 \mathsf{Conjunction} && (Râ‚ \Then{C} Gâ‚) âˆ§ (Râ‚‚ \Then{C} Gâ‚‚) \quadâ‡’\quad (Râ‚ âˆ§ Râ‚‚) \Then{C} (Gâ‚ âˆ§ Gâ‚‚) \\
 \mathsf{Case\; Analysis} && (Râ‚ \Then{C} Gâ‚) âˆ§ (Râ‚‚ \Then{C} Gâ‚‚) \quadâ‡’\quad (Râ‚ âˆ¨ Râ‚‚) \Then{C} (Gâ‚ âˆ¨ Gâ‚‚) \\
 \end{align*}$$
#+end_box

 The skip rule tells us that the =skip= command is executed by doing nothing: When
 $R â‡’ G$, and we are in state $R$, then we get to state $G$ by doing nothing.

 Note: One then defines the equivalence of programs â€œup to specificationâ€:
 \[C = Câ€² \qquadâ‰¡\qquad \big(R \Then{C} G \quadâ‰¡\quad R \Then{Câ€²} G\big) \text{ for
 all } R, G \]
 Using this, one can show that =skip= is the unit of sequencing,
 and that it can be defined in terms of assignments.
 | ~C â® skip = C = skip â® C~ |
 | ~skip = (x := x)~         |

* Induction
  :PROPERTIES:
  :CUSTOM_ID: Induction
  :END:

How we prove a theorem $P\, n$ ranging over natural numbers $n$?

For instance, suppose the property $P$ is that using only 3 and 5 dollar bills,
any amount of money that is at-least 8 dollars can be formed.

Since there are an infinite number of natural numbers, it is not possibly to
verify $P\, n$ is true by /evaluating/ $P\, n$ at each natural number $n$.

#+begin_box Knocking over dominos is induction
The natural numbers are like an infinite number of dominoes ---i.e., standing
tiles one after the other, in any arrangement. Can all dominoes be knocked over?
That is, if we construe $P\, n$ to mean â€œthe /n/-th domino can be knocked overâ€,
then the question is â€œis $âˆ€ n â€¢ P\, n$ trueâ€. Then, clearly if we can knock over
the first domino, $P\, 0$, and if when a domino is knocked over then it also
knocks over the next domino, $P\, n â‡’ P\, (n + 1)$, then â€˜clearlyâ€™ all dominoes
will be knocked over. This â€˜basic observationâ€™ is known as /induction/.
#+end_box

#+begin_box Climbing a ladder is induction
The natural numbers are like an infinite ladder ascending to heaven.  Can we
reach every step, rung, on the ladder?  That is, if we construe $P\, n$ to mean
â€œthe /n/-th rung is reachableâ€, then the question is â€œis $âˆ€ n â€¢ P\, n$
trueâ€. Then, clearly if we can reach the first rung, $P\, 0$, and whenever we
climb to a rung then we can reach up and grab the next rung, $P\, n â‡’ P\, (n +
1)$, then â€˜clearlyâ€™ all rungs of the ladder can be reached. This â€˜basic
observationâ€™ is known as /induction/.
#+end_box

#+begin_box Constant functions are induction
A predicate $P : â„• â†’ ğ”¹$ is a function. When is such a function constantly the
value $\true$? That is, when is $âˆ€ n â€¢ P\, n = \true$?  Clearly, if $P$ starts
off being $\true$ ---i.e., /P 0/--- and it preserves truth at every step ---i.e.,
/P n â‡’ P (n + 1)/--- then /P n/ will be true for any choice of $n$.

That is, if we consider $(â„•, â‰¤)$ and $(ğ”¹, â‡’)$ as ordered sets and $P$ starts at
the â€˜topâ€™ of ğ”¹ ---i.e., /P 0 = true/--- and it is ascending ---i.e., /P n â‡’ P (n +
1)/--- and so â€˜never goes downâ€™, then clearly it must stay constantly at the top
value of ğ”¹. This â€˜basic observationâ€™ is known as /induction/.
#+end_box

For the money problem, we need to start somewhere else besides 0.

#+begin_box Principle of (â€œWeakâ€) Mathematical Induction
To show that a property $P$ is true for all natural numbers starting with some
number $n_0$, show the following two properties:
+ Base case :: Show that $P\, nâ‚€$ is true.
+ Inductive Step :: Show that whenever (the *inductive hypothesis*) $n$ is a
  natural number that such that $n â‰¥ nâ‚€$ and $P\, n$ is true, then $P\, (n + 1)$
  is also true.
#+end_box
For the money problem, we need to be able to use the fact that to prove $P\,
(n + 1)$ we must have already proven $P$ for all smaller values.

#+begin_box Principle of (â€œStrongâ€) Mathematical Induction
To show that a property $P$ is true for all natural numbers starting with some
number $n_0$, show the following two properties:
+ Base case :: Show that $P\, nâ‚€$ is true.
+ Inductive Step :: Show that whenever (the *inductive hypothesis*) $n$ is a
  natural number that such that $n â‰¥ nâ‚€$ and $P\, n_0, P\, (n_0 + 1), P\, (n_0 +
  2), â€¦, P\, n$ are true, then $P\, (n + 1)$ is also true.
#+end_box

These â€˜strengthâ€™ of these principles refers to the strength of the inductive
hypothesis. The principles are provably equivalent.

# (It is also a way to say that â„• has non-empty meets.)
#+begin_box The Least Number Principle ---Another way to see induction
Every non-empty subset of the natural numbers must have a least element,
â€˜obviouslyâ€™. This is (strong) induction.
# Possibly infinite!

#+begin_details Induction â‡’ LNP
\begin{calc}
(âˆƒ x â€¢ x âˆˆ S)
\step{ Double negation }
Â¬ Â¬ (âˆƒ x â€¢ x âˆˆ S)
\step{ De Morgan }
Â¬ (âˆ€ x â€¢ x âˆ‰ S)
\step{ Strong induction }
Â¬ (âˆ€ n âˆ£ (âˆ€ m : â„• âˆ£ m < n â€¢ m âˆ‰ S) â€¢ n âˆ‰ S)
\step{ De Morgan and double negation }
(âˆƒ n âˆ£ (âˆ€ m : â„• âˆ£ m < n â€¢ m âˆ‰ S) â€¢ n âˆˆ S)
\step{ Definition of â€˜leastâ€™ }
\text{S has a least element}
\end{calc}

# neato observation
#
# 0 âˆˆ S âˆ¨ (âˆƒ n âˆ£ (âˆ€ m : â„• âˆ£ m < n â€¢ m âˆ‰ S) â€¢ n âˆˆ S))
# \stepmany{ \line{Since 0 is the least number, if 0 âˆˆ S then S would have a least
# element.}
# \line{The right disjunct says n âˆˆ S is a least element.}
# \line{(â‡’) In both cases, S has a least element.}
# \line{Conversely, (â‡), if S has a least element,}
# \line{then either it is 0 or the right disjunct is satisfied. }
# }
# \text{S has a least element}
#
#+end_details

#+begin_details LNP â‡’ Induction
\begin{calc}
  (âˆ€ n â€¢ P\, n)
\step{ Double negation }
  Â¬ Â¬ (âˆ€ n â€¢ P\, n)
\step{ De Morgan }
  Â¬ (âˆƒ n â€¢ Â¬ P\, n)
\step{ Negation applied to both sides of LNP }
Â¬ (\text{there is a least n with } Â¬ P\, n)
\step{ Formalise â€˜leastâ€™ }
Â¬ (âˆƒ n âˆ£ (âˆ€ m : â„• âˆ£ m < n â€¢ Â¬ (Â¬ P\, n)) â€¢ Â¬ P\, n)
\step{ De Morgan and double negation }
(âˆ€ n âˆ£ (âˆ€ m : â„• âˆ£ m < n â€¢ P\, n) â€¢ P\, n)
\end{calc}
#+end_details

*Application of LNP to showing that algorithms terminate*:
In particular, every decreasing non-negative sequence of integers
$râ‚€ > râ‚ > râ‚‚ > â‹¯$ must terminate.
#+end_box


:Hide:
The money problem can now be proven by showing
$P\,8, P\, 9\, P\, 10\, P\, 11, P\, 12$ are all true,
then showing $P\, n$ for $n â‰¥ 12$ is true by using the inductive hypothesis
on $n - 3 â‰¥ 12 - 3 = 8$ and so $n = (n - 3) + 3$ and so $n$ can be formed
using 3s and 5s.

#+begin_details Exercises
1. the sum of the first $n$ natural numbers is $n Â· (n + 1) / 2$
2. using only 3 and 5 dollar bills, any amount of money that is at-least 8
   can be formed
3. $7^n + 5$ is divisible by 3
#+end_details
:End:

* Number Theory
  :PROPERTIES:
  :CUSTOM_ID: Number-Theory
  :END:

# =# _-_= is the gateway from â„• to â„¤,
# and =_/_= is the gateway from â„¤ to â„š.
# But what are these operations by themselves /without/ looking at the extended
# domains? Semicomplements and residuals (respectively)!
# ( these are â€˜truncated substitituion and divisionâ€™ )

   #+begin_latex-definitions
\newcommand{Law}[3][]{ #1\;\;\textbf{#2:}\quad #3 }
\def\abs{\mathsf{abs}\,}
   #+end_latex-definitions

  Number Theory is concerned with the properties of whole numbers, such as 0,
  42, and 1927 rather than fractional numbers such as 0.3, Ï€, or 1/3.

  Division is one of the most important concepts of number theory.  It is a
  partial order on the naturals and its infimum, meet, is formed constructively
  using Euclid's Greatest Common Divisor algorithm.

  # Algorithms may provide constructive solutions to problems but they can also be
  # used to /reason/ about the resulting constructions.

#  --------------------------------------------------------------------------------

** COMMENT Integer Division
   :PROPERTIES:
   :CUSTOM_ID: Integer-Division
   :END:

  The integer division of $P$ by $Q$, denoted $P Ã· Q$, is specified by
  the following Galois connection:
  \[ k Ã— Q â‰¤ P \quadâ‰¡\quad k â‰¤ P Ã· Q \]

  Here, $Q$ is a natural number (otherwise, the inclusion might need to change)
  and $P$ is any integer.

  Replacing $k$ by $P Ã· Q$ yields,
  \[ (P Ã· Q) Ã— Q â‰¤ P \]

  Replacing $k$ by 0 yields,
  \[ 0 â‰¤ P \quadâ‰¡\quad 0 â‰¤ P Ã· Q \]

  Using the law of indirect equality, one can show
  \[ (a Ã· b) Ã· c \;=\; a Ã· (c Ã— b) \]

** COMMENT Deriving an imperative algorithm
   :PROPERTIES:
   :CUSTOM_ID: Deriving-an-imperative-algorithm
   :END:

  The characterisation of integer division
  could also serve as a specification for an algorithm
  that actually computes $x = P Ã· Q$.

  \[ k Ã— Q â‰¤ P \quadâ‰¡\quad k â‰¤ x \]

  If a solution $x$ exists to this equation, then it is the largest integer with
  $x Ã— Q â‰¤ P$ (just take $k â‰” x$) and so is unique.
  Moreover, taking $k â‰” x + 1$ yields $Â¬ (x + 1) Ã— Q â‰¤ P$. Hence, we have the
  problem:
  \[ ? \Then{?} x Ã— Q â‰¤ P \;âˆ§\; Â¬ (x + 1) Ã— Q â‰¤ P\]

  Since the first conjunct is easily truthified (by $x â‰” 0$ and additionally
  requiring $0 â‰¤ P$), we take it to be the invariant and take the negation of
  the second conjunct as the loop guard, thereby obtaining.

  #+begin_latex-definitions
\def\While#1{ \mathsf{while}\,#1\,\textbf{:}\, }
  #+end_latex-definitions

  \[ 0 â‰¤ P \Then{x := 0â® \While{(x + 1) Ã— Q â‰¤ P} ?} x Ã— Q â‰¤ P \;âˆ§\; Â¬ (x + 1) Ã— Q â‰¤ P\]

  Now ~?~ could only be an assignment to ~x~, and so we calculate such an assignment
  /with the aim/ of preserving the invariant, wherein we know the loop guard also
  holds.  That is, we want to solve for $A$ in the following equation.

  \[ (x + 1) Ã— Q â‰¤ P \;âˆ§\; x Ã— Q â‰¤ P \Then{x := A} x Ã— Q â‰¤ P \]

  By the assignment rule, it is clear that $A = x + 1$ works.

  Hence, we have
 \[ 0 â‰¤ P \Then{x := 0â® \While{(x + 1) Ã— Q â‰¤ P} x := x + 1} x Ã— Q â‰¤ P \;âˆ§\; Â¬ (x + 1) Ã— Q â‰¤ P\]

 To prove that the algorithm terminates, we have to define a /bound function/,
 which is a natural-number-valued function of the program variables that
 measures the size of the problem to be solved.  A guarantee that the value of
 such a bound function is always decreased at each iteration is a guarantee that
 the number of times the loop body is executed is at most the initial value of
 the bound function.

 It seems we can take $P - x$ to be the bound function.
 However, for this function to be bounded below, we require
 $0 < Q$.

 In summary,
 \[ 0 â‰¤ P \;âˆ§\; 0 < Q \Then{x := 0â® \While{(x + 1) Ã— Q â‰¤ P} x := x + 1} x Ã— Q â‰¤ P \;âˆ§\; Â¬ (x + 1) Ã— Q â‰¤ P\]

 Notice that additional assumptions on $P$ and $Q$ emerged in the process of
 constructing a correct program.

 *Improvement:* As it stands, the algorithm computes $x + 1$ twice, let us /aim to
 simplify the loop guard, possibly by introducing a new variable/.  Since
 $(x + 1) Ã— Q â‰¤ P \quadâ‰¡\quad Q â‰¤ P - x Ã— Q$ we may introduce a variable $r$
 whose purpose is to maintain the value $P - x Ã— Q$; it's purpose is guaranteed
 if we make it part of the invariant. This results in:
 \[ 0 â‰¤ P \;âˆ§\; 0 < Q \Then{x, r := 0, Pâ® \While{(x + 1) Ã— Q â‰¤ P} x, r := x + 1,
 B} x Ã— Q â‰¤
 P \;âˆ§\; Â¬ (x + 1) Ã— Q â‰¤ P \;âˆ§\; r = P - x Ã— Q \]

 To satisfy the invariant, $x Ã— Q â‰¤ P \;âˆ§\; r = P - x Ã— Q$, we are forced to
 initially set $r$ to $P$. Then we update $r$ via the yet unkwon $B$
 so that the invariant is preserved:
 \[ (x + 1) Ã— Q â‰¤ P \;âˆ§\; x Ã— Q â‰¤ P \;âˆ§\; r = P - x Ã— Q \Then{x, r := x + 1, B}
 x Ã— Q â‰¤ P \;âˆ§\; r = P - x Ã— Q \]

 The assignment rule quickly shows that $B = r - Q$.

 In summary, \[ 0 â‰¤ P \;âˆ§\; 0 < Q \Then{x, r := 0, Pâ® \While{(x + 1) Ã— Q â‰¤ P} x,
 r := x + 1, r - Q} x Ã— Q â‰¤ P \;âˆ§\; Â¬ (x + 1) Ã— Q â‰¤ P \;âˆ§\; r = P - x Ã— Q \]

** COMMENT Deriving a recursive algorithm
   :PROPERTIES:
   :CUSTOM_ID: Deriving-a-recursive-algorithm
   :END:

   The characterisation of integer division as a Galois connection is so
   versatile that it can also be used directly to obtain a recursive algorithm
   for the definition of the new operation.

   The first step is to express the new operation in terms
   of itself but with an argument reduced.

   \begin{calc}
   k â‰¤ P Ã· Q
   \step{ definition of Ã·, using proviso $Q > 0$ }
   k Ã— Q â‰¤ P
   \step{ cancellation; aiming to reduce an argument }
   k Ã— Q - Q â‰¤ P - Q
   \step{ distributivity }
   (k - 1) Ã— Q â‰¤ P - Q
   \step{ definition of Ã·, using proviso $P - Q > 0$; i.e., $P â‰¥ Q > 0$ }
    k - 1 â‰¤ (P - Q) Ã· Q
   \step{ arithmetic }
    k â‰¤ (P - Q) Ã· Q + 1
   \end{calc}

   Hence, by indirect equality, we have shown
   \[ P Ã· Q \;=\; (P - Q) Ã· Q + 1  \quadâ‡\quad  Q â‰¤ P \]

   We have found a reduction in an argument, but it is conditional.  So let us
   consider the negation of this condition so as to cover all possible cases.
   Supposing, $P < Q$, we calculate:
   \begin{calc}
   k â‰¤ P Ã· Q
   \step{ definition of Ã· }
   k Ã— Q â‰¤ P
   \step{ transitivity using assumption }
   k Ã— Q < Q
   \step{ cancellation using $0 < Q$ }
   k < 1
   \step{ integers }
   k â‰¤ 0
   \end{calc}

  Hence, by indirect equality, we have shown
  \[ P Ã· Q \;=\; 0  \quadâ‡\quad  P < Q \]

  Putting these two cases together, we have the following
  recursive definition for natural $P$ and positive $Q$:
  #+begin_src math
P Ã· Q  |  P < Q  =  0
P Ã· Q  |  Q â‰¤ P  =  (P - Q) Ã· Q + 1
  #+end_src

  This function clearly terminates since an argument, $P$, is being reduced at
  each recursive call and stops when $P < Q$.

** Division Algorithm :ignore:
   :PROPERTIES:
   :CUSTOM_ID: Division-Algorithm
   :END:

#+begin_box The Division Algorithm
For any integers $a$ and $b â‰  0$, there are unique
integers $q, r$ with $a = b Â· q + r$ and $0 â‰¤ r < \abs b$.

That is, you /can/ divide one integer by another.

Here is the algorithm for the positive /b/ case.
#+begin_src math
{ a â‰¥ 0  âˆ§  b > 0 }
q, r := b, 0;
while r â‰¥ m:  q, r := r - a, q + 1
{ a = b Â· q + r  âˆ§  0 â‰¤ r < b }
#+end_src

We write $q, r$ as $a Ã· b$ and /a mod b/.

+ /a Ã· b/ : The quotient of /a/ and /b/ on dividing /a/ by /b/.

  Division is successive subtraction:
  /a Ã· b/ is the number of times we can subtract /b/ from
  /a/ /before/ we hit a negative number; i.e., /a Ã· b/ is
  the largest integer /k/ with /a - k Ã— b â‰¥ 0/
  i.e., /k Ã— b â‰¤ a/ and so division can be defined in terms of multiplication.
  - For example, to find /27 Ã· 7/ we look for the largest /k/ with
    /27 - 7k â‰¥ 0/: We try /k=1/, then /k=2/, ..., to eventually find that /k=4/
        gives a negative result and so /k=3/ is the answer;
        and the /remainder/ /is r = a - (a Ã· b) = 27 - 7Ã—3 = 2/.

  :Well_order:
  - By the well-ordering principle,
    assuming T = {a - bk â™ a - b k â‰¥ 0 } is non-empty,
    there is a least element in T, call it r.
    Then r = a - bq â‰¥ 0 for some q
    and so 0 â‰¤ r. Moreover,

      r â‰¥ b
    = r - b â‰¥ 0
    = a - bq - b â‰¥ 0
    = a - b(q+1) â‰¥ 0
    = a - b(q+1) âˆˆ T
    = r > a - b(q+1) âˆˆ T
    = r not the least element of T

    Hence, r < b.
    :End:

+ /a mod b/: The remainder of /a/ and /b/ on dividing /a/ by /b/.
  - When /a mod b = 0/, one says â€œb divides aâ€ or â€œa is a multiple of bâ€.
    Equivalently, /b divides a â‰¡ (âˆƒ c â€¢ a = b Â· c)/.
#+end_box

# If you refer to q as the quotient and r as the remainder, the theorem makes a lot of sense.
# If you divide a by d, you get quotient q and remainder r. The theorem says the exist and are unique.
# When we need these values, we will write a div b=q and a mod b=r.

For instance, if we take $b = 3$, then the theorem says every integer is of the
form $3 Â· q, 3 Â· q + 1$, or $3 Â· q + 2$.

If today is Thursday, what weekday is 90 weekdays from today?  A week has 7 days
and so by the division algorithm we have /90 = 12 Â· 7 + 6/. Since a weekly cycle
(each 7 days) leaves us on the same weekday (today, Thursday), after 12 Â· 7 days
it will again be Thursday, and so the weekday in 90 days is the same as the day
6 days from now; which happens to be Wednesday.

** The Division Relation :ignore:
   :PROPERTIES:
   :CUSTOM_ID: The-Division-Relation
   :END:

:MORE:
+ Absorption: m âˆ£ (n + m) â‰¡ m âˆ£ n
+ prime p â‰¡ (âˆ€ i â™ 1 â‰¤ i < p â€¢ p âˆ¤ i)
+ prime p â‡’ (p âˆ£ (Î  i â€¢ B) â‰¡ (âˆƒ i â€¢ p âˆ£ B))
+ Euclid's Theorem: m âˆ£ n â‡ m âˆ£ pÂ·n âˆ§ m âˆ‡ p = 1
+ prime p â‡’ (m âˆ‡ p = 1 â‰¡ p âˆ¤ m)
+ y âˆ£ z â‡’ (xy âˆ£ z â‡’ x âˆ£ {z \over y})  .... is this true, what of the inner-most converse?
+ prime p âˆ§ 1 â‡ i < p â‡’ p âˆ£ \binom{p}{i} = {p! \over i! Ã— (p-i)!}
+ composite m â‰¡ (âˆƒ d â™ 2 â‰¤ d < m â€¢ d âˆ£ m)
  - composite (m + n) â‰¡ m âˆ£ n  provided 2 â‰¤ m and 1 â‰¤ n.
+ composite m â‰¢ prime m â‰¢ m = 1 â‰¢ m = 0  ... ?
+ The gcd of at least two consecutive numbers is equal to 1.
  - m âˆ‡ (m + 1) = 1
  - (âˆ‡ i â™ 0 â‰¤ i < N â€¢ m + i) = 1  â‡  1 â‰¤ N

--------------------------------------------------------------------------------
:END:

   The division relation is the relation on integers defined to be the converse
   of the â€œis-a-multiple-ofâ€ relation:
   \[\Law{Definition of Divisibility}{m âˆ£ n  \,\;â‰¡\;\, (âˆƒ k : â„¤ â€¢ n = k Ã— m)}  \]

   # Although, the notation $m âˆ£ n$ is more common, the asymmetric â€˜â•²â€™ symbol is
   # more suggestive.  In /Concrete Mathematics/, p102, it is pointed out that
   # vertical bars are overused and $m â•² n$ (read â€œm under nâ€) gives an impression
   # that $m$ is the denominator of the implied ratio ($k = {n \over m}$), and in
   # the order sense it implies that, well, $m$ is â€˜underâ€™ (or â€œat mostâ€) $n$.

   | d âˆ£ m                          |
   | â‰ˆ d is a divisor (factor) of m |
   | â‰ˆ m is a multiple of d         |

   Be careful:
   | ${n \over m},\; n/m$ | real division         | a number      |
   | $n Ã· m$              | integer division      | a number      |
   | $m âˆ£ n,\; m âˆ£ n$     | divisibility relation | a proposition |

   Since multiplication distributes over addition, divisibility is (almost)
   preserved by linear combination.

   \[ \Law{Divisibility preserves semi-linear
   combinations}{k âˆ£ x \;âˆ§\; k âˆ£ y \quadâ‰¡\quad k âˆ£ (x + a Ã— y) \;âˆ§\; k âˆ£ y} \]

   # That is, k is a factor of x and y precicely when it is a factor of
   # x +ay and y.

   That is,

   \[ \Law{Divisibility preserves semi-linear combinations}{
   k âˆ£ y \quadâ‡’\quad \big(k âˆ£ (x + a Ã— y) \;â‰¡\; k âˆ£ x\big)}\]

   + Often this theorem is
     used in the /simpler, less generic/, shape â€œsince /k/ divides /x/ and /y/, it also
     divides their sumâ€.
     - If b and c are multiples of a, then so is their linear combination.
       \[ \Law{Divisibility of Linear Combinations}{a âˆ£ b  âˆ§  a âˆ£ c  â‡’  a âˆ£ (b Ã— x + c
        Ã— y)} \]

        In particular, a âˆ£ (b Â· x) and a âˆ£ (b Â± c).

     - If b is a multiple of a, then for b+c to also be a multiple, we need c to be a
       multiple of a.
       \[\Law{Cancellation}{a âˆ£ b  âˆ§  a âˆ£ (b + c)  â‡’  a âˆ£ c}\]

     - Application: Do there exist integers /x, y, z/ with
       /6x + 9y + 15z = 107/?

       :Soln:
       6,9,15 clearly have 3 as a common divisor
       and so 3 must divide their linear combination
       3 âˆ£ (6x + 9y + 15z)
       = 3 âˆ£ 107

       but 3 âˆ£ 107 â‰¡ false, hence, no such integers could exist.
       :End:

     - Application: Is there an integer n with 7 âˆ£ (nÂ² + n + 1) and 7 âˆ£ (n + 1)?
       # No, otherwise 7 âˆ£ (nÂ² + n +1 - n(n+1)) = 1 and so 7 = 1 and so false.

     - Application: If m âˆ£ 4n+7 and m âˆ£ 3n+5, then m = 1.
       # m âˆ£ (3Â·(4n+7) - 4Â·(3n+5)) = 1 and so m = 1.

     # m âˆ£ (a + c) and m âˆ£ (b + c) then m âˆ£ (a + b).

   If b is a multiple of a, then cÃ—b is a multiple of cÃ—a;
   conversely if c is not-zero, we can divide-it-out.
   \[ \Law{Isotonicity of Multiplication, â€œCancellationâ€}{c â‰  0 â‡’ (a âˆ£ b â‰¡ (c Ã— a) âˆ£ (c Ã— b))} \]
   \[ \Law{Monotonicity of Multiplication}{m âˆ£ a âˆ§ n âˆ£ b â‡’ (m Ã— n) âˆ£ (a Ã— b)}\]
   \[ \Law{Monotonicity of Absolute Value}{m âˆ£ n â‡’ \abs m âˆ£ \abs n}\]

   Divisibility is invariant under additive inverse,
   \[\Law{Divisibility is invariant under additive inverse}{ m âˆ£ n \quadâ‰¡\quad m âˆ£ (-n)} \]

   Since multiplication has a unit and is associative, divisibility is reflexive
   and transitive. That is, divisibility is a /preorder/ on the integers,
   and an /order/ on the naturals; moreover, it has 0 as the greatest element and 1 as the smallest.
   \[ \Law{Transitivity of divisibility}{m âˆ£ n âˆ£ k \quadâ‡’\quad m âˆ£ k} \]
   ( When a number is divisible by another number,
   it is also divisible by the factors of the number. )

   \[ \Law{Quasi-antisymmetry of divisibility}{m âˆ£ n \;âˆ§\; n âˆ£ m \quadâ‰¡\quad \abs n = \abs m} \]
   Indeed, 7 âˆ£ -7 and -7âˆ£7 but 7 â‰  -7, instead abs 7 = abs (-7).
   :Proof:
     m âˆ£ n âˆ£ m
   = n = sm âˆ§ m = tn
   â‡’ m = stm
   â‡’ 1 = st,  provided m â‰  0 and so can be cancelled
   â‡’ abs s = abs t = 1, since we're working with integers
   â‡’ m = tn = (Â±1)n
   â‡’ abs m = abs n

   The case with m = 0 is just the bottom of divisibility (inlined) and so n = 0
   and so abs m = abs n.
   :End:

   Multiplication interacts nicely with divisibility:
   \[ \Law{Divisibility of multiples}{ m âˆ£ (c Ã— m)} \]

   From this, we may easily obtain the following, in order,
   \[ \Law{Reflexivity of divisibility}{ m âˆ£ m} \]
   \[ \Law{Top of divisibility}{m âˆ£ 0} \]
   \[ \Law{Bottom of divisibility}{1 âˆ£ m} \]

   Some texts insist that /m â‰  0/ for the notation /m âˆ£ n/ to be meaningful.
   However, if /m = 0/ and /m âˆ£ n/ then that means there is a /q/ with /n = q Â· m = q Â·
   0 = 0/ and so /n = 0/. Hence, the only divisibility statement we can make of the
   shape /0 âˆ£ n/ is really just /0 âˆ£ 0/.  This observation is captured by the
   following theorem. (Note: /0 âˆ£ n/ does not mean we are dividing by 0!
   Divisibility, â€˜âˆ£â€™, is defined in-terms of multiplication ---not division!)

   Then, using antisymmetry,
   \[ \Law{Bottom of divisibility}{n âˆ£ 1 â‰¡ \abs n = 1} \]
   \[ \Law{Top of divisibility}{0 âˆ£ n â‰¡ n = 0} \]

   When a number is a factor of another number, it is also a factor
   of any multiple of that number.
   - For instance, 4 âˆ£ 36 â‡ 4 âˆ£ (3 Ã— 12) â‡  4 âˆ£ 12 â‡ 4 âˆ£ (3 Ã— 4) â‡ 4 âˆ£ 4 â‡ true.
   \[ \Law{âˆ£-Weakening,Strengthening}{k âˆ£ n â‡’ k âˆ£ (m Ã— n)} \]
   :Proof:
      k âˆ£ n
   â‡’ n = kq
   â‡’ mÃ—n = kÃ—qÃ—m
   â‡’ mÃ—n = k Ã— qâ€²
   â‡’ k âˆ£ mÃ—n
   :End:

   Another fact,
   \[ \Law{Sufficient Case Analysis}{a âˆ£ b  âˆ¨  a âˆ£ c  â‡’  a âˆ£ (b Ã— c)} }\]

   Later, one can show an approximate converse.

   Since for /positive/ integers /m, n/ we have that $m â‰¤ m Ã— n$, we have:
   \[ \Law{size-order inclusion}{m > 0 âˆ§ n > 0 \quadâ‡’\quad m âˆ£ n â‡’ m â‰¤ n} \]
   :Proof:
   If m > 0 and n > 0
   then m âˆ£ n â‡’ n = mq â‡’ q > 0 â‡’ q â‰¥ 1 â‡’(monotonciity of multiplication) mq â‰¥ m
   â‡’ n â‰¥ m.
   :End:
   ( Note that this is not true for n=0, since, eg, 99 âˆ£ 0 but 99 â‰° 0. )

   As such,
   \[ m âˆ£ n \quadâ‡’\quad \abs m â‰¤ \abs n \]

   We can also compare divisibility and subset inclusion.
   Let $Î“ x = \{m â™ m âˆ£ x \}$ be the set of all of the multiples of $x$;
   then \[Î“ n âŠ† Î“ m \quadâ‰¡\quad m âˆ£ n \text{ (note the reversal) } \]
   ( Many abstract algebra books write $(x)$ for $Î“ x$
   and then use $(aâ‚€, aâ‚, â€¦, aâ‚™)$ to mean $aâ‚ âˆ‡ â‹¯ âˆ‡ aâ‚™$. )

   \[ \Law{divisibility-and-precise-division}{m âˆ£ n \quadâ‰¡\quad (n / m) Ã— m = n} \]
   That is, m âˆ£ n precisely when n / m is an integer;
   i.e., when n mod m = 0.

   --------------------------------------------------------------------------------

   How many numbers below n are divisible by m?
   :Soln:
   I.e., #{d : â„¤ â™ d â‰¤ n âˆ§ m âˆ£ d} = ?

   Now,

     d â‰¤ n âˆ§ m âˆ£ d
   ={ âˆ£ includes â‰¤ }
     m â‰¤ d â‰¤ n âˆ§ d âˆ£ m
   ={ All numbers divisible by m are of the form m Ã— q }
     m â‰¤ m Ã— q â‰¤ n
   ={ arithmetic }
     1 â‰¤ q â‰¤ n / m

   Thus, there are n Ã· m many such integers.
   :End:

#   --------------------------------------------------------------------------------

  :congruence:
  |   | $a =_m b$                                           |
  | â‰ˆ | /a is congruent to b modulo m/                        |
  | â‰ˆ | /a and b have the same remainder after division by m/ |
  | â‰ˆ | $m âˆ£ (a - b)$                                        |

  $$\def\mod{\,\mathsf{mod}\,}$$
  Indeed, $a =_m b \quadâ‰¡\quad a \mod m = b \mod m$.

  If a =_m b and c =_m d, then
  - a Â± c =_m b Â± d
  - a Â· c =_m b Â· d

  As such, we can do â€œmodular arithmeticâ€; and it can be used to show the
  validity of a number of common divisibility tests.

  Also: If c Ã— a =_m c Ã— b  âˆ§  c âˆ‡ m = 1, then a =_m b
  [ cf the cancellation lemma: m âŠ¥ z  and m âˆ£ yz implies m âˆ£ y
   and m âŠ¥ z means z is invertible mod m. ]

  + Casting out nines: An integer is divisible by 9 iff the sum
    of its digits is divisible by 9.


     N mod 9
    ={ express N in base ten, with digits dáµ¢ âˆˆ 0..9 }
     (âˆ‘áµ¢ dáµ¢ Ã— 10â±) mod 9
    ={ modular arithmetic }
     âˆ‘áµ¢ (dáµ¢ mod 9) Ã— (10â± mod 9)
    ={ 10 mod 9 = 1 and so 10â± mod 9 = 1 }
     âˆ‘áµ¢ (dáµ¢ mod 9) Ã— (1 mod 9)
    ={ modular arithmetic }
     âˆ‘áµ¢ dáµ¢ mod 9

     Hence, N is precisely as divisible by 9 as is the sum of its digits.

  + Casting out threes: An integer is divisible by 3 iff the sum
    of its digits is divisible by 3.

    Proof is similar to above.

    Eg 52563744 has the sum of its digits being
    5 + 2 + 5 + 6 + 3 + 7 + 4 + 4  =  36, which is divisible by 3
    and so the orginal number is too.

  + Casting out elevens: An integer is divisible by 11 iff
    the alternating sum of its digits is divisible by 11.

    Since 10 = -1 mod 11, we have 10â± = (-1)â± mod 11.

  + See https://lexique.netmath.ca/en/divisibility-rule/ for *more* divisibility rules
  + Divisibility by 2: The units digit must be even.
  + Divisibility by 4: The number formed by its last two digits must be divisible
    by 4.
  + Divisibility by 5: The units digit must be 0 or 5.
  + Divisibility by 6: It must be even and divisible by 3.
  + Divisibility by 7: When the units digit is doubled and subtracted
    from the number formed by the remaining digits, the resulting number
    must be divisible by 7; i.e., write N as 10a+b, with b < 10,
    then 7 âˆ£ N  â‰¡  7 âˆ£ (a - 2b).
  + Divisibility by 8: The number formed by its last 3 digits must be divisible
    by 8.

    Eg 52563744 has its last three digits being 744 which is divisible by 8,
    and so the orginal number is too.

  + Divisibility by 10: Its last digit must be 0.

  A â€œlinear congruenceâ€ is an expression of the form â€œa Ã— x =_m bâ€
  where x is the unknown, and all names are integers. E.g., does 8x =_4 2 have
  any solutions in x, what about 8x =_7 2?

  Theorem: ax =_m b has a solution in x iff (a âˆ‡ m) âˆ£ b.
  Moreover, when (a âˆ‡ m) âˆ£ b and a âˆ‡ m = sa + tm,
  then the solutions are given by
  x = sb/d + mu/d for any u : â„¤.

    a âˆ‡ m âˆ£ b
  â‰¡{ divisibility }
    b = q Ã— (a âˆ‡ m) for some q
  â‰¡{ gcd is a linear combination }
    b = q Ã— (sÃ—a + tÃ—m) for some s and t
  ={ multiplication }
    b = qÃ—sÃ—a + qÃ—tÃ—m
  ={ subtraction }
    b - (qÃ—s)Ã—a = qÃ—tÃ—m
  ={ divisibility }
    m âˆ£ (b - qsa)
  ={ defn of =_m }
    a Ã— (-qsa) =_m  b
  â‡’{ âˆƒ-intro }
    a Ã— x =_m b has a solution in x.

  Now, the converse ... not as easy!

  :End:

  --------------------------------------------------------------------------------

  Observation: By âˆ£-reflexivity and âˆ£-least-element, every number has at least
  two divisors, itself and 1.

  Numbers $p$ with the following property are known as *primes* (or â€˜atomsâ€™).
  \[ d âˆ£ p \quadâ‰¡\quad d = 1 \quadâ‰¢\quad d = p \text{ (for any d)}\]

  That is, p has exactly two divisors, 1 and itself;
  numbers with more than two divisors are known as *composite* numbers.

  In particular, 1 is neither prime nor composite since
  it has only 1 divisor, itself.

  --------------------------------------------------------------------------------

  ( Notice that â€˜âˆ£â€™ is very similar to â€˜â‡’â€™; e.g., $m âˆ£ 0$ is akin to $p â‡’
   \true$, and $m âˆ£ m$ is akin to $p â‡’ p$.  Indeed, the similarly is due to the
   fact that implication â€˜â‡’â€™ is also an order with least and greatest
   elements. )

   --------------------------------------------------------------------------------

   If we have /N/ consecutive integers $a_0, a_1, â€¦, a_{N-1}$, then
   $N$ divides ones of these.

   Since the $a_i$ are consecutive, there is an $m$ with
   $a_i = m + i$. By the division algorithm,
   $m = q Ã— N + r$ for some $q$ and $0 â‰¤ r < N$.
   Then, $a_{N - r} = m + (N - r) = (q Ã— N + r) + (N - r) = (q + 1) Ã— N$
   and so $N âˆ£ a_{N -r}$.

   As such, the product of any /N/ consecutive integers is divisible by /N/.

** COMMENT âœ“ gcd
   :PROPERTIES:
   :CUSTOM_ID: COMMENT-gcd
   :END:

   Let's consider â„•atural numbers only â€¦

   The infimum (greatest lower bound) in the division ordering is denoted by â€˜âˆ‡â€™
   and characterised as follows.

   \[\Law{Characterisation of gcd}{ k âˆ£ m \;âˆ§\; k âˆ£ n \quadâ‰¡\quad k âˆ£ (m âˆ‡ n)} \]

   |   | /m âˆ‡ n/                                  |
   | â‰ˆ | the greatest common divisor of /m/ and /n/ |
   | â‰ˆ | the highest common factor of /m/ and /n/   |

   Taking $n â‰” m$ and $n â‰” 0$ gives us
   \[ \Law{Idempotence of gcd}{m âˆ‡ m = m} \]
   \[ \Law{Identity of gcd}{m âˆ‡ 0 = m} \]

   The second law is also due to the fact that 0 is the top element.

   Taking $k â‰” m âˆ‡ n$ yields,
   \[\Law{Gcd is a divisor of its arguments}{(m âˆ‡ n)âˆ£m \quadâˆ§\quad (m âˆ‡ n)âˆ£n}\]

   Of-course, the divisibility order can be â€˜definedâ€™ using meet (just take $k â‰”
   m$ and apply antisymmetry of divisibility):
   \[ \Law{Definition of divisibility via gcd}{ m âˆ£ n \quadâ‰¡\quad m âˆ‡ n = m } \]

   Since conjunction is symmetric and doc-associative, so is the meet.
   \[ \Law{Symmetry of gcd}{ m âˆ‡ n = n âˆ‡ m} \]
   \[ \Law{Associative of gcd}{(m âˆ‡ n) âˆ‡ p = m âˆ‡ (n âˆ‡ p)} \]

   Since divisibility is invariant under additive inverse,
   \[ \Law{Gcd is invariant under additive inverse}{ (-m) âˆ‡ n \;=\; m âˆ‡ n} \]

   Since divisibility is almost preserved by linear combinations, meets are
   invariant under them.

   \[ \Law{Gcd is invariant under semi-linear combinations}{(m + a Ã— n) âˆ‡ n \;=\; m âˆ‡ n} \]

   Taking $m â‰” 0$ yields,
   \[ \Law{Gcd of multiples}{ (a Ã— n) âˆ‡ n \;=\; n} \]

   # In particular, if $g = (âˆ‡ i : 1..n â€¢ x_i)$ then the set of
   # all common divisors of all of the $x_i$ is given by
   #   /{d/m â™ m âˆˆ â„¤ and m âˆ£ d }/.
   #
   # Proof?

   #+begin_box â€œGreatestâ€
   For positive integers, the â€œlargest number wrt to the âˆ£-ordering that divides
   both of the numbersâ€ is the same as â€œthe largest number wrt to the size
   â‰¤-ordering that divides both of the numbersâ€.  Hence, we may refer to $m âˆ‡ n$
   as the greatest common divisor ---and the word â€˜greatestâ€™ causes no problems
   in ambiguity, when the numbers are positive.

   \[\Law{Inclusion of Positives}{ 0 < m \;âˆ§\; 0 < n \quadâ‡’\quad m âˆ£ n \;â‡’\; m â‰¤
   n} \]
   #+end_box

   Just as â€˜â†“â€™ is used for minima, we use â€˜âˆ‡â€™ for gcd.
   Likewise, â€˜â†‘â€™ for maxima and â€˜Î”â€™ for lcm.

   Multiplication interacts nicely with gcd:
   \[ k âˆ£ (c Ã— m) \;âˆ§\; k âˆ£ (c Ã— n) \quadâ‰¡\quad k âˆ£ (c Ã— (m âˆ‡ n)) \]

   Taking $k â‰” m$ yields, \[ m âˆ£ (c Ã— n) \quadâ‰¡\quad m âˆ£ (c Ã— (m âˆ‡ n))\]

   \[
   \Law{Relative-Prime Cancellation for divisibility}{\big(m âˆ£ (c Ã— n) \quadâ‰¡\quad m âˆ£
   c\big) \quadâ‡\quad m âˆ‡ n = 1} \]
   ( The second one above is also known as
   â€œEuclid's Lemmaâ€ when the consequences is presented as an implication instead
   of an equivalence. )
   #
   # This gives us a â€œcancellationâ€ law for divisibility:
   # \[ \Law{Cancellation}{k âˆ‡ m = 1 â‡’ (k âˆ£ (m Ã— n) â‰¡ k âˆ£ n)} \]
   # :Proof:
   #   k âˆ£ (m Ã— n) âˆ§ k âˆ‡ m = 1
   # â‡’ m Ã— n  âˆ‡  k âˆ‡ n  =  (m âˆ‡ k) Ã— n = 1 Ã— n = n
   #   but k âˆ£ (m Ã— n) by assumption and k âˆ£ (k Ã— n) clearly,
   #   and so k âˆ£ ((k Ã— n) âˆ‡ (m Ã— n))
   # â‡’ substitution one equation for the other gives: k âˆ£ n
   #
   # Converse is due to âˆ£-weakening.
   # :End:
   ( this is like â€œif p â‡ q, then (p â‡’ q âˆ¨ r) â‰¡ (p â‡’ r)â€
   or â€œif a â‰° b, then a â‰¤ b â†“ c  â‰¡  a â‰¤ câ€.
   )

   In particular, we can show that multiplication distributes over gcd:
   For $c : â„•$,
   \[\Law{Multiplication distributes over gcd}{c Ã— (m âˆ‡ n) \quadâ‰¡\quad (c Ã— m) âˆ‡ (c Ã— n)} \]

   Consequently,
   \[\Law{Division distributes over gcd}{(m âˆ‡ n)/d \quadâ‰¡\quad (m / d) âˆ‡ (n / d)
   \qquadâ‡\qquad d âˆ£ m âˆ§ d âˆ£ n} \]
   # using divisibility-and-precise-division along with mult-distr-over-gcd
   \[ \Law{Relative-Prime Cancellation for gcd}{ (m Ã— p) âˆ‡ n = m âˆ‡ n \quadâ‡\quad p âˆ‡ n = 1} \]
   # Super tiny proof: Start with RHS and replace m by m Ã— 1 ;-)

   When $p âˆ‡ n = 1$, one says /p/ and /n/ are *relatively prime*
   or â€˜coprimeâ€™ ---this is similar to $A âˆ© B = âˆ…$, for â€˜disjointâ€™ collections.

   ( Relative-primality is closely related to â€˜invertabilityâ€™:
     $m âˆ‡ n = 1 \quadâ‰¡\quad âˆƒ x â€¢\; n Ã— x \;\stackrel{\large =}{\tiny
     m}\; 1$,
     where $=_m$ denotes equality mod-$m$.

     # Indeed, m âˆ‡ n = 1 â‡’ âˆƒs,x â€¢ sm + nx = 1
     # â‡’ âˆƒ nx =â‚˜ 1. Conversely, if nx =â‚˜ 1
     # â‡’ âˆƒ y â€¢ nx = 1 + my
     # â‡’ nx + m(-y) = 1
     # â‡’ m âˆ‡ n = 1.
   )

   Since 1 is the least element:
   \[ \Law{1 is relatively prime (with all integers)}{ p âˆ‡ 1 = 1} \]
   \[\Law{Division distributes over gcd}{m/(m âˆ‡ n) \;âˆ‡\; n/(m âˆ‡ n) \;=\; 1}\]
   # follows from div-ditrs-over-gcd.

   \[ \Law{Example}{(7 Ã— n + 1) âˆ‡ (15 Ã— n + 2) = 1} \]
:Hide:
   By indirect inclusion:
#+begin_src math
  d âˆ£ ((7 Ã— n + 1) âˆ‡ (15 Ã— n + 2))
â‰¡ d âˆ£ (7 Ã— n + 1)  âˆ§  d âˆ£ (15 Ã— n + 2)
â‰¡ d âˆ£ (7 Ã— n + 1)  âˆ§  d âˆ£ (2 Ã— (7 Ã— n + 1) + n)
â‰¡âŸ¨ divisibility preserves siemi-linear combinations âŸ©
  d âˆ£ (7 Ã— n + 1)  âˆ§  d âˆ£ n
â‰¡âŸ¨ divisibility preserves siemi-linear combinations âŸ©
  d âˆ£ 1  âˆ§  d âˆ£ n
â‰¡âŸ¨ 1 is the least element and substitution âŸ©
â‰¡ d = 1   âˆ§  1 âˆ£ n
â‰¡âŸ¨ 1 is the least element (twice) âŸ©
â‰¡ d âˆ£ 1
#+end_src
:End:

   \[ \Law{Common Relative-primes are closed under multiplication}{(a Ã— b) âˆ‡ m =
   1 \quadâ‡\quad a âˆ‡ m = 1 = b âˆ‡ m} \]

   When a number is divisible by two or more co-prime numbers, it is also
   divisible by their product.
   \[ a âˆ£ c âˆ§ b âˆ£ c âˆ§ a âˆ‡ b = 1 â‡’ (a Ã— b) âˆ£ c \]

   *Theorem:* The gcd is the least positive value of a polynomial:
   $m âˆ‡ n$ is the least positive value of $m Ã— x + n Ã— y$ as $x, y$ range over
   â„¤; provided m,n are positive. As such, the gcd can be expressed as a linear
   sum of its arguments. Moreover, this means we know that an equation
   $a Ã— x + b Ã— y = c$ has integral solutions for $x, y$ precisely when
   $c âˆ£ (a âˆ‡ b)$.
   # indeed, c = q Ã— (a âˆ‡ b) = q Ã— (sÃ—a + tÃ—b) = a Ã— (qs) + b Ã— (qt).

   :Proof_outline:
   Let S = {mx + ny â™ mx+ny > 0; x,y âˆˆ â„¤ } and d = min S.

   Then, d = mx + ny > 0 for some mx and ny.

   By division algorithm, m = dq + r for 0 â‰¤ r < d,
   and so r = m - dq = m - (mx + ny)q = m(1-x) + n(-yq).
   Now, if r > 0 then r âˆˆ S and so r â‰¥ min S = d, which contradicts r < d;
   hence r â‰¤ 0 but 0 â‰¤ r and so r = 0. Thus, d âˆ£ m. Likewise, d âˆ£ n;
   and so d âˆ£ (m âˆ‡ n). But since m > 0, n >0, we must have
   m âˆ‡ n > 0, but d > 0, and since divisibility includes size and d âˆ£ (m âˆ‡ n)
   we must have d â‰¤ m âˆ‡ n.

   Conversely, g = (m âˆ‡ n) â‡’, by definition of gcd, âˆƒ b,c with m = gb âˆ§ n = gc,
   and so d = mx + ny = gbx + gcy = g(bx + ny) and so gâˆ£d.
   Since g > 0, d >0, and gâˆ£d, we have (since size contains divisibility)
   that m âˆ‡ n = g â‰¤ d.

   Hence, by â‰¤-antisymmetry, m âˆ‡ n = d = mx + ny.
   :End:

   Since divisibility preserves semi-linear combinations,
   \[ \Law{Gcd Invariance}{ m âˆ‡ n = m âˆ‡ n = m âˆ‡ (-n) = m âˆ‡ (n + x Ã— m)} \]

   Since gcd can be expressed as a sum of its arguments, and divisibility is
   invariant under linear expressions (and 1 is the least element):
   \[ \Law{Relatively-prime Criterion}{m âˆ‡ n = 1 \quadâ‰¡\quad âˆƒ s, t : â„¤ â€¢ 1 = s
   Ã— m + t Ã— n }\]

   --------------------------------------------------------------------------------

   + Golden Rule: m Ã— n = (m âˆ‡ n) Ã— (m Î” n)
   + Golden Rule: m + n = (m â†“ n) + (m â†‘ n)
   + Golden Rule: m âŠ• n = (m âŠ“ n) âŠ• (m âŠ” n), in any total order and âŠ• symmetric

   The next two rules may act as a ( computationally  expensive) algorithm
   to compute the gcd: Compute the prime factorisation (which is expensive to
   do) then form the number having the minimum of each exponent.
   + $\exp_p (m âˆ‡ n) = \exp_p m â†“ \exp_p n$
   + $\exp_p (m Î” n) = \exp_p m â†‘ \exp_p n$

   --------------------------------------------------------------------------------

   Moving to lcm.

   If an integer is divisible by two or more different numbers, then is it also
   divisible by the least common multiple of those numbers.
   - 2 âˆ£ 24 âˆ§ 3 âˆ£ 24 â‡’ (2 Î” 3) âˆ£ 24 â‡’ 6 âˆ£ 24.

** COMMENT A recursive algorithm for GCD
   :PROPERTIES:
   :CUSTOM_ID: A-recursive-algorithm-for-GCD
   :END:

   Theorem: a âˆ‡ b = r âˆ‡ b  â‡  a = b Ã— q + r

   Proof: By indirect equality, show a,b have the same divisors as r,b.

   Using this theorem we can compute gcd recursively:
#+begin_src math
a âˆ‡ 0 = a
a âˆ‡ b = (a mod b) âˆ‡ b
#+end_src

  Note that a control flow chart can act as both
  a unified representation of an imperative algorithm as well
  as a recursive algorithm.
  - See: https://orgmode.org/worg/org-tutorials/org-dot-diagrams.html (nice!)
#+begin_src dot :file del.png
digraph {
  Start [label="Start with a and b" style=filled fillcolor=orange]
  Check [label="b = 0?" shape=diamond style=filled fillcolor=lightgreen]
  Yes   [label="Answer is b" style=filled fillcolor=orange]
  No    [label="b = a mod b" shape=square style=filled fillcolor=lightblue]
  Start -> Check
  Check -> No -> Start
  nope [style=invisible]
  No -> nope [style=invisible]
  Yes -> nope [style=invisible]
         Check -> Yes
        }
#+end_src

#+RESULTS:
[[file:del.png]]

** COMMENT Deriving Euclid's Algorithm
   :PROPERTIES:
   :CUSTOM_ID: Deriving-Euclid's-Algorithm
   :END:

   The characterisation of the meet could act as a specification for an
   imperative algorithm computing $x = m âˆ‡ n$.

   \[ (âˆ€ k â€¢ k âˆ£ m \;âˆ§\; k âˆ£ n \quadâ‰¡\quad k âˆ£ x) \]

   Unfortunately, these is no explicit conjunction that we could truthify one
   part easily and so take it as invariant, and leave the other conjunct for the
   loop guard. For instance, the only variable we have is $x$ and the only
   reasonable choices for it are $0, 1, m, n$ but none truthify anything.
   So we add â€˜another degree of freedomâ€™: The following is an equivalent
   re-statement, but now we have two variables to work with, $x$ and $y$.

   \[ x = y \;âˆ§\; (âˆ€ k â€¢ k âˆ£ m \;âˆ§\; k âˆ£ n \quadâ‰¡\quad k âˆ£ x \;âˆ§\; k âˆ£ y) \]

   Now an (âˆ€)quantification is not something we want to consider for loop guard
   and so we consider it for invariant. Indeed, taking $x,y â‰” m, n$ immediately
   truthifies the right conjunct, and so the right conjunct is a good candidate
   for invariant. So, we take $x â‰  y$ as loop guard.

   In summary,
   \[ 0 < m \,âˆ§\, 0 < n \Then{x,y â‰” m, n â® \While{x â‰  y} ?} x = y \,âˆ§\, (âˆ€ k â€¢ k âˆ£ m \;âˆ§\; k âˆ£ n \quadâ‰¡\quad
   k âˆ£ x \;âˆ§\; k âˆ£ y) \]

   We now need to alter $x$ and $y$ in such a way as to maintain the invariant
   whilst making progress to a state satisfying $x = y$.  Similar considerations
   as was done for the division algorithm lead us to the following algorithm
   ---where conditionals have been absorbed into the top-level scope: $x â‰  y â‰¡ x
   < y âˆ¨ y < x$.

   #+begin_src math
{ 0 < m  âˆ§ 0 < n }
x, y â‰” m, n â®
{ Invariant: 0 < x  âˆ§  0 < y  âˆ§  (âˆ€ k â€¢ k âˆ£ m  âˆ§  k âˆ£ n  â‰¡  k âˆ£ x  âˆ§  k âˆ£ y)
  Bound function: x + y
}
do y < x âŸ¶ x := x - y
â™  x < y âŸ¶ x := y - x
od
{ Invariant: 0 < x  âˆ§  0 < y  âˆ§  x = y  âˆ§  (âˆ€ k â€¢ k âˆ£ m  âˆ§  k âˆ£ n  â‰¡  k âˆ£ x  âˆ§  k âˆ£ y) }
   #+end_src
* Quantification and Predicate Logic
  :PROPERTIES:
  :CUSTOM_ID: Quantification-and-Predicate-Logic
  :END:

     #+begin_latex-definitions
\newcommand{Law}[3][]{ #1\;\;\textbf{#2:}\quad #3 }
\def\abs{\mathsf{abs}\,}
\def\Id{\mathsf{Id}}
   #+end_latex-definitions

1. Quantification let's us â€œsum a bunch of /numeric/ valuesâ€ or â€œor a bunch of
  /Boolean/ valuesâ€.

  Quantifiers allow one to denote the operation of applying some binary operator
   (like addition or disjunction) to an arbitrary, possibly infinite, bag of
   values. For programming, quantifiers specify iteration control structures
   ---loops.

  Quantifications are everywhere in mathematics, so we need basic tools to
   handle them.  Herein we develop the notation and general techniques that make
   summation user-friendly. We introduce a /uniform notation/ for quantifiers
   ---and show instances for addition +/Î£, multiplication Â·/Î , conjunction âˆ§/âˆ€,
   disjunction âˆ¨/âˆƒ, and minima â†“ (which does not have a conventionally accepted
   â€˜large operatorâ€™ notation, but is nonetheless supported by the uniform
   notation). We also compare the uniform notation with an existing notation.

#+begin_quote
It is a profoundly erroneous truism, repeated by all copybooks and by eminent
people when they are making speeches, that we should cultivate the habit of
thinking of what we are doing. The precise opposite is the case. Civilization
advances by extending the number of important operations which we can perform
without thinking about them. Operations of thought are like cavalry charges in a
battle ---they are strictly limited in number, they require fresh horses, and
must only be made at decisive moments.

--- A. N. Whitehead

Alfred North Whitehead, An Introduction to Mathematics. London and 503.
New York, 1911.
#+end_quote

 # See LADM p148 for Leibniz rules for quantifiers.

Suppose âŠ• is associative, symmetric, and has unit $\Id_âŠ•$.
- â‡¨ The laws of âŠ•-quantifiers are *generalised* forms of the laws of the binary operations âŠ•. â‡¦
- ( For finite ranges, the rules can be proven inductively. )

*Side Condition.* A general side condition on all the rules is that their
application should not result in the capture of free variables or release of
bound variables, and should not result in a variable occurring more than once in
a list of dummies. We will specialise this proviso for each rule as needed.

|                | /Terse summary of a few quantifier rules/                             |
|----------------+---------------------------------------------------------------------|
| *Dummy Rules*    |                                                                     |
| Renaming       | Dummy names are place-holders and don't matter                      |
| Nesting        | A list of dummies abbreviates multiple quantifications              |
| Permutation    | The order of variables in a list of dummies doesn't matter          |
|----------------+---------------------------------------------------------------------|
| *Range Rules*    |                                                                     |
| Empty range    | The empty range is the do-nothing (â€œhave no effectâ€) value          |
| One-point rule | When there is only one option, a quantification is a â€œlet clauseâ€   |
| Range Split    | The principle of inclusion-exclusion ---draw a Venn diagram!        |
| Trading        | Range information can be moved to the body as a conditional         |
|----------------+---------------------------------------------------------------------|
| *Body Rules*     |                                                                     |
| Rearranging    | The quantification notation distributes over the binary operation   |
| Distributivity | Functions that distribute over âŠ• also distribute over âŠ•-quantifiers |
|----------------+---------------------------------------------------------------------|

** Looking at sums, Î£
   :PROPERTIES:
   :CUSTOM_ID: Looking-at-sums-Î£
   :END:
2. The sum of the first /n/ integers is $1 + 2 + 3 + â‹¯ + (n - 1) + n$.

   The â€œâ‹¯â€ tells us to complete the pattern established by the surrounding terms
   ---a burden on the reader.

   Sometimes such patterns are not clear, as in $1 + 2 + â‹¯ + 16$: Is this the
   sum of the first 16 integers or the sum of the first five powers of 2
   ---i.e., $2â° + 2Â¹ + â‹¯ + 2â´$? To make the pattern clear, the latter requires
   us to communicate the general shape (â€œ$2^i$â€) in a number instances.

3. The three-dots notation has many uses, but it can be ambiguous and a bit long-winded.

   # The three dots may show too much detail, which can be overwhelming.

   Quantifier notation is compact, impressive to family and friends, and often
   suggestive of manipulations that are not obvious in three-dots form.

4. The notation $(Î£ i\;â™\;1 â‰¤ i â‰¤ n â€¢ aâ‚–)$ is used to communicate sums $aâ‚ + aâ‚‚ +
   â‹¯ + aâ‚™$.

   The index variable /i/ is said to be /bound/ to the Î£ because any /i/ in /aáµ¢/ is
   unrelated to appearances of /i/ outside the Î£-notation. Any other letter could
   be substituted for /i/ here without changing the meaning of the
   quantification. ( The letter /i/ is often used perhaps because it stands for
   â€œindexâ€. )

5. It turns out that a generalised quantification-notation is even more
   useful than the delimited form: We simply write one or more
   conditions in the /range/ position to specify the set of indices
   over which summation should take place.

   For example, the general form allows us to take sums over index sets that
   aren't restricted to consecutive integers. For example, we can express the
   sum of the squares of all odd positive integers below 100 as follows: \[(Î£ i
  \;â™\;1 â‰¤ k < 100 âˆ§ k \,\mathsf{odd} â€¢ kÂ²)\]
  Below is the delimited equivalent of this sum; it is more cumbersome
  and less clear.
  \[Î£ k\;â™\;0 â‰¤ k â‰¤ 49 â€¢ (2 Â· k + 1)Â² \]

  Sometimes no delimited form is possible; e.g.,
  the sum of the first /n/ primes is
  $(Î£ p\;â™\;0 â‰¤ p â‰¤ n âˆ§ p \,\mathsf{prime} â€¢ p)$
  ---yet there is no explicit formula for primes
  and so we need to pick a notation for primes before a delimited
  form could be phrased; e.g., $pâ‚–$ to denote the /k/-th prime.

6. The biggest advantage of general quantification notation is that we can
   manipulate it more easily than the delimited form ---since the range of the
   index variable is allowed to be arbitrary propositions, and so, for example,
   it is easy to manipulate the index variables since we don't have to worry
   about boundary conditions.

7. People are often tempted to write $(Î£ i â™ 2 â‰¤ i â‰¤ n - 1 â€¢ i Â· (i - 1))$
   instead of $(Î£ i â™ 0 â‰¤ i < n â€¢ i Â· (i - 1) Â· (i - 2))$ because the terms for
   $i = 0, 1$ are 0 and the former insists on inclusive bounds.

   Somehow it seems more efficient to add up $n - 2$ terms instead of $n$
   terms. But such temptations should be resisted; efficiency of computation is
   not the same as efficiency of understanding!

   We will find it advantageous to keep the range ---and, in turn, the bounds on
   the index variable--- as simple as possible, because quantifications can be
   manipulated much more easily when the bounds are simple.

   (In Î£s) Zero-valued terms cause no harm, and they often save a lot of
   trouble.

8. Kenneth Iverson introduced a wonderful idea in his programming language APL.
   The idea is simply to enclose a true-or-false statement in brackets, and to
   say that the result is 1 if the statement is true, 0 if the statement is
   false. ( E.g., the Kronecker delta is then Î´áµ¢â±¼ = [i = j]. )

   Iverson brackets let one express sums with no constraints whatever on the
   index of summation: $Î£_{R\, i} aáµ¢ \;=\; Î£áµ¢ aáµ¢ Â· [R\, i]$.
   If $R\, i$ is false, the term $aáµ¢ Â· [R\, i]$ is zero, so we can safely
   include it among the terms being summed.


| $(Î£ i\;â™\;R â€¢ P)$ | â‰ˆ | /â€œthe sum of all terms P where i is an integer satisfying Râ€/ |

** COMMENT Properties of Î£
   :PROPERTIES:
   :CUSTOM_ID: Properties-of-Î£
   :END:

Let's look at Î£ first, a concrete case.

The main advantage of a formal quantifier notation over the informal dotdotdot
notation is that it is easier to formulate and use calculational rules.

We formulate the rules in terms of each of the components of a quantification.
So there are rules governing the use of dummies, rules exploiting the structure
of the range, and rules exploiting the structure of the term.  Additionally,
there is a so-called /trading/ rule that allows information to be moved to and
from the range of a quantification.

# --------------------------------------------------------------------------------

** COMMENT Types :Relocate_to_section_on_expressions:
   :PROPERTIES:
   :CUSTOM_ID: Types
   :END:

2. A /type/ denotes a set of values that can be associated with a variable.

   - The introduction of types causes us to refine our notion of an expression.
   - To be an expression, not only must a sequence of symbols satisfy the normal
     rules of syntax concerning balanced parentheses, et., it
     must also be /type correct/.
   - Thus, some expressions that earlier seemed okay will no longer be called
     expressions because they do not satisfy the typing rules.

3. Every expression $E$ has a type Ï„, which we can declare by writing $E : Ï„$.
   - In particular, every variable has a type, which is mentioned in the text
     accompanying an expression that uses the variable, and sometimes it is
     given in some sort of a declaration (as is the case in programming).

4. Besides constants and variables, the only other kind of expression we have
   encountered thus far is function application.

   Each function $f$ has a type $Ï„â‚ Ã— â‹¯ Ã— Ï„â‚™ â†’ Ï„$, which describes the types of
   its parameters (the Ï„áµ¢) and the type of its result (the final Ï„).

   - For such a function $f$, the application $f(aâ‚, â€¦, aâ‚™)$ is
     defined to be an expression iff each argument $aáµ¢$ has type Ï„áµ¢.

   - The type of the function application is then Ï„.

   In this way, â€œexpressionhoodâ€, as well as the types of expressions,
   is determined from the types of its operands.

5. It is important to recognise that type and type correectness
   are *syntactic notions*: They depend only on the sequence of *symbols*,
   in a proposed expression, and not on the *evaluation* of the proposed
   expression (in a state).

   - E.g., $(1 / x) : â„$ does not evaluate if $x = 0$, but it is still an
     (useful) expression.

6. With the notion of type, some restrictions are needed to ensure type
   correectness during manipulations:

   (8.3) In a textual substitution $E[x â‰” F]$, $x$ and $F$ must have the same
   type. This ensures that textual substitution preserves expressionhood.

** Syntax :p142:
   :PROPERTIES:
   :CUSTOM_ID: Syntax-p142
   :END:

1. The notation $âˆ‘_{i = 1}^n f(i)$ is used to /abbreviate/ $f(1) + f(2) + â‹¯ +
   f(n)$.

2. Instead, we use the /linear notation/ $(Î£ i \;â™\; 1 â‰¤ i â‰¤ n â€¢ f\, i)$
   or $(+ i \;â™\; 1 â‰¤ i â‰¤ n â€¢ f\, i)$
   for several reasons.

3. The parentheses make explicit the /scope/ of the /dummy/ or /quantified variable
   i/: The places where /i/ can be referenced. The scope comprises the expressions
   within the parentheses.

   The â€œ$Î£\,i$â€ (or â€œ$+\, i$â€) acts as a declaration, introducing *<<<dummy>>>* /i/.
   - This dummy is not a variable in the usual sense, for it does not obtain a
     value from the state in which the expression is evaluated.

4. The linear notation makes it easier to write more general ranges for $i$.

   We can write /any/ Boolean expression to describe the values of $i$
   for which $f\, i$ should be summed. For example, to sum the evens
   between 1 and 7:
   \[ (Î£ i \;â™\; 1 â‰¤ i â‰¤ 7 âˆ§ \mathsf{even}\, i â€¢ i) \;=\; 2 + 4 + 6\]

5. The linear notation extends easily to allow more than one dummy, as shown in
   the following example.  \[ (Î£ i,j \;â™\; 1 â‰¤ i â‰¤ 2 âˆ§ 3 â‰¤ j â‰¤ 4 â€¢ i^j) \;=\; 1^3 +
   1^4 + 2^3 + 2^4\] In determining which values $i^j$ are being summed, we
   choose all combinations of $i$ and $j$ that satisfy the /range/ $1 â‰¤ i â‰¤ 2 âˆ§ 3
   â‰¤ j â‰¤ 4$.

6. In the summations about, our intent is that dummy $i$ ranges over integer
   values, rather than real values (say). To make the type explicit, we can
   write $(Î£ i : â„¤ \;â™\; 1 â‰¤ i â‰¤ 2 â€¢ f\, i)$.

7. What has been said about summation generalises to other operators.

   The general shape of a /quantification over monoidal operation $\_{}âŠ•\_{}$/
      is exemplified by:
      \[ (âŠ• x : Ï„â‚, y : Ï„â‚‚ \;â™\; R â€¢ P) \]

   - Distinct variables $x$ and $y$ are the /bound variables/ or /dummies/ of the
     quantification. There may be one or more dummies.

   - Boolean expression $R$ may depend on $x$ and $y$; it is the /range/ of the
     quantification ---values assumed by $x$ and $y$ satisfy $R$.

     When there are no constraints ---i.e., the range is as large as possible,
     $R$ is /true/--- we may omit it:
     \[ (âŠ• x â€¢ P) \;=\; (âŠ• x \;â™\; true â€¢ P)\]

   - Expression $P$ is the /body/ of the quantification; it may refer to dummies
     $x$ and $y$.

   - The type of the result of the quantification is the type of $P$; i.e., if
     $\_{}âŠ•\_{} : Ï„ â†’ Ï„ â†’ Ï„$ then $P : Ï„$ --whenever /x : Ï„â‚, y : Ï„â‚/--- and $(âŠ•
     x : Ï„â‚, y : Ï„â‚‚ \;â™\; R â€¢ P) : Ï„$.

8. *Expression $(âŠ• x : Ï„ \;â™\; R â€¢ P)$ denotes the application of operator âŠ• to the
   values $P$ for all /x/ in Ï„ for which range $R$ is true.*

   Next, we will define the notation /axiomatically/ instead of relying on a
   particular evaluation model, implementation.

9. By convention quantifications for +, Â·, âˆ¨, âˆ§ are denoted by Î£, Î , âˆƒ, âˆ€.

# 10. See LADM p144

** Quantifier Introduction & Elimination
   :PROPERTIES:
   :CUSTOM_ID: Introduction-Elimination-of-quantifiers
   :END:

| (8.13) Axiom, Empty Range    | $(âŠ• x \;â™\; false â€¢ P) = \Id_âŠ•$    |
| (8.14) Axiom, One-point rule | $(âŠ• x \;â™\; x = E â€¢ P) = P[x â‰” E]$ |
| (8.u) Axiom, Unital body     | $(âŠ• x \;â™\; R â€¢ \Id_âŠ•) = \Id_âŠ•$    |

*Proviso:* For (8.14), we need /x/ not to occur in /E/.

Indeed, the LHS does not depend on /x/ (in the state in which it is evaluated),
since all occurrences of $x$ are bound. Hence, for the equivalence to hold, the
RHS also cannot depend on $x$, and this requires (in general) that $x$ not occur
free in $E$.

For example,
- (8.u.Î£): $(Î£ x \;â™\; R â€¢ 0) = 0$
- (8.u.Î ): $(Î  x \;â™\; R â€¢ 1) = 1$
- (8.u.âˆƒ) (9.8): $(âˆ€ x \;â™\; R â€¢ true) â‰¡ true$
- (8.u.âˆ€) (9.24): $(âˆƒ x \;â™\; R â€¢ false) = false$

** Â Dummy Renaming
   :PROPERTIES:
   :CUSTOM_ID: Dummy-Renaming
   :END:

The first rule expresses the fact that a â€˜dummyâ€™ is just a place holder, the
particular name chosen for the dummy is not relevant provided it does not clash
with the names of other variables in the quantified expression.  Renaming is
often used when performing other algebraic manipulations in order to avoid
capture of free variables or release of bound variables.

Recall that $E[x â‰” F]$ denotes the expression obtained by replacing every /free/
occurrence of /x/ in /E/ by /F/. Then,
+ (Axiom 8.21) Dummy Renaming :: Provided /y/ does not occur in /R/ nor in /P/,
  \[ (âŠ• x \;â™\; R â€¢ P) \quad=\quad (âŠ• y \;â™\; R[x â‰” y] â€¢ P[x â‰” y]] \]

Remember: The â€˜occursâ€™ restrictions on these laws ensure that an expression that
contains an occurrence of a dummy is not moved outside (or inside) the scope of
the dummy.

This rule is also known as â€œalpha renamingâ€.

** Nesting
   :PROPERTIES:
   :CUSTOM_ID: Nesting
   :END:

The terms of a sum might be specified by two or more indices, not just by one.
Only one quantification is needed, although there is more than one index of
summation, and so the quantification denotes a sum over /all combinations/ of
indices that apply.

That is, the second rule for dummies state, essentially, that the use of more than one
dummy is a convenient abbreviation for a collection of quantifications.

+ (Axiom 8.20) Nesting :: Provided /y/ does not occur in /R/,
  \[ (âŠ• x, y \;â™\; R âˆ§ S â€¢ P) \quad=\quad (âŠ• x \;â™\; R â€¢ (âŠ• y \;â™\; S â€¢ P)) \]

A sum that depends on more than one index can be summed first on any one of its
indices. The rule says we can choose whichever is more convenient.

This is also known as /interchanging the order of summation/ and
it generalises the associative/distributive law for quantifications.

See the â€œgeneral distribute lawâ€ below for a useful result involving nesting.

--------------------------------------------------------------------------------

+ When used left to right, this rule is known as /nesting/ and the proviso
  ensures that bound variables are not released.

  - Were $R$ to depend on /y/, that variable would be bound in the LHS but it would be
  free in the RHS. The right side would, thus, be an expression that depends on
  the value of this variable, whereas the left side does not.

  It is always possible to avoid such complications by suitably renaming
  bound variables before using this rule.

+ When used right to left, this rule is known as /unnesting/ ---since
  quantifications become unnested.

  - Notice that /the range of y could depend on the value of x/; i.e.,
    /S may mention x/.

--------------------------------------------------------------------------------

Sometimes the range of an inner sum *depends on*
the index variable of the outer sum ... !

Then what ;-)

Frequently in applications the dependent ranges are inclusions and then
we have particularly useful factorisations:
- $1 â‰¤ i â‰¤ j â‰¤ n \quadâ‰¡\quad 1 â‰¤ j â‰¤ n \,âˆ§\, 1 â‰¤ i â‰¤ j$
- $1 â‰¤ i â‰¤ j â‰¤ n \quadâ‰¡\quad 1 â‰¤ i â‰¤ n \,âˆ§\, i â‰¤ j â‰¤ n$

Then, nesting allows us to write
\[(âŠ• i, j\;â™\;1 â‰¤ i â‰¤ j â‰¤ n â€¢ P) \quad=\quad (âŠ• j\;â™\;1 â‰¤ j â‰¤ n â€¢ (âŠ• i\;â™\;1 â‰¤ i â‰¤ j â€¢ P))\]
\[(âŠ• i, j\;â™\;1 â‰¤ i â‰¤ j â‰¤ n â€¢ P) \quad=\quad (âŠ• i\;â™\;1 â‰¤ i â‰¤ n â€¢ (âŠ• j\;â™\;i â‰¤ j â‰¤ n â€¢ P))\]

One of these two sums is usually easier to evaluate than the other;
we can use these factorisations to switch from the hard one to the easy one.

- If the body involves a phrase â€œ$j - i$â€, it may be helpful to replace /j/ by /j -
  i/ ---using the permutative law below--- then apply the second factorisation
  above.

** Â Dummy Rearrangment ---Generalised Symmetry & Associativity
   :PROPERTIES:
   :CUSTOM_ID: Rearranging-Dummy-List-Permutation
   :END:

The third rule is very powerful because, in combination with the nesting rule,
it allows us to rearrange the order in which the values in a summation are added
together. It simply states that the order in which the dummies are listed in a
summation is irrelevant.

+ Rearranging, Dummy List Permutation ::
  \[ (âŠ• x, y \;â™\; R â€¢ P) \quad=\quad (âŠ• y, x \;â™\; R â€¢ P) \]

Here is an example of how the nesting and rearrangement rules are combined to
change the order of the values being summed.  Nesting and rearrangement depend
critically on the associativity and symmetry of the operation âŠ•.  The
parenthesisation corresponds to the nesting of summands.
\begin{calc}
(1 x 1 + 1 Ã— 2 + 1 Ã— 3) + (2 Ã— 2 + 2 Ã— 3) + (3 Ã— 3)
\step{ definition of Î£ }
(Î£ i â™ 1 â‰¤ iâ‰¤ 3 â€¢ (Î£ j â™ i â‰¤ j â‰¤ 3 â€¢ i Ã— j))
\step{ unnesting }
(Î£ i, j â™ 1 â‰¤ iâ‰¤ 3 \,âˆ§\, i â‰¤ j â‰¤ 3 â€¢ i Ã— j)
\step{ rearranging }
 (Î£ j, i â™ 1 â‰¤ iâ‰¤ 3 \,âˆ§\, i â‰¤ j â‰¤ 3 â€¢ i Ã— j)
\step{ nesting: $1 â‰¤ i â‰¤ 3 âˆ§ i â‰¤ j â‰¤ 3 \;â‰¡\; 1 â‰¤ j â‰¤ 3 âˆ§ 1 â‰¤ i â‰¤ j$ }
(Î£ i â™ 1 â‰¤ iâ‰¤ 3 â€¢ (Î£ j â™ i â‰¤ j â‰¤ 3 â€¢ i Ã— j))
\end{calc}

The change of inclusions may not be familiar, so here is its proof.
\begin{calc}
a â‰¤ x â‰¤ b âˆ§ x â‰¤ y â‰¤ b
\step{ conjunctional notation }
a â‰¤ x âˆ§ x â‰¤ b âˆ§ x â‰¤ y âˆ§ y â‰¤ b
\step{ Transitivity of â‰¤ : $a â‰¤ x â‰¤ y \;â‰¡\; a â‰¤ x â‰¤ y \,âˆ§\, a â‰¤ y$ }
a â‰¤ x âˆ§ x â‰¤ b âˆ§ a â‰¤ y âˆ§ x â‰¤ y âˆ§ y â‰¤ b
\step{ Transitivity of â‰¤: $x â‰¤ y â‰¤ b \,âˆ§\, x â‰¤ b \;â‰¡\; x â‰¤ y â‰¤ b$ }
a â‰¤ x âˆ§ x â‰¤ y  \;âˆ§\; a â‰¤ y âˆ§ y â‰¤ b
\step{ conjunctional notation }
a â‰¤ x â‰¤ y  âˆ§  a â‰¤ y â‰¤ b
\end{calc}

We may capture the unnesting-rearrangement-nesting pattern in the above example as a theorem.
+ (8.19) Interchange of dummies :: Provided each quantification is defined,
  /y/ does not occur in /R/, and /x/ does not occur in /S/,
  \[ (âŠ• x \;â™\; R â€¢ (âŠ• y \;â™\; S â€¢ P)) \quad=\quad (âŠ• y \;â™\; S â€¢ (âŠ• x \;â™\; R â€¢ P)) \]
:Proof:
\begin{calc}
(âŠ• x \;â™\; R â€¢ (âŠ• y \;â™\; S â€¢ P))
\step{ unnesting; assuming â€˜yâ€™ not in â€˜Râ€™ }
(âŠ• x, y \;â™\; R âˆ§ S â€¢ P)
\step{ rearrangement and âˆ§-symmetry }
(âŠ• y, x \;â™\; S âˆ§ R â€¢ P)
\step{ nesting; assuming â€˜xâ€™ not in â€˜Sâ€™ }
(âŠ• y \;â™\; S â€¢ (âŠ• x \;â™\; R â€¢ P))
\end{calc}
:end:

** Range Split
   :PROPERTIES:
   :CUSTOM_ID: Range-Split
   :END:

The next axiom is called /range split/, because the range /R âˆ¨ S/ in its LHS
is split into two ranges $R$ and $S$ in its LHS.

*(8.16) Axiom, Range Split:* Provided $R âˆ§ S â‰¡ false$, and each quantification is
defined,
\[ (âŠ• x \;â™\; R âˆ¨ S â€¢ P) \quad=\quad (âŠ• x \;â™\; R â€¢ P) âŠ• (âŠ• x \;â™\; S â€¢ P) \]

For instance, suppose we have a bag of â€˜Râ€™ed numbers and â€˜Sâ€™ilver numbers.  The
LHS says we can sum them in any order we like, whereas the RHS says we sum the
â€˜Râ€™ed ones, sum the â€˜Sâ€™ilver ones, then add the two sums.

The restriction that $R âˆ§ S â‰¡ false$ ensures that an operand is not accumulated
twice in the RHS ---once because a value $x$ satisfies $R$ and once because the
same value $x$ satisfies $S$. The next axiom eliminates this restriction by
adding to the LHS the accumulation $(âŠ• s \;â™\; R âˆ§ S â€¢ P)$; thus, the values of $P$
are accumulated twice because values for the dummies satisfy both $R$ and $S$
are accumulated twice on both sides of the equation.

*(8.17) Axiom, Range Split:* Provided each quantification is defined,
\[ (âŠ• x \;â™\; R âˆ¨ S â€¢ P) âŠ• (âŠ• x \;â™\; R âˆ§ S â€¢ P) \quad=\quad (âŠ• x \;â™\; R â€¢ P) âŠ• (âŠ• x \;â™\; S â€¢ P) \]

--------------------------------------------------------------------------------

- Range split can be used to combine adjacent disjoint intervals:
  For $a â‰¤ b â‰¤ c$,
  \[
  (âŠ• x â™ a â‰¤ x < b â€¢ P) âŠ• (âŠ• x â™ b â‰¤ x < c) \;=\; (âŠ• x â™ a â‰¤ x < c â€¢ P)
  \]

  Notice the familiarity with $\int_a^b f(x) dx + \int_b^c f(x) dx = \int_a^c f(x)dx$.

- It can also be used to split off a single term from a sum: For $0 â‰¤ n$,
  \[
  (âŠ• x â™ 0 â‰¤ i < n + 1 â€¢ P) \;=\; (âŠ• x â™ 0 â‰¤ i < n â€¢ P) âŠ• P[i â‰” n]
  \]

--------------------------------------------------------------------------------

On the other hand, if operator âŠ• is idempotent ---so that $e âŠ• e = e$ for all
$e$, then it does not matter how many times $e$ is accumulated. Hence, we have

*(8.18) Axiom, Range Split for Idempotent âŠ•:* Provided each quantification is
defined, \[ (âŠ• x \;â™\; R âˆ¨ S â€¢ P) \quad=\quad (âŠ• x \;â™\; R â€¢ P) âŠ• (âŠ• x \;â™\; S â€¢ P) \]

** Body Rearrangment ---Generalised Symmetry & Associativity
   :PROPERTIES:
   :CUSTOM_ID: Body-Rearrangment-Generalised-Symmetry-Associativity
   :END:

Since âŠ• is symmetric and associative, the order in which the operands are
accumulated has no bearing on the result.  Below is an example of how
range-split can be used alongside the nesting and rearrangement rules to change
the order of the values being summed.  Nesting and rearrangement depend
critically on the associativity and symmetry of the operation âŠ•.
\begin{calc}
(1 + 2^1) + (2 + 2^2) + (3 + 2^3)
\step{ definition of Î£ }
(Î£ x â™ 1 â‰¤ x â‰¤ 3 â€¢ x + 2^x)
\step{ definition of Î£: Let $Pâ‚€ = x, Pâ‚ = 2^x$ }
(Î£ x â™ 1 â‰¤ x â‰¤ 3 â€¢ (Î£ i â™ i = 0 âˆ¨ i = 1 â€¢ Páµ¢))
\step{ unnesting }
(Î£ x, i â™ 1 â‰¤ x â‰¤ 3 âˆ§ (i = 0 âˆ¨ i = 1) â€¢ Páµ¢)
\step{ âˆ¨ over âˆ§ }
(Î£ x, i â™ (1 â‰¤ x â‰¤ 3 âˆ§ i = 0) âˆ¨ (1 â‰¤ x â‰¤ 3 âˆ§ i = 1) â€¢ Páµ¢)
\step{ disjoint range split: $1 â‰¤ x â‰¤ 3 âˆ§ i = 0 âˆ§ 1 â‰¤ x â‰¤ 3 âˆ§ i = 1 \;â‰¡\; false$ }
(Î£ x, i â™ 1 â‰¤ x â‰¤ 3 âˆ§ i = 0 â€¢ Páµ¢) + (Î£ x, i â™ 1 â‰¤ x â‰¤ 3 âˆ§ i = 1 â€¢ Páµ¢)
\step{ nesting and one-point rule; definition of $Páµ¢$ }
(Î£ x â™ 1 â‰¤ x â‰¤ 3 â€¢ x) + (Î£ x, i â™ 1 â‰¤ x â‰¤ 3 â€¢ 2^x)
\step{ definition of Î£ }
(1 + 2 + 3) + (2^1 + 2^2 + 2^3)
\end{calc}

We may capture the nesting-rearranging pattern in the above example as a
theorem.
+ (8.15) Distributivity, â€œBody Rearrangmentâ€ :: Provided each quantification is defined,
  \[ (âŠ• x \;â™\; R â€¢ P âŠ• Q) \quad=\quad (âŠ• x \;â™\; R â€¢ P) \,âŠ•\, (âŠ• x \;â™\; R â€¢ Q) \]

# Proof: Nesting and rearranging suffice since Pâ‚€ + Pâ‚ = (âŠ• i â™ i = 0 âˆ¨ i = 1 â€¢ Páµ¢)

This law allows us to break a quantification into two parts, or to combine
two quantifications into one.

- The dummies and ranges are all the same in each quantification in (8.15).
- Ignoring the proviso, we could write $(Î£ i â€¢ 0) = (Î£ i â€¢ i - i) = (Î£ i â€¢ i) +
  (Î£ i â€¢ -i)$ where the left-most is 0 but the rightmost is undefined since it is an
  unbounded sum.
- Quantifications with finite ranges are always defined, and quantifications
  using operator âˆ§ or âˆ¨ are always defined.

** Distributivity Rules
   :PROPERTIES:
   :CUSTOM_ID: Distributivity-Rules
   :END:

 The following axiom shows how distributivity laws generalise to quantifiers.

 *(8.d) Axiom, Distributivity of f:* Provided /x/ does not occur in /P/,
 and â€œf is âŠ•-âŠ—-distributiveâ€ ---i.e., /f (x âŠ• y) = f x âŠ— f y/ and $f\, \Id_âŠ• =  \Id_âŠ—$.
 \[ f\, (âŠ• x \;â™\; R â€¢ P) \quad=\quad (âŠ— x \;â™\; R â€¢ f\, P) \]

 For instance,
 - $\log (Î  x\;â™\;R â€¢ P) \;=\; (Î£ x\;â™\;R â€¢ \log P)$
 - â€œPower of sumsâ€: $a ^{ (Î£ x\;â™\;R â€¢ P)} \;=\; (Î  x\;â™\;R â€¢ a^P)$
 - *Generalised De Morgan (9.18)*: $Â¬ (âˆ€ x\;â™\;R â€¢ P) \;â‰¡\; (âˆƒ x\;â™\;R â€¢ Â¬ P)$
 - *Numeric De Morgan* (LADM Ch15): $- (â†“ x\;â™\;R â€¢ P) \;=\; (â†‘ x\;â™\;R â€¢ - P)$

--------------------------------------------------------------------------------

 *(8.dâ€²) Theorem, Distributivity of âŠ— over âŠ•:* Provided /x/ does not occur in /P/, and
 â€œâŠ— distributes over âŠ•â€ ---/a âŠ— (b âŠ• c) = (a âŠ— b) âŠ• (a âŠ— c)/ and $a âŠ— \Id_âŠ• =
 \Id_âŠ•$.
 \[ P âŠ— (âŠ• x \;â™\; R â€¢ Q) \quad=\quad (âŠ• x \;â™\; R â€¢ P âŠ— Q) \]

 ( Proof: Use 8.d with $f(x) = a âŠ— x$. )

 This form of the distributive law allows us to move constants in and out of a
 quantification.

 The expression $P$ that is being moved out of the scope (or into it, depending
 on your point of view) cannot contain $x$ as a free variable. This restriction
 ensures that the LHS and the RHS of the axiom refer to the same free variables
 ---otherwise, the LHS and RHS would, in general, not be equivalent.

 For instance,
 - (8.d.Î£) $P Â· (Î£ x \;â™\; R â€¢ Q) \quad=\quad (Î£ x \;â™\; R â€¢ P Â· Q)$.
 - (8.d.Î ) $(Î  x \;â™\; R â€¢ Q)^P \quad=\quad (Î  x \;â™\; R â€¢ Q^P)$.
 - (8.d.â†“) $P + (â†“ x \;â™\; R â€¢ Q) \quad=\quad (â†“ x \;â™\; R â€¢ P + Q)$; likewise
   for â†‘ (see Ch15 LADM).
 - *(9.5)  Distributivity of âˆ¨ over âˆ€:*  $P âˆ¨ (âˆ€ x \;â™\; R â€¢ Q) \quadâ‰¡\quad (âˆ€ x \;â™\; R â€¢ P âˆ¨ Q)$.
 - *(9.21) Distributivity of âˆ§ over âˆƒ:*  $P âˆ§ (âˆƒ x \;â™\; R â€¢ Q) \quadâ‰¡\quad (âˆƒ x \;â™\; R â€¢ P âˆ§ Q)$.

 # This allows us to prove the following theorems.
 #
 #  *# (9.6) Body Extraction:* Provided /x/ does not occur in /P/,
 # $(âŠ• x \;â™\; R â€¢ P) \;=\; (âŠ• x â€¢ \mathsf{if}\, R
 # \,\mathsf{then}\, P \,\mathsf{else}\, \Id_âŠ• \mathsf{fi})$
 #
 # \[ (âŠ• x \;â™\; R â€¢ P) \;=\; P âŠ— (âŠ• x â€¢ ) \]
 #
 # c.f., 9.22

--------------------------------------------------------------------------------

 *(8.dâ€²) Theorem, General Distributive Law of âŠ— over âŠ•:* Provided /x/ does not occur
 in /S/ nor /Q/; /y/ does not occur in /R/ nor /P/; and
 â€œâŠ— distributes over âŠ•â€ ---/a âŠ— (b âŠ• c) = (a âŠ— b) âŠ• (a âŠ— c)/ and $a âŠ— \Id_âŠ• =
 \Id_âŠ•$.
 \[ (âŠ• x, y â™ R âˆ§ S â€¢ P âŠ— Q) = (âŠ• x â™ R â€¢ P) âŠ— (âŠ• y \;â™\; S â€¢ Q) \]

 A sum of independent products is the product of sums.

--------------------------------------------------------------------------------

 *(8.dâ€³) Distributivity of âŠ— over non-empty âŠ•:* Provided /x/ does not occur in /P/,
 and â€œâŠ— distributes over non-empty âŠ•â€ ---/a âŠ— (b âŠ• c) = (a âŠ— b) âŠ• (a âŠ— c)/ (but
 maybe not $a âŠ— \Id_âŠ• = \Id_âŠ•$.
 \[ (âˆƒ x â€¢ R) \quadâ‡’\quad P âŠ— (âŠ• x \;â™\; R â€¢ Q) \quad=\quad (âŠ• x \;â™\; R â€¢ P âŠ— Q) \]

Be careful using this theorem. A term can be moved outside the scope of the
quantification only if the range $R$ is not empty ---i.e., not everywhere /false/.

 For instance,
 - *(9.7) Distributivity of âˆ§ over âˆ€:*   $(âˆƒ x â€¢ R) \quadâ‡’\quad P âˆ§ (âˆ€ x \;â™\; R â€¢ Q)
   \quadâ‰¡\quad (âˆ€ x \;â™\; R â€¢ P âˆ§ Q)$
 - *(9.23) Distributivity of âˆ¨ over âˆƒ:*   $(âˆƒ x â€¢ R) \quadâ‡’\quad P âˆ¨ (âˆƒ x \;â™\; R â€¢ Q)
   \quadâ‰¡\quad (âˆƒ x \;â™\; R â€¢ P âˆ¨ Q)$

** COMMENT Hello --------------------------------------------------------------------------------
   :PROPERTIES:
   :CUSTOM_ID: Hello
   :END:
** TODO COMMENT Dummies
   :PROPERTIES:
   :CUSTOM_ID: Dummies
   :END:

The next three axioms concern dummies. The first indicates that nested
quantifications with the same operator can be interchanged.  The second
indicates how a single quantification over a list of dummies can be viewed as a
nested quantification. The third shows that a dummy can be replaced (in a
consistent fashion) by any fresh dummy.

** Permutative Law ---â€œTranslation Ruleâ€
   :PROPERTIES:
   :CUSTOM_ID: Permutative-Law-Translation-Rule
   :END:
Multiple quantification has an interesting connection with the general operation
of changing the index of quantification in /single/ sums.

If we try to replace /x/ by $f\, y$ using an /arbitrary function/ /f/ ---not
necessarily an invertible function--- then the general formula for index
replacement is:
\[ (âŠ• x \;â™\; R â€¢ (âŠ• y â™ f\, y = x â€¢ P)) \quad=\quad (âŠ• y \;â™\; R[x â‰” f\, y] â€¢ P[x â‰” f\, y]) \]

The inner sum of the LHS accounts for the number of values /y/ such that /f y = x/.
When /f/ is a one-to-one correspondence, the number of such values is 1 and so the
inner sum disappears ---we use /f/'s invertible to â€œsolve for yâ€ then use the
one-point rule to get rid of the sum--- to give us the permutative law (8.22).

The poof of the general index replacement rule is easy (once you've seen it!)
- ( We omit the ranges for brevity, since they can always be traded into the body. )
#+begin_src haskell
  (âŠ• y â€¢ P[x â‰” f y])
={- One-point rule -}
  âŠ• y â€¢ (âŠ• x â™ x = f y â€¢ P)
={- Quantifier interchange -}
  âŠ• x â€¢ âŠ• y â™ x = f y â€¢ P
#+end_src

+ Trick: If we have a double sum on index variables /i, j/ whose terms involve /j +
  f i/, where /f/ is an arbitrary function, it's a good idea to try replacing /j/ by
  /j - f i/ and sum on /i/.

--------------------------------------------------------------------------------

The next rule let's us â€œshuffle around the possible values being summed overâ€.
It says that we can reorder the terms in any way we please; here $f$ is any
permutation.

*(8.22) Theorem, Permutative Law, Commutative Law, Change of Dummy:* Provided
/y/ does not occur in /R/ nor in /P/, and /f/ is a permutation (i.e., is invertible),
\[ (âŠ• x \;â™\; R â€¢ P) \quad=\quad (âŠ• y \;â™\; R[x â‰” f\, y] â€¢ P[x â‰” f\, y]) \]

A function /f/ is a permutation, or is invertible, if there is a function $fâ»Â¹$
such that \[x = f\, y \quadâ‰¡\quad fâ»Â¹ x = y\]
- Transformations like $f\, y = y + c$ or $f\, y = y - c$ are always
  permutations, so they always work.

- To see if /f/ is a permutation, start with the equation $x = f\, y$, then â€œ
  solve for /y/ â€ to get $g\, x = y$, then the found /g/ is the inverse of /f/.

The proof in LADM, p151, is very nice.

- This rule can be used to /simplify the range/ or to â€œeliminate a repeated
  (permutation) expression $f$ from the quantificationâ€.

--------------------------------------------------------------------------------

We can relax the permutation restriction a little bit: We need to require only
that there be exactly one value /j/ with /f j = i/ when /i/ satisfies /R/. If /i/ does not
satisfy /R/, it doesn't matter how often /f j = i/ occurs, because such /j/ don't take
part in the sum.

That is, we need â€œ /f/ to be an invertible only for values satisfying /R/ â€; i.e.,
there is a function $fâ»Â¹$ such that
 \[R\, x  âˆ§  R\, y â‡’ \big( x = f\, y \quadâ‰¡\quad fâ»Â¹ x = y \big) \]

 For example, doubling is not invertible, but it is when we are only considering
 even integers:
 \[ \even x âˆ§ \even y â‡’ \big(x = 2 Â· y \quadâ‰¡\quad x / 2 = y \big) \]

As such,
$(âŠ• x\;â™\;R âˆ§ \even x â€¢ P) = (âŠ• y\;â™\;R[x â‰” 2 Â· y] â€¢ P[x â‰” 2 Â· y])$.

--------------------------------------------------------------------------------

** COMMENT Manipulating Ranges. Â§8.4
   :PROPERTIES:
   :CUSTOM_ID: COMMENT-Manipulating-Ranges-8-4
   :END:

*Theorem, Split off (nested) term:*

\[ (âŠ• i, j \;â™\; 0 â‰¤ i â‰¤ j < n + 1 â€¢ P)
\quad=\quad (âŠ• i, j \;â™\; 0 â‰¤ i â‰¤ j < n â€¢ P) âŠ• (âŠ• i, j \;â™\; 0
â‰¤ i â‰¤ n â€¢ P[j â‰” n]) \]

** Trading and Contextual Rules ---Quantification and src_haskell[:exports code]{if-then-else}
   :PROPERTIES:
   :CUSTOM_ID: Trading-and-Contextual-Rules-Quantification-and-src-haskell-exports-code-if-then-else
   :END:

Trading rules let us move a range into the body.

*(8.t) Axiom, Trading:* $(âŠ• x \;â™\; R â€¢ P) \;=\; (âŠ• x â€¢ \mathsf{if}\, R
\,\mathsf{then}\, P \,\mathsf{else}\, \Id_âŠ• \mathsf{fi})$

# For brevity, we may define
# \[ R â‡¨ P \quad=\quad \mathsf{if}\, R \mathsf{then}\, P \,\mathsf{else}\, \Id_âŠ• \mathsf{fi} \]

*(8.tâ€²) Theorem, Trading:* $(âŠ• x \;â™\; Q âˆ§ R â€¢ P) \;=\; (âŠ• x \;â™\; Q â€¢ \mathsf{if}\, R
\,\mathsf{then}\, P \,\mathsf{else}\, \Id_âŠ• \mathsf{fi})$

Define the Iverson brackets, as follows.
#+begin_src haskell
[_] : ğ”¹ â†’ â„•
[ true  ] = 0
[ false ] = 1
#+end_src

For instance:
# let $[\_{}] : ğ”¹ â†’ â„•; [true] = 1, [false] = 0$, then,
- *(8.t.Î£) Theorem, Trading:* $(Î£ x \;â™\; Q âˆ§ R â€¢ P) = (Î£ x \;â™\; Q â€¢ P Â· [R] )$
- *(8.t.Î ) Theorem, Trading:* $(Î  x \;â™\; Q âˆ§ R â€¢ P) = (Î  x \;â™\; Q â€¢ P^{[R]})$
  # - *(8.t.â†“) Theorem, Trading:* $(â†“ x \;â™\; Q âˆ§ R â€¢ P) = (â†“ x \;â™\; Q â€¢ P / [R])$,
  # where the special value $âˆ := 1/0$ is the identity of minima â†“ and the zero
  # of multiplication.
  #
  # Eek!
- *(8.t.âˆ€) (9.4) Theorem, Trading:* $(âˆ€ x \;â™\; Q âˆ§ R â€¢ P) â‰¡ (âˆ€ x \;â™\; Q â€¢ R â‡’ P)$
- *(8.t.âˆƒ) (9.20) Theorem, Trading:* $(âˆƒ x \;â™\; Q âˆ§ R â€¢ P) â‰¡ (âˆƒ x \;â™\; Q â€¢ R âˆ§ P)$

--------------------------------------------------------------------------------

   By contextual rules, and conditional rules,

   *(8.c) Theorem, Context:* $(âŠ• x \;â™\; R âˆ§ e = f â€¢ P[z â‰” e]) = (âŠ• x \;â™\; R â€¢ P[z â‰” f])$

   *(8.câ€²) Theorem, Context:* $(âŠ• x \;â™\; R â€¢ P[z â‰” R]) = (âŠ• x \;â™\; R â€¢ P[z â‰” true])$

** Order Rules
   :PROPERTIES:
   :CUSTOM_ID: Order-Rules
   :END:
Suppose we have an order $\_{}âŠ‘\_{}$.

*(8.m) Axiom, (Generalised) Monotonicity; Body Weakening/Strengthening:*
Provided âŠ• is monotonic ---/a âŠ‘ aâ€² âˆ§ b âŠ‘ bâ€²  â‡’  a âŠ• b  âŠ‘  aâ€² âŠ• bâ€²/---
\[ (âŠ• x\;â™\;R â€¢ P) âŠ‘ (âŠ• x\;â™\;R â€¢ Pâ€²) \quadâ‡\quad (âˆ€ x\;â™\;R â€¢ P âŠ‘ Pâ€²) \]

For example, this covers both of (9.12) and (9.27); consequently giving us
(9.11) and (9.26) ---by weakening $p âˆ§ q â‡’ p$ and $p â‡’ p âˆ¨ q$.

*** Antitonicity Rules for Eliminatory-âŠ•
    :PROPERTIES:
    :CUSTOM_ID: Antitonicity-Rules-for-Eliminatory
    :END:

 Suppose we have an order $\_{}âŠ‘\_{}$
 and say the operation âŠ• is *eliminatory* iff /(a âŠ• b) âŠ‘ a/ for all /a, b/.
 - E.g., for ğ”¹, conjunction is eliminatory.
 - Any meet, $\_{}âŠ“\_{}$ is eliminatory. (See A2; H10)

 *(8.rws.e) Range Weakening/Strengthening for eliminatory-âŠ•:*
 $(âŠ• x \;â™\; R âˆ¨ Q â€¢ P) âŠ‘ (âŠ• x \;â™\; R â€¢ P)$
 #+begin_src math
  (âŠ• x â™ R âˆ¨ Q â€¢ P)
={- Absorption; to prepare for using range split -}
  (âŠ• x â™ R âˆ¨ (Q âˆ§ Â¬ R) â€¢ P)
={- Range split; R âˆ§ (Q âˆ§ Â¬ R) â‰¡ false by contradiction -}
  (âŠ• x â™ R â€¢ P) âŠ• (âŠ• x â™ Q âˆ§ Â¬ R â€¢ P)
âŠ‘{- âŠ• eliminatory -}
  (âŠ• x â™ R â€¢ P)
 #+end_src

 If âŠ• were /introductory/ instead, (see below), then the exact same proof would be
 used, but the last hint would be src_haskell[:exports code]{âŠ’{- âŠ• introductory
 -}}.

 *(8.rws.e) Range Antitonicity, Weakening/Strengthening, for eliminatory-âŠ•:*
 \[(âŠ• x \;â™\; Râ€² â€¢ P) âŠ‘ (âŠ• x \;â™\; R â€¢ P)  \quadâ‡\quad (âˆ€ x â€¢ R â‡’ Râ€²)\]
 Proof:
 #+begin_src math
  (âŠ• x â™ Râ€² â€¢ P)
={- Assumption R â‡’ Râ€² and definition of â‡’ -}
   (âŠ• x â™ R âˆ¨ Râ€² â€¢ P)
âŠ‘{- Range weakening for eliminatory-âŠ• -}
   (âŠ• x â™ R â€¢ P)
 #+end_src

 *(8.i) Generalised Elimination (â€œInstantiationâ€) for eliminatory-âŠ•:*
 \[ (âŠ• x â€¢ P) \;âŠ‘\; P[x â‰” E] \]
 Proof:
 #+begin_src math
  (âŠ• x â€¢ P)
={- Abbreviation; preparing to use one-point rule -}
  (âŠ• x â™ true â€¢ P)
={- Zero of âˆ¨ -}
  (âŠ• x â™ x = E  âˆ¨  true â€¢ P)
âŠ‘{- Range strengtheing for eliminatory âŠ• -}
  (âŠ• x â™ x = E â€¢ P)
={- One-point rule -}
  P[x â‰” E]
 #+end_src

 In the presence of meets, we may obtain an equality:
 $(âŠ• x â€¢ P) = (âŠ• x â€¢ P) âŠ“ P[x â‰” E]$.

*** Monotonicity Rules for Introductory-âŠ•
    :PROPERTIES:
    :CUSTOM_ID: Monotonicity-Rules-for-Introductory
    :END:

 Suppose we have an order $\_{}âŠ‘\_{}$
 and say the operation âŠ• is *introductory* iff /a âŠ‘ (a âŠ• b)/ for all /a, b/.
 - E.g., for â„•, addition is introductory. (See H12!)
 - E.g., for ğ”¹, disjunction is introductory.
 - Joins, $\_{}âŠ”\_{}$ is introductory. (See A2; H10)

 # i.e., âŠ• is non-decreasing

 The rules are analogous to eliminatory-âŠ• and proven similarly.

 - *(8.rws.i) Range Weakening/Strengthening:* $(âŠ• x \;â™\; R â€¢ P) âŠ‘ (âŠ• x \;â™\; R âˆ¨ Q â€¢ P)$
 #+begin_src math :exports none
  (âŠ• x\;â™\;R âˆ¨ Q â€¢ P)
={- Absorption; to prepare for using range split -}
  (âŠ• x\;â™\;R âˆ¨ (Q âˆ§ Â¬ R) â€¢ P)
={- Range split; R âˆ§ (Q âˆ§ Â¬ R) â‰¡ false by contradiction -}
  (âŠ• x\;â™\;R â€¢ P) âŠ• (âŠ• x\;â™\;Q âˆ§ Â¬ R â€¢ P)
âŠ’{- âŠ• introductory -}
  (âŠ• x\;â™\;R â€¢ P)
 #+end_src

 - *(8.rws.i) Range Monotonicity, Weakening/Strengthening, for introductory-âŠ•:*
 \[(âŠ• x \;â™\; R â€¢ P) âŠ‘ (âŠ• x \;â™\; Râ€² â€¢ P)  \quadâ‡\quad (âˆ€ x â€¢ R â‡’ Râ€²)\]

 - *(8.e) Generalised Introduction (â€œâŠ•-Introductionâ€) for introductory-âŠ•:*
    \[ P[x â‰” E] \;âŠ‘\; (âŠ• x â€¢ P) \]
 # Proof:
 #+begin_src math :exports none
  (âŠ• x â€¢ P)
={- Abbreviation; preparing to use one-point rule -}
  (âŠ• x\;â™\;true â€¢ P)
={- Zero of âˆ¨ -}
  (âŠ• x\;â™\;x = E  âˆ¨  true â€¢ P)
âŠ’{- Range weakening for introductory âŠ• -}
  (âŠ• x\;â™\;x = E â€¢ P)
={- One-point rule -}
  P[x â‰” E]
 #+end_src

 In the presence of joins, we may obtain an equality:
 $(âŠ• x â€¢ P) = (âŠ• x â€¢ P) âŠ” P[x â‰” E]$.

** When are two quantifications equal?
   :PROPERTIES:
   :CUSTOM_ID: When-are-two-quantifications-equal
   :END:

    Two quantifications are equal when â€œeach of their parts are equalâ€.

#+begin_src haskell
  (âŠ• x\;â™\;R â€¢ P) = (âŠ•â€² x\;â™\;Râ€² â€¢ Pâ€²)
â‡     âŠ• = âŠ•â€²             -- same operation
  âˆ§  (âˆ€ x â€¢ R â‰¡ Râ€²)      -- same range
  âˆ§  (âˆ€ x\;â™\;R â€¢ P = Pâ€²)  -- same body
#+end_src

See, for example, (9.9).
** COMMENT Range Part
   :PROPERTIES:
   :CUSTOM_ID: COMMENT-Range-Part
   :END:

There are four rules governing manipulation of the range part.

The first two rules govern the case that the range part defines an empty
collection, and the case that the range defined a collection with exactly one
element.

** COMMENT Sums and Recurrences
   :PROPERTIES:
   :CUSTOM_ID: COMMENT-Sums-and-Recurrences
   :END:

1. OK, we understand how to express sums with fancy notation.

   But how does a person actually go about finding the value of a sum?

   One way is to observe that there's an intimate relation between
   sums and recurrences.

   The sum $ğ’®\, n \;=\; (Î£ i â™ 0 â‰¤ i < n â€¢ aáµ¢)$
   is equivalent to the recurrence
   #+begin_src haskell
ğ’® 0       = 0          -- empty range
ğ’® (n + 1) = ğ’® n + aâ‚™   -- split off term
   #+end_src

** Â Predicate Calculus
   :PROPERTIES:
   :CUSTOM_ID: Predicate-Calculus
   :END:

1. /Predicate logic/ is an extension of propositional logic that allows the use of
   variables of types other than ğ”¹.

2. This extension leafs to a logic with enhanced expressive and deductive power.

3. A predicate-calculus formula is a Boolean expression in which some Boolean
   variables may have been replaced by:

   - /Predicates/, which are applications of Boolean functions whose arguments may
     be of types other than ğ”¹; e.g., $x < y$ is formed using the predicate
     symbol â€˜<â€™.

   - Universal and existential quantification, as discussed in this section.

The general axioms of quantification hold for âˆ§/âˆ€ and âˆ¨/âˆƒ.
along with an additional one below.

| $âˆ€ x \;â™\; R â€¢ P$ | â‰ˆ | â€œfor all x satisfying R, P holdsâ€                  |
| $âˆƒ x \;â™\; R â€¢ P$ | â‰ˆ | â€œthere exists an /x/ satisfying /R/ such that /P/ holdsâ€ |

*(9.29) Theorem, Interchange of quantifications:*
$(âˆƒ x\;â™\;R â€¢ (âˆ€ y\;â™\;Q â€¢ P)) \quadâ‡’\quad (âˆ€ x\;â™\;Q â€¢ (âˆƒ x\;â™\;R â€¢ P))$.

( See LADM p166 for the proof. )

** Specific theorems about Î£
   :PROPERTIES:
   :CUSTOM_ID: Specific-theorems-about-Î£
   :END:

+ *â€œAll numbers are obtained by adding 1â€:* $(Î£ i\;â™\;0 â‰¤ i < n â€¢ 1) = n$ ---for $0 â‰¤ n$.

  More generally, *â€œMultiplication is iterated sumâ€*: $(Î£ i\;â™\;0 â‰¤ i < n â€¢ P) = n Â· P$ /provided/ /i/ does not occur in
  /P/ and $0 â‰¤ n$.
  - An index variable that doesn't appear in the body (here /i/) can simply be
    eliminated if we multiply what's left by the size of that variable's index
    set (here /n/).

  *â€œSuperfluous quantification for idempotent âŠ•â€*:
   For âŠ• idempotent, $(âŠ• i\;â™\;0 â‰¤ i < n â€¢ P) = P$ /provided/ /i/ does not occur in
   /P/ and $0 < n$ (i.e., the LHS is non-empty!)
    - â€œWhen you can choose from P, or P, or P, or ... /n/-many times, then really
      there is only /P/ to pick.â€ (E.g., this works for â†‘, â†“, âˆ€, âˆƒ.)

    ( The /0 â‰¤ n/ proviso is to ensure that the LHS's range is not empty and so
    the LHS is not just $\Id_âŠ•$, which may differ from the RHS. )

+ $2 Â· (Î£ i\;â™\;0 â‰¤ i < n â€¢ i) = n Â· (n + 1)$.
  - â€œThe sum of the first /n/ integers is ${n Â· (n + 1) \over 2}$.â€
  - â€œThe sum of the first /n/ even integers is $n Â· (n + 1)$.â€

+ *Exponentiation is iterated productâ€:* $(Î  i \;â™\; 0 â‰¤ i < n â€¢ P) = P^n$
  /provided/ /i/ does not occur in /P/ and /0 < n/.

+ $(Î  i \;â™\; 0 â‰¤ i < n â€¢ i) = n!$, the product of the first $n$ integers
  is $n !$ (â€œn factorialâ€).

+ $(â†“ i \;â™\; 0 â‰¤ i < n â€¢ i) = 0$, the smallest number among the first
  /n/ non-negative integers is 0.

+ $(â†‘ i \;â™\; 0 â‰¤ i â‰¤ n â€¢ i) = n$, the largest number among the first
  /n/ (inclusive) non-negative integers is /n/.

** Infinite Sums
   :PROPERTIES:
   :CUSTOM_ID: Infinite-Sums
   :END:

   1. Everybody knows what a finite sum is: Add up a bunch of terms, one by one,
      until they've all been added.

      But what's an infinite sum?

   2. In an completely ordered domain, *define* $(âŠ• x â™ R â€¢ P)$ to be the least upper bound
      on all finite sub-quantifications; i.e., it is the least /ğ’¬/ such that $(âŠ• x
      â™ F â€¢ P) âŠ‘ ğ’¬$ for all /finite/ ranges /F/ with /âˆ€ x â€¢ F â‡’ R/.

      ( Completeness implies that the set of all such values ğ’¬ always has a
      least element. )

      If there is no bounding constant ğ’¬, we *define* $(âŠ• x â™ R â€¢ P) = âŠ¤$ ---the
      â€œâˆâ€ of the ordered domain.  This means that if ğ’¬ is an value, there's a
      set of finitely many terms /P/ whose sum exceeds ğ’¬.

      --------------------------------------------------------------------------------

      Formally,
      $(âŠ• x \;â™\; R â€¢ P) \quad=\quad (âŠ” F \;â™\; F \,\text{finite-sub-range-of}\, R â€¢
      (âŠ• x \;â™\; F â€¢ P))$

      The completeness criterion means that the join in the RHS /always/
      converges.

      In-particular, a sum indexed by all the natural numbers
      is the result of looking at partial sums.
      \[ (âŠ• i : â„• â€¢ P) \quad=\quad (âŠ” n : â„• â€¢ (âŠ• i : â„• \;â™\; i â‰¤ n â€¢ P)) \]

      :Nope:
      Look at CM, p61

      Hence, to prove a property holds for possibly infinite ranges, we can use
      this reformation to let us only consider finite ranges. Then we may
      manipulate the finite ranges ---say using induction, which is the basis of
      all the finite quantification rules--- then use this definition to remove
      the supremum operator, â€˜âŠ”â€™, and get back to a top-level quantification.

      - The good news is that the manipulation rules are perfectly valid whenever
        we're dealing with sums that converge, as just defined.

      - We verify this by showing that each transformation rule preserves the
       value of all convergent sums. This means, more explicitly, that we must
       prove the quantifier axioms hold for possibly infinite ranges; everything
       else (the resulting theorems) have been derived from the axioms.

      - For example, let's prove the nesting rule: Convergent sums over multiple
        indices can always be summed first with respect to any one of those indices.

        1. Suppose $(âŠ• i, j â™ I âˆ§ J â€¢ P)$ converges to $ğ’¬$.
        2. Let us show that for each $i$, the sum $(âŠ• j â™ J â€¢ P)$ converges
           to some value ğ’¬áµ¢, and $(âŠ• i â™ I â€¢ ğ’¬áµ¢)$ converges to ğ’¬.

        3. By assumption, $(âŠ• i, j â™ F â€¢ P) âŠ‘ ğ’¬$ for all finite sub-ranges $F$
           of $I âˆ§ J$. As such, for any $i$ satisfying $I$, and $Fáµ¢$ a finite
           sub-range of $F$, we have that $(âŠ• j â™ Fáµ¢ â€¢ P) âŠ‘ ğ’¬$ ---provided âŠ• is
           range-monotonic! [Maybe]

        4. Hence, the finite sums $(âŠ• j â™ Fáµ¢ â€¢ P)$ are bounded above and so, by
           definition, each $(âŠ• j â™ J â€¢ P)$ converges to a value, say, ğ’¬áµ¢.

        5. Next, we need to show that the sum of the ğ’¬áµ¢ is ğ’¬.

           1. Suppose /G/ is a finite sub-range of /J/.

        :END:

   3. Notice the definition for possibly infinite ranges /R/ has been formulated
      to that it doesn't depend on any ordering of the terms ---after all, we
      want to preserve the symmetry and associativity of âŠ• even for infinite
      ranges.

   # 4. Numerically this is just a limit: $(Î£_{i â‰¥ 0} aáµ¢ \;=\; \lim_{n â†’ âˆ} Î£^n_{i
   #   = 0} aáµ¢)$. See /Concrete Mathematics, Â§2.7/ for details.

* TODO COMMENT Sets
  :PROPERTIES:
  :CUSTOM_ID: Sets
  :END:
* TODO COMMENT Relations and Functions
  :PROPERTIES:
  :CUSTOM_ID: Relations-and-Functions
  :END:
* TODO COMMENT Induction and Sequences
  :PROPERTIES:
  :CUSTOM_ID: Induction-and-Sequences
  :END:
* TODO COMMENT Graphs and Counting
  :PROPERTIES:
  :CUSTOM_ID: Graphs-and-Counting
  :END:
