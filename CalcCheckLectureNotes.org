# -*- eval: (my/execute-startup-blocks) -*-
# 56
#+title: Mathematics for Computing
#+subtitle: Abridged Lecture Notes @@html:<br>@@ Based on â€œA Logical Approach to Discrete Mathâ€
#+author: Musa Al-hassy
#+options: tags:nil d:nil toc:t
#+PROPERTY: header-args:calccheck :tangle (concat (file-name-sans-extension (buffer-name)) ".calc") :comments nil

#+Description: Abridged Lecture Notes Based on â€œA Logical Approach to Discrete Mathâ€

# Make HTML
# (f-move (org-html-export-to-html) "~/CalcCheck/LectureNotes.html")
#+HTML_HEAD: <link href="https://alhassy.github.io/org-notes-style.css" rel="stylesheet" type="text/css" />
#+HTML_HEAD: <link href="https://alhassy.github.io/floating-toc.css" rel="stylesheet" type="text/css" />
#+HTML_HEAD: <link href="https://alhassy.github.io/blog-banner.css" rel="stylesheet" type="text/css" />
# The last one has the styling for lists.

# One of the first problems was that mathematicians hardly manipulate their formu- lae: instead, they interpret them; and one of the reasons is that the notation they use is not adequate for manipulation.

#+begin_center
#+include: ~/Desktop/colours_palette.html export html
#+end_center

* keywords                                                           :ignore:
  :PROPERTIES:
  :CUSTOM_ID: keywords
  :END:
#+begin_details Keywords

#+begin_documentation template
#+end_documentation
# doc:template

#+begin_documentation Theorem
A /theorem/ is a syntactic object, a string of symbols with a particular property.

A /theorem/ of a calculus is either an axiom or the conclusion of an inference
rule whose premises are theorems.

Different axioms could lead to the same set of theorems, and many texts use
different axioms.
#+end_documentation
doc:Theorem

#+begin_documentation Calculus
A /calculus/ is a method or process of reasoning by calculation
with symbols. A /propositional calculus/ is a method of calculating with Boolean
(or propositional) expressions.
#+end_documentation
doc:Calculus

#+begin_documentation Propositional Calculus
A /calculus/ is a method or process of reasoning by calculation
with symbols. A /propositional calculus/ is a method of calculating with Boolean
(or propositional) expressions.
#+end_documentation
doc:Propositional_Calculus

#+begin_documentation Axiomatic Semantics
Often operations are defined by how they are evaluated (â€œoperationallyâ€), we
take the alternative route of defining operations by how they can be manipulated
(â€œaxiomaticallyâ€); i.e., by what properties they satisfy.

For instance, we may define basic manipulative properties of operators ---i.e.,
/axioms/--- by considering how the operators behave operationally on particular
expressions. That is, one may use an operational, intuitive, approach to obtain
an axiomatic specification (characterisation, interface) of the desired
properties.

More concretely, since $(p â‰¡ q) â‰¡ r$ and $p â‰¡ (q â‰¡ r)$ evaluate to
the same value for any choice of values for $p, q, r$, we may insist that a part
of the definition of equivalence is that it be an associative operation.

Sometimes a single axiom is not enough to â€˜pin downâ€™ a unique operator ---i.e.,
to ensure we actually have a well-defined operation--- and other times this is
cleanly possible; e.g., given an ordering â€˜â‰¤â€™(â€˜â‡’, âŠ†, âŠ‘â€™) we can define minima
â€˜â†“â€™ (â€˜âˆ§, âˆ©, âŠ“â€™) by the axiom: â€œx â†“ y is the greatest lower boundâ€; i.e., $z â‰¤ x
â†“ y \quadâ‰¡\quad z â‰¤ x \,âˆ§\, z â‰¤ y$.
#+end_documentation
doc:Axiomatic_Semantics

#+begin_documentation Operational Semantics
Often operations are defined by how they are evaluated (â€œoperationallyâ€), we
take the alternative route of defining operations by how they can be manipulated
(â€œaxiomaticallyâ€); i.e., by what properties they satisfy.

For instance, we may define basic manipulative properties of operators ---i.e.,
/axioms/--- by considering how the operators behave operationally on particular
expressions. That is, one may use an operational, intuitive, approach to obtain
an axiomatic specification (characterisation, interface) of the desired
properties.

More concretely, since $(p â‰¡ q) â‰¡ r$ and $p â‰¡ (q â‰¡ r)$ evaluate to
the same value for any choice of values for $p, q, r$, we may insist that a part
of the definition of equivalence is that it be an associative operation.

Sometimes a single axiom is not enough to â€˜pin downâ€™ a unique operator ---i.e.,
to ensure we actually have a well-defined operation--- and other times this is
cleanly possible; e.g., given an ordering â€˜â‰¤â€™(â€˜â‡’, âŠ†, âŠ‘â€™) we can define minima
â€˜â†“â€™ (â€˜âˆ§, âˆ©, âŠ“â€™) by the axiom: â€œx â†“ y is the greatest lower boundâ€; i.e., $z â‰¤ x
â†“ y \quadâ‰¡\quad z â‰¤ x \,âˆ§\, z â‰¤ y$.
#+end_documentation
doc:Operational_Semantics

#+begin_documentation Associative
An operation _âŠ•_ is associative when it satisfies $(p âŠ• q) âŠ• r = p âŠ• (q âŠ• r)$.

Associativity allows us to be informal and insert or delete pairs of
parentheses in sequences of âŠ•'s, just as we do with sequences of
additions ---e.g., $a + b + c + d$ is equivalent to $a + (b + c) + d$.

Hence, we can write $p âŠ• q âŠ• r$ instead of $(p âŠ• q) âŠ• r$ or $p âŠ• (q âŠ• r)$.

When an operation is associative, it is best to avoid â€œmaking a choiceâ€ of how
sequences of âŠ• should be read, by using parentheses ---unless to make things
clear or explicit for manipulation.

--------------------------------------------------------------------------------

More generally, for any two operations _âŠ•_ and _âŠ_, the â€œ(left to right) mutual
associativity of âŠ• and âŠâ€ is the property $(x âŠ• y) âŠ z = x âŠ• (y âŠ z)$. It allows
us to omit parentheses in mixed sequences of âŠ• and âŠ. For instance, addition and
subtraction are (left to right) mutually associative.

#+end_documentation
doc:Associative

#+begin_documentation Identity
An operation _âŠ•_ has identity ğ‘° when it satisfies $ğ‘° âŠ• x = x = x âŠ• ğ‘°$.

If it satisfies only the first equation, $ğ‘° âŠ• x = x$, one says
that â€œğ‘° is a left-identity for âŠ•â€. If it satisfies only the second
equation, $x âŠ• ğ‘° = x$, one says that â€œğ‘° is a right-identity for âŠ•â€.

For example, implication only has a left identity, $(false â‡’ x) = x$, and
subtraction only has a right identity, $(x - 0) = x$.

An identity implies that occurrences of â€œâŠ• ğ‘°â€ and â€œğ‘° âŠ•â€ in an expression are
redundant. Thus, $x âŠ• ğ‘°$ may be replaced by $x$ in any expression without
changing the value of the expression. Therefore, we usually eliminate such
occurrences unless something encourages us to leave them in.
#+end_documentation
doc:Identity

#+begin_documentation Distributive
An operation âŠ— distributes over âŠ• when they satisfy
â€œleft-distributivityâ€ $x âŠ— (y âŠ• z) = (x âŠ— y) âŠ• (x âŠ— y)$
and
â€œright-distributivityâ€ $(y âŠ• z) âŠ— x = (y âŠ— x) âŠ• (z âŠ— x)$.

When âŠ• = âŠ—, one says that the operation is â€œself-distributiveâ€.

Distributivity can be viewed in two ways, much like distributivity of
multiplication Ã— over addition +. Replacing the left side by the right side
could be called â€œmultiplying outâ€; replacing the right side by the left side,
â€œfactoringâ€.
#+end_documentation
doc:Identity

#+begin_documentation Metatheorem
A /theorem/ in the technical sense is an expression derived
from axioms using inference rules.

A /metatheorem/ is a general *statement* about a logic that
one argues to be *true*.

For instance, â€œany two theorems are equivalentâ€ is a statement that speaks about
expressions which happen to be theorems. A logic may not have the linguistic
capability to speak of its own expressions and so the statement may not be
expressible as an expression *within* the logic ---and so cannot be a theorem of
the logic.

For instance, the logic ğ’‘ğ‘ has expressions formed from the symbols â€œğ’‘â€, â€œğ’’â€, and
â€œ-â€ (dash). It has the axiom schema $xğ’‘-ğ’’x-$ and the rule â€œIf $xğ’‘yğ’’z$ is a theorem
then so is $x-ğ’‘y-ğ’’z-$â€. Notice that $x, y, z$ are /any/ strings of dashes;
the language of this logic does not have variables and so cannot even speak
of its own expressions, let alone its own theorems!

[Informal] theorems about [technical, logic-specific] theorems are thus termed
â€˜metatheoremsâ€™.
#+end_documentation
doc:Metatheorem

#+begin_documentation Programming
Programming is solving the equation /R â‡’[C] G/ in the unknown /C/; i.e., it is the
activity of finding a â€˜recipeâ€™ that satisfies a given specification. Sometimes
we may write /R â‡’[?] G/ and solve for â€˜?â€™. Programming is a goal-directed activity: From a specification, a program is found by examining the shape of its postcondition.
#+end_documentation
doc:Programming

#+begin_documentation Specification
 A specification is an equation of a certain shape.
 /Programming/ is the activity of solving a specification
 for its unknown. Its unknown is called a /program/.

 See also â€œProgrammingâ€.
#+end_documentation
doc:Specification

#+begin_documentation Expression

A â€˜variableâ€™ or a â€˜function applicationâ€™; i.e., the name of a function along
with a number of existing expressions.

In a sense, an expression is like a sentence with the variables acting as
pronouns and the function applications acting as verb clauses and the argument
to the application are the participants in the action of the verbal clause.
#+end_documentation
doc:Expression

#+begin_documentation Algorithmic Problem Solving
There are two ways to read this phrase.

Algorithmic-problem solving is about solving problems that
involve the construction of an algorithm for their solution.

Algorithmic problem-solving is about problem solving in general,
using the principles of correct-by-construction algorithm-design.

#+end_documentation
doc:Algorithmic_Problem_Solving
# Computing science is all about solving algorithmic problems (or, as some authors pre- fer to say, it is all about instructing computers to solve problems).

#+begin_documentation Calculational Proof
A story whose events have smooth transitions connecting them.

A proof wherein each step is connected to the next step by an explicit
justification.
#+end_documentation
doc:Calculational_Proof

â€¦I'll add more in timeâ€¦
#+end_details
* Notational Setup                                                   :ignore:
  :PROPERTIES:
  :CUSTOM_ID: Notational-Setup
  :END:
:Calc_notation:

\begin{calc}
x
\step[op]{ hint }
y
\end{calc}

:End:

#+name: startup-code
#+begin_src emacs-lisp :exports none
(load-file "~/Desktop/power-blocks.el")
#+end_src

#+BEGIN_export html
<style>

/* Using source blocks â€œmathâ€ as aliaas for haskell */
pre.src-math:before { content: 'Mathematical! Algebraic! Axiomatic!'; }
/* Execute this for alias: (add-to-list 'org-src-lang-modes '("math" . haskell)) */

</style>
#+END_export

# The following snippet let's us export calc clauses in HTML nicely.
#+begin_latex-definitions
\def\BEGINstep{ \left\langle }
\def\ENDstep{ \right\rangle }
\newcommand{\step}[2][=]{ \\ #1 \;\; & \qquad \color{maroon}{\BEGINstep\text{ #2
} \ENDstep} \\ & }

% multi-line step with many lines of text
\newcommand{\line}[1]{ \text{#1}\hfill\\ }
\newcommand{\stepmany}[2][=]{ \\ #1 \;\; & \qquad \color{maroon}{\BEGINstep \large\substack{ #2 } \ENDstep} \\ & }

% multi-line step with 4 lines of text
\newcommand{\stepfour}[5][=]{ \stepmany[#1]{\line{#2} \line{#3} \line{#4}
\line{#5}} }

\newenvironment{calc}{\begin{align*} & }{\end{align*}}

% Inference rules
\def\And{\quad}
\newcommand\Rule[3][]{ {#2 \over #3}\mathsf{#1} }

\def\eq{\,=\,}

\def\true{\mathsf{true}}
\def\false{\mathsf{false}}

\def\even{\mathsf{even}\,}
#+end_latex-definitions

#+html: <p style="display:none">
$$\newcommand\exp[1]{\mathsf{exp}_{#1}\,}
%
% for calc environment
% line breaks with extra whitespace using phantom formula
% \hdashline
\def\NL{\\ \phantom{Î£} \\}
\def\NLtwo{\\ \phantom{\substack{Î£ \\ Î£}} \\}
$$
#+html: </p>


# This snippet let's us, in an org file, do C-c C-x C-l to see the calculation
# rendered prettily. It will not work if you do #+begin_calc â€¦ #+end_calc.
#+begin_src emacs-lisp :exports none
(add-to-list 'org-latex-packages-alist
  '("fleqn, leqno, block" "calculation" t))

(setq org-format-latex-header
      (concat org-format-latex-header
              "\\usepackage{color}
               \\def\\BEGINstep{ \\langle }
               \\def\\ENDstep{ \\rangle }
               \\newcommand{\\step}[2][=]{ \\\\ #1 \\;\\; & \\qquad \\color{maroon}{\\BEGINstep \\text{ #2 } \\ENDstep} \\\\ & }
               \\newenvironment{calc}{\\begin{align*} & }{\\end{align*}}"))
#+end_src

:hide:
 \begin{calc}
  x
\step{nice}
  y
\end{calc}
:end:

# $1 colour eg â€œpinkâ€ or â€œhsl(157 75% 20%)â€ or â€œ#e5f5e5â€; $2 title
#+macro: begin-box @@html: <div style="padding: 1em; background-color: $1; border-radius: 15px; font-size: 0.9em; box-shadow: 0.05em 0.1em 5px 0.01em  #00000057;"> <h3>$2</h3>@@

#+macro: end-box @@html: </div>@@

:Hide:
  {{{begin-box(teal, Salam!)}}}

  {{{end-box}}}
:End:


   #+begin_latex-definitions
\newcommand{Law}[3][]{ #1\;\;\textbf{#2:}\quad #3 }
   #+end_latex-definitions

* subtle colours :ignore:
  :PROPERTIES:
  :CUSTOM_ID: subtle-colours
  :END:

#+name: startup-code
#+begin_src emacs-lisp  :exports none
(defun subtle-colors (c)
  "Names are very rough approximates.

   Translations from: https://www.december.com/html/spec/softhues.html"
  (pcase c
    ("teal"    "#99FFCC") ;; close to aqua
    ("brown"   "#CCCC99") ;; close to moss
    ("gray"    "#CCCCCC")
    ("purple"  "#CCCCFF")
    ("lime"    "#CCFF99") ;; brighter than â€˜greenâ€™
    ("green"   "#CCFFCC")
    ("blue"    "#CCFFFF")
    ("orange"  "#FFCC99")
    ("peach"   "#FFCCCC")
    ("pink"    "#FFCCFF")
    ("yellow"  "#FFFF99")
    ("custard" "#FFFFCC") ;; paler than â€˜yellowâ€™
    (c c)
  ))
#+end_src

#+RESULTS: startup-code
: t

# $1 colour eg â€œpinkâ€ or â€œhsl(157 75% 20%)â€ or â€œ#e5f5e5â€; $2 title
#+macro: begin-box (eval (concat "@@html: <div style=\"padding: 1em; background-color: " (subtle-colors $1) "; border-radius: 15px; font-size: 0.9em; box-shadow: 0.05em 0.1em 5px 0.01em  #00000057;\"> <h3>" $2 "</h3>@@"))

#+macro: end-box @@html: </div>@@

:Hide:
â€œSubtle coloursâ€
#+begin_parallelNB

   {{{begin-box(teal,    This is â€œtealâ€!)}}} {{{end-box}}} \\
   {{{begin-box(brown,   This is â€œbrownâ€!)}}} {{{end-box}}} \\
   {{{begin-box(gray,    This is â€œgrayâ€!)}}} {{{end-box}}} \\
   {{{begin-box(purple,  This is â€œpurpleâ€!)}}} {{{end-box}}} \\
   {{{begin-box(lime,    This is â€œlimeâ€!)}}} {{{end-box}}} \\
   {{{begin-box(green,   This is â€œgreenâ€!)}}} {{{end-box}}} \\
   {{{begin-box(blue,    This is â€œblueâ€!)}}} {{{end-box}}} \\
   {{{begin-box(orange,  This is â€œorangeâ€!)}}} {{{end-box}}} \\
   {{{begin-box(peach,   This is â€œpeachâ€!)}}} {{{end-box}}} \\
   {{{begin-box(pink,    This is â€œpinkâ€!)}}} {{{end-box}}} \\
   {{{begin-box(yellow,  This is â€œyellowâ€!)}}} {{{end-box}}} \\
   {{{begin-box(custard, This is â€œcustardâ€!)}}} {{{end-box}}} \\

#+end_parallelNB
:End:
* Introduction to Calculational Reasoning
  :PROPERTIES:
  :CUSTOM_ID: Introduction-to-Calculational-Reasoning
  :END:
** What are Calculational Proofs
   :PROPERTIES:
   :CUSTOM_ID:
   :END:

 We advocate *calculational proofs* in which reasoning is goal directed and
 justified by simple axiomatic laws that can be checked syntactically rather
 than semantically. ---/Program Construction/ by Roland Backhouse

 For example, below are two arguments showing that $\sqrt[n]{k}$ is a rational
 number precisely when /integer/ $k$ is a so-called perfect /n/-th root ---consequently,
 since 2 is not a perfect square, $\sqrt{2}$ is not rational.

{{{begin-box(#e5f5e5, Conventional Proof: Non-perfect powers have irrational
roots)}}}

Suppose that /integer/ $k$ is not a perfect /n/-th power ---i.e., not of the form /ğ“â¿/---
then there is some prime $p$ in the factorisation of $k$ that has its exponent,
say $\exp{p} k$, being not a multiple of $n$.  But if $\sqrt[n]{k} \eq
a/b$ then $\exp{p}{k} \eq \exp{p}{a^n} - \exp{p}{b ^ n} \eq n Â· \exp{p}{a} - n Â·
\exp{p}{b}$ and the difference of multiples of $n$ is a multiple of $n$, and so
we have a contradiction. Hence, no such $a, b$ could exist and so $\sqrt[n]{k}$
is irrational.
{{{end-box}}}

This is an example of an *informal proof*, which is a mixture of natural language,
English, and mathematical calculations.  The English text outline the main steps
of the proof, and the mathematical calculations fill in *some* of the details.

Since they only communicate the key ideas, such proofs are preferred by writers
but they place a large semantic burden on readers who are expected to have such
a good understanding of the problem domain that the details of the outline
proofs can be filled in, and so the writer leaves these as an implicit exercise
to the reader.

However, even worse, such informal outline proofs may skip over important
details and thus can be wrong!

Below is a *calculational proof*. It introduces notation and recalls theorems as
needed, thereby making each step of the argument easy to verify and follow.  As
such, the following argument is more accessible to readers unfamiliar with the
problem domain.

{{{begin-box(#e5f5e5, Calculational Proof)}}}

 \begin{calc}
 \def\BEGINstep{\left[} \def\ENDstep{\right.}
 \sqrt[n]{k} \text{ is a rational number }
 \stepfour{ A rational number is the fraction of two integers.}{
          Let variables $a,\, b$ range over integer numbers.}{}{
  }
  âˆƒ\, a, b â€¢\; \sqrt[n]{k} = {a \over b}
 \step{ Use arithmetic to eliminate the $n$-th root operator.
  }
  âˆƒ\, a, b â€¢\; k Â· a ^n = b ^n
  \stepmany{ \line{Let $\exp{m} x$ be the number of times that $m$ divides $x$.}
   \line{For example, $\exp{2} 48 \eq 4$ and $\exp{2} 49 \eq 0$.}
   \NL
   \line{The numbers $p$ with $âˆ€ m : â„¤âº \,â€¢\, \exp{m}p \,â‰ \, 0 \,â‰¡\, m \,=\, p$ are called $prime$ numbers.}
   \line{Let variable $p$ ranges over prime numbers. }
   \NL
   \line{Fundamental theorem of arithmetic: Numbers are determined by their prime powers.}
   \line{That is, $\big(âˆ€ \,p\, â€¢\; \exp{p} x \eq f(p)\big) \;â‰¡\; x \,=\, \big(Î \, p\, â€¢\; p^{f(p)}\big)$ for any $f$.}
   \line{As such, every number is the product of its prime powers:}
   \line{$\qquad x \eq \big(Î  \,p\, â€¢\; p^{\exp{p} x}\big)$. }
   \line{And so, any two numbers are the same precisely when they have the same primes:}
   \line{$\qquad x \eq y \;â‰¡\; \big(âˆ€ p \,â€¢\, \exp{p} x \eq \exp{p} y\big)$.}
  }
  âˆƒ\, a, b â€¢\; âˆ€\, p â€¢\; \exp{p}(k Â· a ^n) \eq \exp{p}(b ^n )
  \stepmany{\line{When $p$ is prime, $\exp{p}(x Â· y) \eq \exp{p} x \,+\, \exp{p} y$.}
   \line{Aside: In general, $\exp{p}(Î  \,i\, \,â€¢\, x_i) \eq (Î£ \,i\, \,â€¢\, \exp{p} x_i)$.}
  }
  âˆƒ\, a, b â€¢\; âˆ€\, p â€¢\; \exp{p} k + n Â· \exp{p} a \eq n Â· \exp{p} b
  \step{ Use arithmetic to collect similar terms.
  }
  âˆƒ\, a, b â€¢\; âˆ€\, p â€¢\; \exp{p} k \eq  n Â· \Big(\exp{p} b - \exp{p} a\Big)
  \stepmany{ \line{(â‡’) is the definition of multiplicity;}
             \line{(â‡) take $a \,â‰”\, 1$ and define $b$ by its prime powers:}
             \line{ $\qquad âˆ€\, p \,â€¢\, \exp{p} b \,â‰”\, {\exp{p} k \,/\, n}$}
  }
  âˆ€\, p â€¢\; \exp{p} k \text{ is a multiple of } n
  \step{ Fundamental theorem of arithmetic and definition of â€˜perfectâ€™ }
  k \text{ is a perfect $n$-th power; i.e., of the shape } x^n
\end{calc}

{{{end-box}}}

# Go back to the âŸ¨hint notationâŸ©.
#+begin_latex-definitions
\def\BEGINstep{ \left\langle }
\def\ENDstep{ \right\rangle }
#+end_latex-definitions

Observe that the calculational form is *more general*.  The use of a
/formal/ approach let us keep track of when our statements are equivalent (â€œ=â€)
rather than being weakened (â€œâ‡’â€).  That is, the use of English to express the
connection between steps is usually presented naturally using â€œif this, then
thatâ€ statements ---i.e., implication--- rather than stronger notion of equality.
- In contrast, the conventional proof is a â€˜proof by contradictionâ€™;
  a method that is over-used.

- Other features of conventional proofs are the /dot dot dot/ notations, â€œâ‹¯â€, to
  indicate â€œand so on, of the same idea/formâ€ ---leaving readers the burden to
  guess the generic shape of the â€˜idea/formâ€™ that should be repeated.

  Calculational proofs use quantifiers ---and loops--- instead.
- Finally, conventional proofs tend to use prefix notation and thereby
  implicitly forcing a syntactic distinction between equivalent expressions;
  e.g., $\gcd(m, \gcd(n, p))$ and $\gcd(\gcd(m, n), p)$.

The above proof is a generalisation of a proof in Backhouse's text for square
roots, which may be viewed as a [[https://youtu.be/t39wHoFHbvY][Youtube video]] which makes use of kbd:CalcCheck
[[https://alhassy.github.io/CalcCheck/Docs][â‡­]]: A proof checker for the logic of â€œA Logical Approach to Discrete Mathâ€
(â€˜LADMâ€™).
    - It /checks/ your arguments in a notation similar to that of the book.
    - *You can check your work before handing it in.*
    - You can formalise your own theorems from other books and check them
      ---unlimited exercises!

    #+begin_center
    kbd:Control_+_Alt_+_Enter to check a cell.
    #+end_center

     Going forward, instead of defining expressions by how they are evaluated,
     we define expressions in terms of how they can be manipulated.
     # operational versues aximatic method.

     A *<<<calculus>>>* is a method or process of reasoning by calculation with
     symbols.
     A Boolean variable that can denote a proposition is sometimes called a
     /propositional variable/.
     A *<<<propositional calculus>>>* is so named beacuse it is a method
     of calculating with expressions that involve propositional variables.

  {{{begin-box(pink, The propositional calculus of LADM is called â€œequational
  logic ğ‘¬â€)}}}
     One part of ğ‘¬ is a set of /axioms/, which are certain Boolean expressions
     that define basic manipulative properties of Boolean operators.
     For example, the axiom $p âˆ¨ q â‰¡ q âˆ¨ p$ indicates (semantically)
     that the value of a disjunction doesn't depend on the order of its arguments
     and (syntactically) we may swap their order when manipulating expressions.
     The other part of this calculus are the 3 inference rules Substitution,
     Leibniz, and Transitivity.

     A *<<<theorem>>>* of this calculus is either an axiom, the conclusion
     of an inference rule whose premises are theorems, or a Boolean expression
     that, using the inference rules, is proved equal to an axiom or a previously
     proved theorem.
  {{{end-box}}}

#+begin_box Algorithmic Problem Solving ---â€œmath is programmingâ€
Problems may be formulated and solved using, possibly implicitly, the
construction of correct programs:

|   | â€œfor all $x$ satisfying $R(x)$, there is a $y$ such that $G(x, y)$ is trueâ€ |
| â‰ˆ | /âˆ€ x â€¢ R x â‡’ âˆƒ y â€¢ G x y/                                                     |
| â‰ˆ | ~R â‡’[C] G~ for some program command ~C~ with inputs /x/ and outputs /y/             |

This is known as a /constructive proof/ since we have an algorithm ~C~ that actually
shows how to find a particular $y$ to solve the problem, for any given $x$.  In
contrast, non-constructive proofs usually involving some form of counting
followed by a phrase â€œthere is at least one such $y$ â€¦â€, without actually
indicating /how/ to find it!
# p âˆ¨ Â¬ p ...!

More concretely,
|   | Any two consectuive Fibonnaci numbers are coprime    |
| â‰ˆ | /âˆ€ n â€¢ n â‰¥ 1 â‡’ gcd(fib n, fib (n + 1)) = 1/            |
| â‰ˆ | ~a = fib n  âˆ§  b = fib (n + 1)  âˆ§  n â‰¥ 0~              |
|   | ~Â Â Â Â Â Â Â Â Â Â Â Â â‡’[C]~                                     |
|   | ~a = b = gcd(fib n, fib (n + 1)) = 1~, for a program ~C~ |
#+end_box

** What is Discrete Mathematics
   :PROPERTIES:
   :CUSTOM_ID: Discrete-Mathematics
   :END:
 1. *Discrete Mathematics*
    includes logic (calculational reasoning), (data) sets, functions, relations,
    graphs, inductive types, and more.

    Conscious and fluent use of the language of (discrete) mathematics
    is the foundation for precise specification and rigorous reasoning
    in Computer Science and Software Engineering

 2. *Goal*: Understand the mechanics of mathematical expressions and proof.

 3. <<<Propositional>>>: Statements that can be either /true/ or /false/; not numbers.

    <<<Predicate>>>: Propositional statement about some subjects.

 4. <<<Calculus>>>: Formalised reasoning through calculation.

    â€˜Hand wavyâ€™ English arguments tend to favour /case analysis/
       ---considering what could happen in each possible scenario---
       which increases exponentially with each variable; in contrast,
       equality-based calculation is much simpler since it delegates
       intricate case analysis into codifed algebraic laws.

       E.g., Portia's Suitor's Dilemma has 4 unknowns, each being either true or false,
       and so has $2^4$ many possible scenarios to consider. Whereas a
       calculation solving the problem can be formed in less than 10 super simple
       lines.

       #+begin_details Portia's Suitor's Dilemma

 Portia has a gold casket and a silver casket and has placed a picture of herself
 in one of them. On the caskets, she has written the following inscriptions:

 + Gold ::  The portrait is not in here
 + Silver :: Exactly one of these inscriptions is true.

 Portia explains to her suitor that each inscription may be true or false, but
 that she has placed her portrait in one of the caskets in a manner that is
 consistent with the truth or falsity of the inscriptions.

 If the suitor can choose the casket with her portrait, she will marry him.

 -----

 ( This is a â€˜teaserâ€™; you're not expected to know the details in the following
 calculation. )

 Formalisation is the first step towards solution!

 #+begin_src calccheck
Declaration: G, S : ğ”¹

Explanation: G â‰” â€œThe inscription on the gold casket is trueâ€
Explanation: S â‰” â€œThe inscription on the silver casket is trueâ€
 #+end_src

 â€¦ and
 #+begin_src calccheck
Declaration: gc : ğ”¹
Explanation: gc â‰” â€œThe portrait is in the gold casketâ€
 #+end_src

â€¦ we know the portrait is in a casket preciely when
that casket's inscription is true â€¦

 #+begin_src calccheck
Axiom â€œInscription on gold casketâ€: G â‰¡ Â¬ gc
Axiom â€œInscription on silver casketâ€: S â‰¡ (S â‰¡ Â¬ G)
 #+end_src

 â€¦ let us start from what we know about the silver casket:
 #+begin_src calccheck
Calculation:
    S â‰¡ (S â‰¡ Â¬ G)    â€” This is â€œInscription on silver casketâ€
  â‰¡âŸ¨ â€œReflexivity of â‰¡â€ âŸ©
    S â‰¡ S â‰¡ Â¬ G
  â‰¡âŸ¨ â€œSymmetry of â‰¡â€ âŸ©
    Â¬ G
  â‰¡âŸ¨ â€œInscription on gold casketâ€ âŸ©
    Â¬ Â¬ gc
  â‰¡âŸ¨ â€œDouble negationâ€ âŸ©
    gc
 #+end_src

 By just *simplifying*, we calculated that the portrait is in the gold casket!

 # See below [[#shape-of-calculations][The Shape of Calculations]] for more on /exploratory calculations/.

 #+end_details

 #+begin_quote
/Knowledge is software for your brain: The more you know, the more problems you
can solve!/
 #+end_quote

# Time for an upgrade!

** Road-map
   :PROPERTIES:
   :CUSTOM_ID: Road-map
   :END:

 In the previous section, we showed how a calculational argument is more structured
 and may be more accessible. Before getting to *using* such a style, we first pause
 to discuss the *foundations* that legitimatise it as a tool of reasoning.

 In general, proofs are evidence of truth of a claim; by demonstrating that the
 claim follows from some /obvious truth/ using rules of reasoning that /obviously
 preserve truth/. Here are some examples of /clearly obviously true things/.

 | Axiom       | â€œself-evident (obvious) truthâ€                |
 |-------------+-----------------------------------------------|
 | Reflexivity | $X = X$ ---Everything is the same as itself   |
 | Symmetry    | $X = Y$ precisely when $Y = X$ ---Sameness is mutual  |

 #+caption: An inference rule is a syntactic mechansim for deriving â€œtruthsâ€ or â€œtheoremsâ€.
 | Infernece Rule | â€œa reasonable way to derive truthsâ€                                                            |
 |----------------+------------------------------------------------------------------------------------------------|
 | Substitution   | If $E(\vec x)$ is true, then so is $E(\vec F)$ ---where $E(\vec R)$ means $E[\vec x â‰” \vec R]$   |
 |                | E.g., Since $x + y = y + 3$ is true, so is $b + 3 = 3 + b$ ---using $x, y â‰” b, 3$              |
 |----------------+------------------------------------------------------------------------------------------------|
 | Transitivity   | If $X = Y$ and $Y = Z$ then $X = Z$                                                            |
 |                | E.g., since $e^{i Â· Ï€} = -1$ and $-1 = iÂ²$, we must have $e^{i Â· Ï€} = iÂ²$.                     |
 |----------------+------------------------------------------------------------------------------------------------|
 | Leibniz        | If $X = Y$ then $E(X) = E(Y)$ ---â€œsubstituting equals for equalsâ€                              |
 |                | E.g., since $n = 2 Â· m$ we must have $\even n = \even (2 Â· m)$                                 |
 |                | E.g., if /Jim = James/ then /Jim's home address = James' home address/.                            |
 |                |                                                                                                |


 #+begin_details Uses of inference rules ---for Logic ğ‘¬

| /                 | <                                             |
| Inference Rule    | Usage                                         |
|-------------------+-----------------------------------------------|
| Leibniz           | We can apply equalities inside expressions    |
| Transitivity of = | We can chain equalities                       |
| Substitution      | We can use substitution instances of theorems |
| Equipollence      | Things equal to theorems are also theorems    |

Equipollence means if we show something is equal to â€˜trueâ€™ (a particular
theorem), then it is a theorem. Consequently, this means all theorems are
equivalent.
#+end_details

 That's a lot of hand-waving; and a few examples don't scale. In order to discuss
 proof, we need to discuss inference rules, which are ways to derive new claims
 from old claims, and so we need to discuss how claims ---expressions or
 formulae--- are written. So let's start at expressions.

   {{{begin-box(teal, Super terse definition ---to be explained in subsequent
    sections)}}}
    A /logic/ is a set of /symbols/ along with a set of /formulas/ formed from the
    symbols, and a set of /infernece rules/ which allow formulas to be derived
    from other formulas. (The formulas may or may not include a notion of variable.)
    Logics are purely syntactic objects.

    # | Syntax    | Proof theory |
    # | Semantics | Model theory |
   {{{end-box}}}

* Expressions
  :PROPERTIES:
  :CUSTOM_ID: hi
  :END:

** Precedence
   :PROPERTIES:
   :CUSTOM_ID: Precedence
   :END:
# Dot guide
# https://www.graphviz.org/pdf/dotguide.pdf

#+begin_center
How do you â€˜readâ€™ (/parse/) the expression $6 - x + 7$?
#+end_center

#+BEGIN_SRC dot :file images/6-x+7_third_time.png :exports results
digraph structs {
 main [shape=plaintext, label="6 - x + 7"];
 main -> parse1 [style = dashed, label = "means"];
 main -> or [style = invis];
 main -> parse2 [style = dashed, label = "means"];

 parse1 [shape=record,label="+ |{{-|{6|x}}| 7}"];
 or[shape=plaintext];
 parse2 [shape=record,label="- |{6 | {+|{x|7}}}"];

 "???"[shape=plaintext];
  or  -> "???" [style = invis];

}
#     5: struct3 [shape=record,label="hello\nworld |{ b |{c|<here> d|e}| f}| g | h"];
#+END_SRC

#+RESULTS:
[[file:images/6-x+7_third_time.png]]


It can be generated from its parts in two different ways:
1. Both $6$ and $x + 7$ are expressions, so $6 - x + 7$ is an expression.
   #+BEGIN_SRC dot :file images/6-x+7_parse2.png :exports results
   digraph structs {
    "-" -> 6;
    "-" -> "+";
    "+" -> x;
    "+" -> 7;
   }
   #+END_SRC

   #+RESULTS:
   [[file:images/6-x+7_parse2.png]]

2. and also both $6 - x$ and $7$ are expressions, so $6 - x + 7$ is an expression.
   #+BEGIN_SRC dot :file images/6-x+7_parse1.png :exports results :results replace
digraph {
 "-" -> 6;
 "-" -> x;
 "+" -> 7;
 "+" -> "-";
}
#+END_SRC

A *convention* on how a /string/ should be parsed
as a /tree/ is known as a *precedence rule*.

** Grammars
   :PROPERTIES:
   :CUSTOM_ID: Grammars
   :END:

    Expressions are defined by the following /grammar/, but /in practice/ one does
    not write $+(1, 2)$ and instead writes $1 + 2$.  However, the phrase $+(1,
    Â·(2, 3))$ is /unambiguous/, whereas the phrase $1 + 2 Â· 3$ /could be read/ as
    $(1 + 2) Â· 3$ or as $1 + (2 Â· 3)$.

    #+begin_quote
    The grammar defines expressions as *abstract syntax (trees)* whereas strings
    with mixfix notation gives a *concrete syntax* where ambiguity is resolved by
    parentheses, precedence, or association rules.
    #+end_quote
    # Parentheses, precedences, and association rules only serve to disambiguate
    # the encoding of trees in strings.

    #+begin_src math
Expr ::= Constant    -- E.g., 1 or â€œappleâ€
      |  Variable    -- E.g., x or apple (no quotes!)
      |  Application -- E.g., f(xâ‚, xâ‚‚, â€¦, xâ‚™)
    #+end_src

    ( One reads =:== as /becomes/ and so the addition of an extra
    colon results in a â€˜stutterâ€™: One reads
     =::== as /be-becomes/. The symbol =|= is read /or/. )

  {{{begin-box(teal)}}}
Notice that a /constant/ is really just an /application/ with $n = 0$ arguments
and so the first line in the definition above could be omitted.
  {{{end-box}}}

** Textual Substitution ---i.e., [[https://en.wikipedia.org/wiki/Grafting][â€œgrafting treesâ€]]
   :PROPERTIES:
   :CUSTOM_ID: Textual-Substitution-i-e-https-en-wikipedia-org-wiki-Grafting-grafting-trees
   :END:

  The *(simultaneous textual) Substitution operation* $E[\vec x â‰” \vec F]$
  replaces all variables $\vec x$ with parenthesised expressions $\vec F$ in an
  expression $E$. In particular, $E[x â‰” F]$ is just $E$ but with all
  occurrences of $x$ replaced by $â€œ(F)â€$. This is the â€œfind-and-replaceâ€ utility
  you use on your computers.

 {{{begin-box(lime)}}}
  Textual substitution on expressions is known as â€œgraftingâ€ on trees: Evaluate
  $E[x â‰” F]$ by going down the tree $E$ and finding all the â€˜leavesâ€™ labelled
  $x$, cut them out and replace them with the new trees $F$.
 {{{end-box}}}

 {{{begin-box(teal)}}}
  Using the informal English definition of substitution, one quickly notices
  $E[x â‰” x] = E$ and /provided/ $y$ does not occur in $E$:
  $E[x â‰” y][y â‰” x] = E = E[y â‰” F]$.
 {{{end-box}}}

  Since expressions are either variables of functions applications,
  substitution can be defined by the following two clauses ---we will get to
  recursion and induction more formally later on.
  \begin{align*}
     y[x â‰” F]              &=  \mathsf{if}\, x = y \,\mathsf{then}\, F \,\mathsf{else}\, y \,\mathsf{fi}\,
  \\ f(tâ‚, â€¦, tâ‚™)[x â‰” F]  &=  f(tâ‚â€², â€¦, tâ‚™â€²) \; \text{ where } táµ¢â€² = táµ¢[x â‰” F]
  \end{align*}

  {{{begin-box(teal, Sequential â‰  Simultaneous)}}}
  \[
  (x + 2 Â· y)[x â‰” y][y â‰” x]  \quadâ‰ \quad  (x + 2 Â· y)[x, y â‰” y, x]
  \]
  {{{end-box}}}

  [[https://alhassy.github.io/PythonCheatSheet/CheatSheet.pdf][Python]], for example, has simultaneous /assignment/; e.g., ~x, y = y, x~ is
  used to swap the value of two variables.

  Within CalcCheck, to simplify and actually perform the substitution, one uses
  the hint kbd:Substitution; e.g.,
  #+begin_src calccheck
  (x + 2 Â· y)[x, y â‰” 3 Â· y, x + 5]
=âŸ¨ Substitution âŸ©
   3 Â· y + 2 Â· (x + 5)
  #+end_src

** â€œMeta-ğ’³â€: Speaking about the concept of ğ’³ using the notions of ğ’³
   :PROPERTIES:
   :CUSTOM_ID: Meta-ğ’³-Speaking-about-the-concept-of-ğ’³-using-the-notions-of-ğ’³
   :END:

    When we write phrases like =â€œLet E be an expressionâ€=, then the /name/ $E$
    varies and so is a variable, but it is an expression and so may consist of a
    function application or a variable. *That is, $E$ is a variable that may
    stand for variables.* This layered inception is resolved by referring to $E$
    as not just any normal variable, but instead as a *meta-variable*: A variable
    capable of referring to other (simpler) variables.

    Aside: A *variable of type Ï„* is a /name/ denoting a yet unknown /value/ of type Ï„;
    i.e., â€œit is a pronoun (nickname) referring to a person in the collection of people Ï„â€.
    E.g., to say $x$ is an integer variable means that we may treat it
    as if it were a number whose precise value is unknown.
    Then, if we let =Expr Ï„= refer to the expressions denoting /values/ of type Ï„;
    then a *meta-variable* is simply a normal variable of type =Expr Ï„=.

    Likewise, a *theorem* is a Boolean expression that is proved equal to an axiom;
    whereas a *meta-theorem* is a general statement about our logic that we prove
    to be true. That is, if ğ‘¬ is collection of rules that allows us to find
    truths, then a /theorem/ is a truth found using those rules; whereas a
    /meta-theorem/ is property of ğ‘¬ itself, such as what theorems it can have.
    That is, theorems are _in_ ğ‘¬ and meta-theorems are _about_ ğ‘¬.  For example, here
    is a meta-theorem that the equational logic ğ‘¬ has (as do many other theories,
    such as lattices): An /equational/ theorem is true precisely when its â€˜dualâ€™ is
    true. Such metatheorems can be helpful to discover new theorems.
    # A meta-theorem is a theorem about theorems.
    #
    # E.g., p âˆ§ q â‡’ q is not an equation, but it is equivalent to the equation
    # p âˆ§ q â‡’ p â‰¡ true, whose dual is p âˆ¨ q â‡ q â‰¡ false; i.e.,
    # p âˆ¨ q â‡ q.

    #+caption: Being self-reflective using â€œmetaâ€ (Greek for â€˜beyondâ€™)
    | meta-ğ’³           | â€œthe study of ğ’³â€ or â€œğ’³ about ğ’³â€ or â€œbeyond ğ’³â€         |
    |------------------+-------------------------------------------------------|
    | meta-joke        | a joke about jokes                                    |
    | meta-data        | data about data; e.g., publication date               |
    | meta-fiction     | a fictional story that acknowledges itself as fiction |
    | meta-game        | a game in which mini-games happen; e.g., Mario Party  |
    | meta-cognition   | thinking about thinking                               |
    | meta-ethics      | what is the ethical way to study ethics               |
    | meta-physics     | the study of that which is beyond the physical        |
    | meta-mathematics | studying systems of reasoning; aka â€˜proof theoryâ€™     |

* Logics
  :PROPERTIES:
  :CUSTOM_ID: Logics
  :END:

  #+begin_quote
A modern mathematical proof is not very different from a modern machine, or a
modern test setup: the simple fundamental principles are hidden and almost
invisible under a mass of technical details. â€” Hermann Weyl
  #+end_quote

** Syntax vs. Semantics
   :PROPERTIES:
   :CUSTOM_ID: Syntax-vs-Semantics
   :END:

   *Syntax* refers to the structure of expressions, or the rules for putting
     symbols together to form an expression. *Semantics* refers to the meaning
     of expressions or how they are evaluated.

   An expression can contain variables, and evaluating such an expression
   requires knowing what values to use for these variables; i.e., a *state*:
   A list of variables with associated values. E.g., evaluation of $x - y + 2$ in
   the state consisting of $(x, 5)$ and $(y, 6)$ is performed by replacing $x$
   and $y$  by their values to yield $5 - 6 + 2$ and then evaluating that to
   yield $1$.

   A Boolean expression $P$ is *<<<satisfied>>>* in a state if its value is /true/
   in that state; $P$ is *<<<satisfiable>>>* if there is a state in which it is
   satisfied; and $P$ is *<<<valid>>>* (or is a *<<<tautology>>>*) if it is
   satisfied in every state.

 --------------------------------------------------------------------------------

     All theorems of the propositional calculus ğ‘¬ are valid. This can be checked by checking
     that each axiom with a truth table and arguing for each inference rule that
     if its premises are valid then so is its conclusion.

     For example, let's show that the Substitution rule preserves validity.  Let
     us write $s(E)$ to denote the value of expression $E$ in state $s$.  If $E$
     is valid, then it is true in any state, let's argue that $E[x â‰” F]$ is also
     true in any state. So, given a state $s$, let $sâ€²$ be the â€˜updatedâ€™ state
     that assigns the same values to all the variables as does $s$ /except/ that
     the variable $x$ is assigned the value $s(F)$.  Then, since $E$ is valid,
     $sâ€²(E)$ is true but $sâ€²(E)$ is just $s\big(E[x â‰” F]\big)$ and so the
     resulting substitution is also valid.

     In programming, if we want the /assignment/ $x â‰” F$ to ensure a property $R$
     holds, then we need $R[x â‰” F]$ to hold /before/ the assignment.
     That is, if the state $s$ of our program variables satisfies $R[x â‰” F]$
     then the updated state $sâ€²$ ---having /sâ€²(x) = s(F)/--- will satisfy $R$.

     Not only are all theorems valid, but all valid expressions are theorems of
     our calculus (although we do not prove this fact). Theoremhood and validity
     are one and the same.

 --------------------------------------------------------------------------------

   Evaluation of the expression $X = Y$ in a state yields the value /true/ if
   expressions $X$ and $Y$ have the same value and yields /false/ if they have
   different values.

   This characterisation of equality is in terms of expression evaluation.

   For reasoning about expressions, a more useful characterisation
   would be a set of laws that can be used to show that two expressions
   are equal, *without* calculating their values.
   # c.f., static analysis versues running a program

   For example, you know that $x = y$ equals $y = x$, regardless
   of the values of $x$ and $y$.

   A collection of such laws can be regarded as a definition
   of equality, *provided* two expressions have the same value
   in all states precisely when one expression can be translated into
   the other according to the laws.

   Later we see that theorems correspond to expressions that are true in all states.

** Inference Rules
   :PROPERTIES:
   :CUSTOM_ID: Logics-and-Inference-Rules
   :END:

   Formally, a â€œproofâ€ is obtained by applying a number of â€œrulesâ€ to known
   results to obtain new results; a â€œtheoremâ€ is the conclusion of a â€œproofâ€.
   An â€œaxiomâ€ is a rule that does not need to be applied to any existing
   results: It's just a known result.

   That is, a *rule* $R$ is a tuple $Pâ‚, â€¦, Pâ‚™, C$ that is thought of as â€˜taking
   *premises* (instances of known results) $Páµ¢$â€™ and acting as a â€˜natural,
   reasonable justificationâ€™ to obtain *conclusion* $C$.  A *proof system* is a
   collection of rules. At first sight, this all sounds very abstract and rather
   useless, however it is a /game/: *Starting from rules, what can you obtain?* Some
   games can be very fun! Another way to see these ideas is from the view of
   programming:

   #+caption: Proofs-are-programs
   | /           | <                                     |
   | Mathematics | Programming                           |
   |-------------+---------------------------------------|
   | logic       | trees (algebraic data types, ğ’²-types) |
   | rules       | constructors                          |
   |-------------+---------------------------------------|
   | proof       | an application of constructors        |
   | axiom       | a constructor with no arguments       |

   For example, recall from elementary school that the addition â€˜+â€™
   of a number 12 and a number 7 to obtain a number 19 is written as
   \begin{align*}
    & 12 \\
   + & \;\;7 \\ \hline
    & 19
   \end{align*}
   This familiar notation is also used for proof rules as well:
   A rule $R = (Pâ‚, â€¦, Pâ‚™, C)$ is traditionally presented in the shape
   \[{Pâ‚ \; Pâ‚‚ \; â€¦ \; Pâ‚™ \over C}R\]

   {{{begin-box(lime, ğ‘°ğ‘­ I have ingredients and a recipe for a cake ğ‘»ğ‘¯ğ‘¬ğ‘µ I can
   make a cake)}}}

   Here are two familiar and eerily similar rules ;-)

   $$\Rule[Function Application]{a : A \And f : A â†’ B}{f(a) : B}$$

   $$\Rule[Modus Ponens]{p \And p â‡’ q}{q}$$

   For instance, the first rule says â€œif you have a road between two cities, /A/ and /B/, then you
   can travel from address /a/ in city /A/ to get to address /f(a)/ in city $B$â€.  The
   second rule says the same thing, but *forgets/ignores* the precise
   locations. Sometimes it's okay for something â€œto existâ€, but other times
   that's not enough and you â€œactually want to get (construct) it somehowâ€;
   e.g., as the title begs: It's /possible/ to make a cake, but /how/? /Which/ recipe
   you use makes a difference!

   # The second rule is also known as /Impication Elimination/
   # as it is â€œthe way an implication can be usedâ€.

   {{{end-box}}}

 --------------------------------------------------------------------------------

     Just as there are meta-variables and meta-theorems, there is â€˜meta-syntaxâ€™:
     - The use of a fraction to delimit premises from conclusion is a form of â€˜implicationâ€™.
     - The use of a comma, or white space, to separate premises is a form of â€˜conjunctionâ€™.

     If our expressions actually have an implication and conjunction operation,
     then inference rules $\Rule[R]{Pâ‚ \And â‹¯ \And Pâ‚™}{C}$ can be presented as
     axioms $Pâ‚ \,âˆ§\, â‹¯ \,âˆ§\, Pâ‚™ \,â‡’\, C$.

     The inference rule says â€œif the $Páµ¢$ are all valid, i.e., true in /all
     states/, then so is $C$â€; the axiom, on the other hand, says â€œif the $Páµ¢$
     are true in /a state/, then $C$ is true in /that state/.â€ Thus the rule and
     the axiom are not quite the same.

     Moreover, the rule is not a Boolean expression.  Rules are thus more
     general, allowing us to construct systems of reasoning that have no
     concrete notions of â€˜truthâ€™ ---see the logic ğ‘¾ğ‘© below.

     Finally, the rule asserts that $C$ follows from $Pâ‚, â€¦, Pâ‚™$.
     The formula $Pâ‚ \,âˆ§\, â‹¯ \,âˆ§\, Pâ‚™ \,â‡’\, C$, on the other hand, is a Boolean
     expression (but it need not be a theorem).

     An example of this relationship between rules and operators
     may be observed by comparing the logics ğ‘¾ğ‘© and ğ‘´ğ‘ºğ‘¯, below.
     One could read â€œâ—‡â€ as â€œandâ€, and â€œâŸ¶â€ as â€œimpliesâ€.

#  --------------------------------------------------------------------------------

#   Let's look at a few simpler rules; the next 3 rules
#   are part of the *Logic E* system used in the LADM text book
#   ---see â€œ[[http://www.cse.yorku.ca/~logicE/misc/logicE_intro.pdf][Equational Propositional Logic]]â€ by Gries & Schneider.


A â€œtheoremâ€ is a syntactic concept: Can we play the game of moving symbols to
get this? Not â€œis the meaning of this trueâ€!
â€˜Semantic conceptsâ€™ rely on â€˜statesâ€™, assignments of values to variables
so that we can â€˜evaluate, simplifyâ€™ statements to deduce if they are true.

Syntax is like static analysis; semantics is like actually running the program
(on some, or all possible inputs).


** [Optional] Strange Logics
   :PROPERTIES:
   :CUSTOM_ID: water-bucket-logics
   :END:

     Here is an example logic, call it <<<ğ‘¾ğ‘©>>>:
     - The symbols are the usual numbers, along with =+= and =-= and
       =,= (comma).
     - A formula is term of the shape =x, y=, where $x$ and $y$ are terms formed
       from numbers, +, and -.
       + Notice that comma is a binary /operator/.
       + Notice that there are /no variables/ (as terms).
     - There are 7 inference rules ---including one axiom.

 #    Let's construct a logic to that models two bukects of water,
 #    one containing 3 liters and the other containing 5 liters, and
 #    an unlimited water supply.

 \[\Rule[Empty]{}{0,0}\]
 \[
 \Rule[ZeroLeft]{x,y}{0, y} \quad
 \Rule[ZeroRight]{x,y}{x, 0}
 \]\[
 \Rule[RefreshLeft]{x, y}{3, y} \quad
 \Rule[RefreshRight]{x, y}{x, 5}
 \]
 \[ \Rule[ShiftLeft_d \quad\text{(provided $y - d = 0$ or $x + d = 3$)}]{x, y}{x + d, y -
 d} \]
 \[
 \Rule[ShiftRight_d \quad\text{(provided $x - d = 0$ or $y + d = 5$)}]{x, y}{x - d, y + d}
 \]

 *Exercise [[#water-bucket-logics]].1*: Using this logic, prove the theorem =0, 4=.
 - Notice that the theorem has nothing to do with â€˜truthâ€™! ---At least not
   explicitly, or intuitively.
 #+begin_details Solution
 \[
 \Rule[ZeroLeft]{\normalsize\Rule[ShiftLeft_1]{\LARGE\Rule[RefreshLeft]{\LARGE\Rule[ShiftLeft_2]{\Rule[ZeroLeft]{\LARGE
 \Rule[ShiftLeft_3]{\LARGE \Rule[RefreshRight]{\LARGE\Rule[Empty]{}{0,0}}{0,
 5}}{3, 2}}{0,2}}{2,0}}{2, 5}}{3, 4}}{0, 4}
 \]
 #+end_details

 *Exercise [[#water-bucket-logics]].2:*
 A logic models reasoning, can you /interpret/ the terms =x, y= in such
 a way that makes the inference rules true?
 #+begin_details Solution

 The logic ğ‘¾ğ‘© /could be/ interpreted as modelling two â€˜water bucketsâ€™, the first
 can contain 3 litres while the second can contain 5 litres, along with an
 unlimited water supply.

 1. The axiom says we start out with empty buckets.
 2. The zero rules says we can empty out buckets.
 3. The refresh rules say we can fill up buckets to being full.
 4. The shift rules say we can pour out water from one bucket to
    the other, such that the first is emptied *or* the second is filled.
    (In particular, we cannot pour an arbitrary /chosen/ amount of water. )

 Then the theorem says we can measure 4 litres of water ---using only a 3 and 5
 litre buckets and an unlimited water supply.
 #+end_details

 --------------------------------------------------------------------------------

 Here is another example logic, call it <<<ğ‘´ğ‘ºğ‘¯>>>:
 + The symbols are the usual numbers, along with =+, -, â—‡, âŸ¶=.
 + A formula is of the form $x â—‡ y âŸ¶ xâ€² â—‡ yâ€²$ where â—‡ binds tightest
   and $x, y, xâ€², yâ€²$ are terms formed from numbers, =+=, and =-=.
 + In contrast to ğ‘¾ğ‘©, this logic has only 1 non-axiom inference rule!

   \[\Rule[Reflexivity]{}{x â—‡ y âŸ¶ x â—‡ y}\]
   \[\Rule[Transitivity]{x â—‡ y âŸ¶ xâ€² â—‡ yâ€² \And xâ€² â—‡ yâ€² âŸ¶ xâ€³ â—‡ yâ€³}{x â—‡ y âŸ¶ xâ€³ â—‡ yâ€³}\]

   \[\Rule[ZeroLeft]{}{x â—‡ y âŸ¶ 0 â—‡ y} \quad \Rule[ZeroRight]{}{x â—‡ y âŸ¶ x â—‡ 0}\]
   \[\Rule[RefreshLeft]{}{x â—‡ y âŸ¶ 3 â—‡ y} \quad \Rule[RefreshRight]{}{x â—‡ y âŸ¶ x â—‡ 5} \]
   \[\Rule[ShiftLeft_d]{\text{(provided $y - d = 0$ or $x + d = 3$)}}{x â—‡ y âŸ¶ (x+d) â—‡ (y-d)} \]
   \[\Rule[ShiftRight_d]{\text{(provided $x - d = 0$ or $y + d = 5$)}}{x â—‡ y âŸ¶ (x - d) â—‡ (y + d)}\]

 *Exercise [[#water-bucket-logics]].3:* Finish reading this section, then come back and
 prove the theorem =0 â—‡ 0 âŸ¶ 0 â—‡ 4= using a /calculational proof/.
 #+begin_details Solution

 As discussed in Â§[[#Rules-of-Equality-and-Proof-Trees-vs-Calculational-Proofs]], we
 form calculational proofs using a transitive relation in the left-most column of
 a calculation.  The transitvity of the relation ensures that the first term is
 related, via the relation, to the last term.

 \begin{calc}
   0â—‡ 0 \step[âŸ¶]{refresh left}
   3â—‡ 0 \step[âŸ¶]{ shift right}
   0â—‡ 3 \step[âŸ¶]{ refresh left }
   3â—‡ 3 \step[âŸ¶]{ shift right, then zero right}
   1â—‡ 0 \step[âŸ¶]{ shift right }
   0â—‡ 1 \step[âŸ¶]{ refresh left, then shift right }
   0â—‡ 4
 \end{calc}

 :AnotherProof:
 #+begin_src C
  0,0
â†’âŸ¨ refresh 2 âŸ©
  0, 5
â†’âŸ¨ draw from 2 âŸ©
  3, 2
â†’âŸ¨ dump 1 âŸ©
  0, 2
â†’âŸ¨ draw from 2 âŸ©
  2, 0
â†’âŸ¨ refresh 1 âŸ©
  2, 5
â†’âŸ¨ draw from 2 âŸ©
  3, 4
â†’âŸ¨ dump 1 âŸ©
  0, 4
     #+end_src
 :End:
 #+end_details

 *Exercise [[#water-bucket-logics]].4:* Provide an interpretation of this logic.
 #+begin_details Solution

 We /may/ think of ğ‘´ğ‘ºğ‘¯ as a â€˜machineâ€™ with two memory banks: A computer with memory
 state $x$ and $y$ is executed and it terminates in memory state $xâ€²$ and $yâ€²$.
 That is, $x â—‡ y âŸ¶ xâ€² â—‡ yâ€²$ is â€œstarting from $(x, y)$, the computer finishes
 with $(xâ€², yâ€²)$â€.

 The theorem then says that it is possible for the computer to start at $(0, 0)$
 and finish with memory store $(0, 4)$.

 The idea to use *inference rules as computation*
 is witnessed by the [[https://alhassy.github.io/PrologCheatSheet/CheatSheet.pdf][Prolog]] programming language.

 Of-course, we could also re-use the water buckets interpretation of ğ‘¾ğ‘©.
 #+end_details

** Rules of Equality and Proof Trees vs. Calculational Proofs
   :PROPERTIES:
   :CUSTOM_ID: Rules-of-Equality-and-Proof-Trees-vs-Calculational-Proofs
   :END:

 # ** Defining equality by how it can be used, manipulated


 # E.g., 4 laws that characterise equality are reflexitivitry, symmetry,
 #   transitvity, and Leibniz.

 Before we can showcase an example of a proof tree ---let alone
 compare them with calculational proofs--- we need a few
 example inference rules that can be used in the construction of the proofs.

 The following rules define equality by how it can be used, manipulated.

 1. Equality is:
    - *reflexive:* $X = Y$;
    - *symmetric:* $X = Y$ implies $Y = X$; and
    - *transitive*: $X = Z$ follows from having both $X = Y$ and $Y = Z$, for any
      $Y$

 2. The *Substitution inference rule*
    says that a substitution $E[\vec x â‰” \vec F]$ is
    a theorem /whenever/ $E$ is a theorem.

    Within CalcCheck, this rule is realised as the kbd:with clause: The phrase =E
    with `xâ‚, xâ‚‚, â€¦, xâ‚™ â‰” Fâ‚, Fâ‚‚, â€¦, Fâ‚™`= is tantamount to invoking the theorem
    $E[\vec x â‰” \vec F]$. The rule is applied /implicitly/, unless =rigid matching=
    is activated ---e.g., to get students *thinking correctly about applying
    theorems* instead of just putting random theorems that look similar and hoping
    the system sees a justification from a mixture of them.

 3. The *Leibniz inference rule* says that $E[z â‰” X] = E[z â‰” Y]$ whenever $X = Y$;
    i.e., it justifies substituting â€œequals for equalsâ€.

    Leibniz allows us to use an equation to rewrite a part of an expression; and
    so, it justifies the use of â€˜calculation hintsâ€™.

    Leibniz says: Two expressions are equal (in all states) precisely when
    replacing one by the other in any expression $E$ does not change the value of
    $E$ (in any state).

      {{{begin-box(blue)}}}
    A /function/ $f$ is a rule for computing a value from another value.

    If we define $f\, x = E$ using an expression, then /function application/ can
    be defined using textual substitution: $f \, X = E[x â‰” X]$. That is,
    expressions can be considered functions of their variables
    ---but it is still expressions that are the primitive idea, the building blocks.

    Using functions, Leibniz says /if X = Y then f X = f Y, for any function f/.
    That is, if two things are actually the same, then any (/f-/)value extracted
    from one must be the same when extracted from the other.
    {{{end-box}}}

    Again: Unlike the Substitution rule, which allows us to instantiate /any/
    theorem, the Leibniz rule is meant for *applying equational theorems deeper
    within expressions*. Later on, we will look at â€˜monotonicity rulesâ€™ which will
    let us apply inclusion (â‰¤, â‡’, âŠ‘) theorems deep within expressions.

    The kbd:with syntax is overloaded for this rule as well.

 ------

 In addition to these rules, suppose that we have
    $2 Â· a = a + a$ (â€œTwiceâ€) and $-1 Â· a = - a$ (15.20) as axioms;
    then we can form the following proof.

 \[
 \Rule[Transitivity\; of\; =]
 {\large
   \Rule[\small Substitution]
   {\Large \Rule{âœ“}{-1 Â· a \,=\, - a} }
   { (- 1) Â· 2 Â· (x + y) \,=\, - (2 Â· (x + y)) }
   \And
   \Rule[\small Leibniz]
   {\Large \Rule{âœ“}{2 Â· a = a + a} }
   { - (2 Â· (x + y)) \,=\,    -((x + y) + (x + y)) }
 }{(- 1) Â· 2 Â· (x + y) \,=\, -((x + y) + (x + y))}
 \]

 This is known as a /natural deduction proof tree/; one begins â€˜readingâ€™ such a
 proof from the very *bottom*: Each line is an application of a rule of reasoning,
 whose assumptions are above the line; so read upward.
 The *benefit* of this approach is that *rules guide proof construction*; i.e., it is
 goal-directed.

 However the *downsides are numerous*:
 - So much horizontal space for such a simple proof!
 - One has to *repeat* common subexpressions, such as the
   $-(2 Â· (x + y))$.
 - For comparison with other proof notations, such as Hilbert style,
   see â€œ[[http://www.cse.yorku.ca/~logicE/misc/logicE_intro.pdf][Equational Propositional Logic]]â€ or LADM-Â§6.

 Instead, we may use a more â€˜linearâ€™ proof format:
 \begin{calc}
 (- 1) Â· 2 Â· (x + y)
 \step{ 15.20) $- a \,=\, - 1 Â· a$
       â”€ Using implicit substitution rule }
 - (2 Â· (x + y))
 \step{ â€œTwiceâ€
       â”€ Using implicit Leibniz with $a â‰” x + y$ }
 -((x + y) + (x + y))
 \end{calc}

 In this equational style, instead of a *tree* (on the left)
 we use a *sequential chain of equalities* (on the right):

 #+begin_parallel org
 $$\Rule[Leibniz]{X \,=\, Y}{E[z â‰” X] \,=\, E[z â‰” Y]}$$

 #+html: <br>

 \begin{calc}
     E[z â‰” X]
 \step{ X = Y }
     E[z â‰” Y]
 \end{calc}
 #+end_parallel

 In this way, we may use the Substitution rule to create theorems that can be
 used with the Leibniz rule and then use the Transitivity rule to conclude
 that the first expression of an equational proof is equivalent to the last one.
 {{{begin-box(orange, )}}}
 To show that $L = R$, transform $L$ into $R$ by a series of substitutions
 of equals for equals. (If $R$ has more â€˜structureâ€™, then begin at $R$ and
 transform to $L$.)
 {{{end-box}}}

 --------------------------------------------------------------------------------

 + Transitivity allows us to conclude the first expression in a calculation
    is equal to the last expression in the calculation.
 + Reflexivity allows us to have â€˜emptyâ€™ calculations and â€œno (expression) changeâ€
      calculation steps
 + Symmetry allows us to use an equation $LHS = RHS$
      â€œin the other directionâ€ to replace an instance of $RHS$ by $LHS$.

 Equational proofs thus have this shape:

 \begin{calc}
   P
 \step{ $P = Q[z â‰” X]$ }
   Q[z â‰” X]
 \stepmany{ \line{make a â€œremarkâ€ about $Q[z â‰” X]$}
            \line{or the direction of the proof}
            \line{or â€œremove superflous parenthesesâ€}
            \line{or â€œinsert parentheses for clairtyâ€} }
   Q[z â‰” X]
 \step{ $X = Y$ }
   Q[z â‰” Y]
 \step{ $R = Q[z â‰” Y]$ â”€â”€note the change in â€˜directionâ€™ }
   R
 \end{calc}

 Which is far *easier to read and write* than:
 \[
 \Rule[Transitivity]{
  P = Q[z â‰” X]
  \And
  \Rule[Transitivity]{
    \Rule[\large Transitivity]{ \LARGE
      \Rule[Reflexivity]{}{Q[z â‰” X] \eq Q[z â‰” X]}
      \And
      \Rule[Leibniz]{X \eq Y}{Q[z â‰” X] \eq Q[z â‰” Y]}
      }{\LARGE Q[z â‰” X] \eq Q[z â‰” Y]}
    \And
    {\LARGE \Rule[\large Symmetry]{R \eq Q[z â‰” Y]}{Q[z â‰” Y] \eq R}
    }}
 {\large \text{$Q[z â‰” X] \eq R$}}}
 {P = R}
 \]

    *The structure of equational proofs allows implicit use of infernece rules
    Leibniz, Transitvitity & Symmetry & Reflexivity of equality, and Substitution.* In contrast, the
    structure of proof trees is no help in this regard, and so all uses of
    inference rules must be mentioned explicitly.
    # In fact, more suitable inference rules for proof trees are those of /natural
    # deduction/ (ğ‘µğ‘«): Each propositional operator âŠ• has two rules, one to show
    # how to introduce it into a theorem (i.e., prove a theorem involving it) and
    # one to show how to use it (eliminate it) to derive new truths; as such, ğ‘µğ‘«
    # has no axioms and the â‡’-elimination inference rule is known as â€œmodus
    # ponensâ€, a theorem in ğ‘¬.

 --------------------------------------------------------------------------------

    Leibniz is often used with Substitution, as follows
    ---supposing we know the theorem =â€œHalfâ€= $2 Â· x / 2 = x$:

    \begin{calc}
      2 Â· j / 2 = 2 Â· (j - 1)
    \step{ Half, with $x â‰” j$ }
      j = 2 Â· (j - 1)
    \end{calc}

    We are using Leibniz with the premise $2 Â· j / 2 = j$.
    We can use this premise only if it is a theorem. It is, because
    $2 Â· x / 2 = x$ is a theorem and, therefore, by Substitution,
    $(2 Â· x / 2 = x)[x â‰” j]$ is a theorem.

    If a use of Substitution is simple enough, as in this case, we may leave
    off the indication â€œwith $x â‰” j$â€.

* Â Propositional Calculus
  :PROPERTIES:
  :CUSTOM_ID: Propositional-Calculus
  :END:

** Preliminaries :ignore:
   :PROPERTIES:
   :CUSTOM_ID: Preliminaries-ignore
   :END:

Often operations are defined by how they are evaluated (â€œoperationallyâ€), we
take the alternative route of defining operations by how they can be manipulated
(â€œaxiomaticallyâ€); i.e., by what properties they satisfy.  For instance, we may
define basic manipulative properties of operators ---i.e., /axioms/--- by
considering how the operators behave operationally on particular
expressions. That is, one may use an operational, intuitive, approach to obtain
an axiomatic specification (characterisation, interface) of the desired
properties. More concretely, since $(p â‰¡ q) â‰¡ r$ and $p â‰¡ (q â‰¡ r)$ evaluate to
the same value for any choice of values for $p, q, r$, we may insist that a part
of the definition of equivalence is that it be an doc:Associative operation.
Sometimes a single axiom is not enough to â€˜pin downâ€™ a unique operator ---i.e.,
to ensure we actually have a well-defined operation--- and other times this is
cleanly possible; e.g., given an ordering â€˜â‰¤â€™(â€˜â‡’, âŠ†, âŠ‘â€™) we can define minima
â€˜â†“â€™ (â€˜âˆ§, âˆ©, âŠ“â€™) by the axiom: â€œx â†“ y is the greatest lower boundâ€; i.e., $z â‰¤ x
â†“ y \quadâ‰¡\quad z â‰¤ x \,âˆ§\, z â‰¤ y$.

A /calculus/ is a method or process of reasoning by calculation with symbols. A
/propositional calculus/ is a method of calculating with Boolean (or
propositional) expressions.

A /theorem/ is a syntactic object, a string of symbols with a particular property.
A /theorem/ of a calculus is either an axiom or the conclusion of an inference
rule whose premises are theorems. Different axioms could lead to the same
set of theorems, and many texts use different axioms.

** Boolean Expressions and Laws
   :PROPERTIES:
   :CUSTOM_ID: Boolean-Expressions-and-Laws
   :END:

  The type of propositions is known as the *Booleans* and denoted ğ”¹.
     #+begin_src math
ğ”¹ ::= true | false
     #+end_src

*** Equality: â€œ=â€ and â€œâ‰¡â€
    :PROPERTIES:
    :CUSTOM_ID: Equality-and
    :END:

    For instance, the notion of equality on any type Ï„ is
    typed ~_=_ : Ï„ â†’ Ï„ â†’ ğ”¹~; i.e., equality takes two values of a type Ï„
    and returns a propositional value.

    #+begin_quote
    In general, the â€œcontinued equalityâ€ $x = y = z$
    is *read conjunctively*: Both $x = y$ /and/ $y = z$.
    However, for the special case Ï„ being ğ”¹, the expression
    $x = y = z$ could be *read associativity*: $(x = y) = z$.

    These two ways to read (parse) a continued equality
    give different operators on ğ”¹. The associative equality
    is popularly written as â€˜â‡”â€™ but, unfortunately, not usually treated
    as an equality at all! In this class, we write the associative equality
    as â€˜â‰¡â€™ and read it as â€œequivalesâ€.

    See [[https://www.researchgate.net/publication/220113201_The_associativity_of_equivalence_and_the_Towers_of_Hanoi_problem][The associativity of equivalence and the Towers of Hanoi problem]].
    #+end_quote

 The phrase $p â‰¡ q$ may be read as
    - /p is equivalent to q/, or
    - /p exactly when q/,
    - /p if-and-only-if q/,

    This operator is just equality on the Booleans:
    | Definition of â‰¡ |   | ~(p â‰¡ q) = (p = q)~ |

    The need for a new name for an existing concept is that they have different
    *notational conventions*: Firstly, â€œâ‰¡â€ has lower precedence than â€œ=â€ and
    secondly,
    - = is conjunctive :: $\big(p = q = r\big) \quad=\quad \big( (p = q)
      \;\land\; (q = r)\big)$
    - â‰¡ is associative :: $\big(p â‰¡ q â‰¡ r\big) \quad=\quad \big((p â‰¡ q) â‰¡ r\big) \quad=\quad \big(p â‰¡ (q â‰¡ r)\big)$

    For example, $\false â‰¡ \true â‰¡ \false$ is $\true$, whereas
    $\false = \true = \false$ is $\false$.

    #+begin_quote
 For the Booleans, equality is equal to equivalence:
 | /(p = q) = (p â‰¡ q)/ for /p, q : ğ”¹/ |

 For the Booleans, equality is equivalent to equivalence:
 | /(p = q) â‰¡ (p â‰¡ q)/ for /p, q : ğ”¹/  |
    #+end_quote

*** Useful Operators
    :PROPERTIES:
    :CUSTOM_ID: Useful-Operators
    :END:
 The Booleans have a number of useful operators that model reasoning,
    such as:
    #+caption: Boolean operators and similar numeric operators
    | Operator    | Booleans (ğ”¹)    | Numbers (â„¤)                 |
    | /           | >               |                             |
    |-------------+-----------------+-----------------------------|
    | â€œandâ€       | =_âˆ§_ : ğ”¹ â†’ ğ”¹ â†’ ğ”¹= | â€œminimumâ€ =_â†“_ : â„¤ â†’ â„¤ â†’ â„¤=   |
    | â€œorâ€        | =_âˆ¨_ : ğ”¹ â†’ ğ”¹ â†’ ğ”¹= | â€œmaximumâ€ =_â†‘_ : â„¤ â†’ â„¤ â†’ â„¤=   |
    | â€œnotâ€       | =Â¬_ : ğ”¹ â†’ ğ”¹=      | â€œnegationâ€ =-_ : â„¤ â†’ â„¤ â†’ â„¤=   |
    | â€œimpliesâ€   | =_â‡’_ : ğ”¹ â†’ ğ”¹ â†’ ğ”¹= | â€œinclusionâ€ =_â‰¤_ : â„¤ â†’ â„¤ â†’ ğ”¹= |
    | [[https://www.researchgate.net/publication/220113201_The_associativity_of_equivalence_and_the_Towers_of_Hanoi_problem][â€œEquivalesâ€]]  | =_â‰¡_ : ğ”¹ â†’ ğ”¹ â†’ ğ”¹= | â€œequalityâ€ =_=_ : â„¤ â†’ â„¤ â†’ ğ”¹=  |

    These operators can be defined /informally/, as done below, but we shall follow
    an /axiomatic/ definition as done in LADM by providing an /interface/ of
    properties that they satisfy instead of any particular /implementation/. Later
    in the class when we get to the =if_then_else_fi= construct, we may provide
    explicit implementations and prove them to be equal to the operations
    specified axiomatically.

    #+caption: Example explicit definitions ---not used in this class
    | â€œp âˆ§ qâ€ is â€œtrueâ€ whenever both â€œpâ€ and â€œqâ€ are â€œtrueâ€, otherwise it is â€œfalseâ€ |
    | â€œm â†“ nâ€ is â€œmâ€ whenever â€œm â‰¤ nâ€, otherwise it is â€œnâ€                            |

    #+caption: Meanings of Boolean operators
    | Expression | Pronounced       | is $\true$ if                                        |
    |------------+------------------+------------------------------------------------------|
    | $p â‰¡ q$    | /p equivales q/    | exactly an even number of arguments is $\false$, (â‹†) |
    | $p â‰¢ q$    | /p differs from q/ | exactly an odd number of its arguments are $\true$   |
    |------------+------------------+------------------------------------------------------|
    | $x = y$    | /x equals y/       | exactly $x$ and $y$ simplify to the same expression  |
    | $x â‰  y$    | /x differs from y/ | $x$ and $y$ do not simplify to the same expression   |
    |------------+------------------+------------------------------------------------------|
    | $p âˆ§ q$    | /p and q/          | all of its arguments are $\true$                     |
    | $p âˆ¨ q$    | /p or q/           | at least one of its arguments is $\true$             |
    |------------+------------------+------------------------------------------------------|
    | $p â‡’ q$    | /p implies q/      | either /q/ is $\true$ or /p/ is $\false$                 |
    |            | /if p, then q/     |                                                      |
    | $p â‡ q$    | /p follows from q/ | either /p/ is $\true$ or /q/ is $\false$                 |
    |            | /p if q/           |                                                      |
    |------------+------------------+------------------------------------------------------|
    | $Â¬ p$      | /not p/            | /p/ is $\false$; read â€œit is not the case that $p$â€    |

    For example,
    |   | â€œp, even if qâ€                               |
    | â‰ˆ | $p âˆ§ (q â‡’ p)$                                |
    |   | ( This is provably equivalent to just $p$. ) |

    (â‹†) Note that if an even number of arguments is /false/, then the /false/'s
    cancel out and only /true/ remains. Note that since /true/ is the identity of
    â€˜â‰¡â€™, we can simply cancel them out of a chain of equivalences.  When there
    are /2 Â· n/ many elements in the chain, then if there are an even number of
    /true/'s, say /k/-many, then there must be an even number of /false/'s: $\even (2 Â·
    n - k) = (\even (2 Â· n) â‰¡ \even (-k)) = \even k = \true$.

    Also,
    | $p â‰¡ q â‰¡ r$         | â‰ˆ | /One or all of p,q, and r are true/     |
    |---------------------+---+---------------------------------------|
    | $p â‰¡ q$             | â‰ˆ | /None or both of p and q is true/       |
    |---------------------+---+---------------------------------------|
    | $p â‰¢ q$             | â‰ˆ | /Exactly one of p and q is true/        |
    |                     |   | /Either p or q, but not both/           |
    |---------------------+---+---------------------------------------|
    | $pâ‚€ â‰¡ pâ‚ â‰¡ â‹¯ â‰¡ pâ‚‚â‚™$ | â‰ˆ | /An even number of the páµ¢ are true/ (â‹†) |
    |                     |   |                                       |

 #+begin_details (â‹†) When is <em>pâ‚€ â‰¡ pâ‚ â‰¡ â‹¯ â‰¡ pâ‚™</em> true?
 When is /pâ‚€ â‰¡ pâ‚ â‰¡ â‹¯ â‰¡ pâ‚™/ true?

 Since /true/ is the identity of â€˜â‰¡â€™; any $páµ¢$ equal to $\true$ can be â€˜cancelled
 outâ€™. Hence, we are left with only $\false$'s. Since $(\false â‰¡ \false) â‰¡
 \true$, we can cancel out any pair of $\false$'s and so if there are an even
 number of $\false$'s the resulting expression is $\true$.

 However, if the number, say $k$, of $\false$'s is even, then
 \begin{calc}
 \text{the parity of trues}
 \step{ Formalise: The trues are the non-falses }
 \even(n - k)
 \step{ Even distributes over sums, subtractions }
 \even n â‰¡ \even (-k)
 \step{ Even is invariant under unary minus }
 \even n â‰¡ \even k
 \step{ By assumption, there are $k$-many falses}
 \even n â‰¡ \true
 \step{ Identity of equivalence }
 \even n
 \end{calc}

 Hence,
 |   | $pâ‚€ â‰¡ pâ‚ â‰¡ â‹¯ â‰¡ pâ‚™$                                                        |
 | â‰ˆ | An even number of the arguments is false.                                 |
 | â‰ˆ | The parity of trues is the same as the parity of the number of arguments. |

 For example,
 |   | $p â‰¡ q$                                     |
 | â‰ˆ | None or both of $p$ and $q$ is true         |
 |---+---------------------------------------------|
 |   | $p â‰¢ q$                                     |
 | â‰ˆ | Exactly one of $p$ and $q$ is true          |
 |---+---------------------------------------------|
 |   | $p â‰¡ q â‰¡ r$                                 |
 | â‰ˆ | One or all of $p, q, r$ are true            |
 |---+---------------------------------------------|
 |   | $p â‰¡ q â‰¡ r â‰¡ s$                             |
 | â‰ˆ | Zero, two, or four of $p, q, r, s$ are true |
 |---+---------------------------------------------|
 |   | $Â¬ (p â‰¡ q â‰¡ r â‰¡ s)$                         |
 | â‰ˆ | One or three of $p, q, r, s$ are true.      |

 The second and last examples rely on the fact that â€œnot an even number are trueâ€
 equivales â€œan odd number are trueâ€.

 #+end_details

*** Boolean Laws and Numeric Laws
    :PROPERTIES:
    :CUSTOM_ID: Boolean-Laws-and-Numeric-Laws
    :END:
 To better understand the ğ”¹ooleans, it can be useful to compare their laws
     with those on numbers. For instance, the =Definition of â‡’= at first glance is
     tremendously cryptic: Why in the world would anyone define implication in
     this way $p â‡’ q \,â‰¡\, p âˆ§ q â‰¡ p$?  However, when compared to the similar law
     for numbers that defines inclusion $m â‰¤ n \,â‰¡\, m â†“ n = m$, the definition
     becomes *â€œobviousâ€*: /p is included in (implies) q precisely when having both p
     and q is the same as just having p/; i.e., /m is at-most n precisely when m is
     the minimum of m and n./

     #+caption: Properties of propositional operators and similar (familiar) numeric laws
     | Law                  | Booleans (ğ”¹)                      | Numbers (â„¤ with Â±âˆ)               |
     | /                    | >                                 |                                   |
     |----------------------+-----------------------------------+-----------------------------------|
     | Symmetry of âˆ§        | $p âˆ§ q â‰¡ q âˆ§ p$                   | $m â†“ n = n â†“ m$                   |
     | Associativity of âˆ§   | $(p âˆ§ q) âˆ§ r â‰¡ p âˆ§ (q âˆ§ r)$       | $m â†“ n = n â†“ m$                   |
     | Idempotency of âˆ§     | $p âˆ§ p â‰¡ p$                       | $n â†“ n = n$                       |
     | Identity of âˆ§        | $p âˆ§ \true â‰¡ p$                   | $n â†“ +âˆ = n$                      |
     | Zero of âˆ§            | $p âˆ§ \false â‰¡ \false$             | $n â†“ -âˆ = -âˆ$                     |
     | Contradiction        | $p âˆ§ Â¬ p â‰¡ \false$                | â”€nopeâ”€                            |
     |----------------------+-----------------------------------+-----------------------------------|
     | Symmetry of âˆ¨        | $p âˆ¨ q â‰¡ q âˆ¨ p$                   | $m â†‘ n = n â†‘ m$                   |
     | Associativity of âˆ¨   | $(p âˆ¨ q) âˆ¨ r â‰¡ p âˆ¨ (q âˆ¨ r)$       | $m â†‘ n = n â†‘ m$                   |
     | Idempotency of âˆ¨     | $p âˆ¨ p â‰¡ p$                       | $n â†‘ n = n$                       |
     | Identity of âˆ¨        | $p âˆ¨ \false â‰¡ p$                  | $n â†‘ -âˆ = n$                      |
     | Zero of âˆ¨            | $p âˆ¨ \true â‰¡ p$                   | $n â†‘ +âˆ = +âˆ$                     |
     | Excluded Middle      | $p âˆ¨ Â¬ p â‰¡ \false$                | â”€nopeâ”€                            |
     |----------------------+-----------------------------------+-----------------------------------|
     | Golden Rule          | $p âˆ§ q â‰¡ p â‰¡ q â‰¡ p âˆ¨ q$           | $m â†“ n = m \,â‰¡\, n = m â†‘ n$       |
     | âˆ§/âˆ¨ Distributivity   | $p âˆ§ (q âˆ¨ r) â‰¡ (p âˆ§ q) âˆ¨ (p âˆ§ r)$ | $m â†‘ (n â†“ r) = (m â†‘ n) â†“ (m â†‘ r)$ |
     | âˆ¨/âˆ§ Distributivity   | $p âˆ¨ (q âˆ§ r) â‰¡ (p âˆ¨ q) âˆ§ (p âˆ¨ r)$ | $m â†‘ (n â†“ r) = (m â†‘ n) â†“ (m â†‘ r)$ |
     |----------------------+-----------------------------------+-----------------------------------|
     | Double negation      | $Â¬ Â¬ p â‰¡ p$                       | $- - n = n$                       |
     | Definition of $\false$ | $\false â‰¡ Â¬ \true$                | $-âˆ \,=\, - (+âˆ)$                 |
     | Negation of $\false$ | $Â¬ \false = \true$                | $- (-âˆ) = +âˆ$                     |
     | De Morgan            | $Â¬(p âˆ§ q) = Â¬ p âˆ¨ Â¬ q$            | $-(m â†“ n) = -m â†‘ -n$              |
     |                      | $Â¬(p âˆ¨ q) = Â¬ p âˆ§ Â¬ q$            | $-(m â†‘ n) = -m â†“ -n$              |
     |----------------------+-----------------------------------+-----------------------------------|
     | Definition of â‡’      | $p â‡’ q â‰¡ p âˆ§ q â‰¡ p$               | $m â‰¤ n \,â‰¡\, m â†“ n = m$           |
     |                      | $p â‡’ q â‰¡ p âˆ¨ q â‰¡ q$               | $m â‰¤ n \,â‰¡\, m â†‘ n = n$           |
     | Consequence          | $p â‡ q â‰¡ q â‡’ p$                   | $m â‰¥ n \,â‰¡\, n â‰¤ m$               |
     | ex falso quodlibet   | $\false â‡’ p â‰¡ \true$              | $-âˆ â‰¤ n \,â‰¡\, \true$              |
     | Left-identity of â‡’   | $\true â‡’ p â‰¡ p$                   | $+âˆ â‰¤ n \,â‰¡\, n = +âˆ$             |
     | Right-zero of â‡’      | $p â‡’ \true â‰¡ \true$               | $n â‰¤ +âˆ \,â‰¡\, \true$              |
     | Definition of Â¬      | $p â‡’ \false â‰¡ Â¬ p$                | â”€nopeâ”€                            |
     |----------------------+-----------------------------------+-----------------------------------|
** â€˜trueâ€™
   :PROPERTIES:
   :CUSTOM_ID: Equivalence-and-true
   :END:

   The symmetry of equivalence could be read as $(p â‰¡ p) â‰¡ (q â‰¡ q)$ and so
   â€˜self-applications of â‰¡â€™ are indistinguishable.  That is, the value of $p â‰¡
   p$ does not depend on the value of $q$ and so we introduce the constant
   symbol /true/ is an abbreviation for $p â‰¡ p$.

  \[\Law[(3.4)]{Axiom, Identity of â‰¡}{\true â‰¡ p â‰¡ p}\]

  When this definition is read as $(\true â‰¡ p) = p$, and by symmetry of â‰¡ as $(p
  â‰¡ \true) = p$, we see that this new constant is an doc:Identity of â‰¡.

  Since â‰¡ is associative, a formula can be read in multiple ways.
  - $p â‰¡ p â‰¡ true$ can be read as the reflexitivty of â‰¡ or the definition
    of true ---both being $(p â‰¡ p) â‰¡ true$ ---
    or as an identity law --- $p â‰¡ (p â‰¡ true)$.
  - The Golden Rule can also be read a way to define âˆ§ in-terms of â‰¡ and âˆ¨,
    or to define âˆ¨ in terms of â‰¡ and âˆ¨, or to phrase â‰¡ in terms of â‰¡, âˆ§, and âˆ¨;
    or to absorb an expression containing â‰¡,âˆ¨, âˆ§ down to a single subexpression:
    $p â‰¡ (q â‰¡ p âˆ¨ q â‰¡ p âˆ§ q)$.

  #+begin_box (3.56) Parsing Heuristic ---Page 56
  Exploit the ability to parse theorems like the â€˜Golden Ruleâ€™
  and the â€˜Definition of trueâ€™ in many different ways.

  For instance, in chains of equivalences, the use symmetry and associativity
  of equivalence increases the number of parses.
  #+end_box

  Using its definition, we can quickly show that
  $\true = \big(\true â‰¡ q â‰¡ q\big)$ and so by equanimity,
  since the right side is a theorem, then the left side is also a theorem.
  Hence,

  \[\Law[3.4]{True is a theorem}{\true}\]

  What is the benefit of this theorem?

  By equanimity, this means that to prove $P$ is a theorem, it is
  enough to show that $P â‰¡ \true$! This is an â€˜expectedâ€™ result :-)

  ( We can phrase this observation as a theorem itself as $(P â‰¡ \true) â‰¡ P$, but
  this is essentially the definition of true, above! )

  Here is an impressive benefit of this theorem.  Suppose we want to prove an
  equation $L = R$ is true; if our proof only alters $L$ to make it the same as
  $R$ so that we obtain $R = R$, then we may the definition of $\true$ to
  obtain, well, $\true$, but since this is a theorem then so too is $L = R$.
  That is,
  \begin{calc}
  L = R
  \step{ Perform a number of steps ... }
  ...
  \step{ ... to transform L to R }
  R = R
  \step{ Definition of identity }
  \true
  \end{calc}
  Since the right side of the equation â€œ= Râ€ is not altered, we can
  /abbreviate/ such calculations, by omitting the final step and avoiding
  the repetitious â€œ= Râ€ on each line, as follows.
  \begin{calc}
  L
  \step{ Perform a number of steps ... }
  ...
  \step{ ... to transform L to R }
  R
  \end{calc}

  That is, (3.4) gives us a new proof method ---which is always a bonus result
  from a theorem.

  #+begin_box (3.6) Simplifiction Proof Method ---Page 45
  To prove $L = R$ is a theorem, transform $L$ to $R$ or $R$ to $L$
  using Leibniz (equals for equals reasoning).

  Usually, you start with the more â€˜complicatedâ€™ (more structured) side of the
  equation and transform that to the â€˜simplerâ€™ side. The (additional) structure
  then narrows the number of applicable laws and thus guides the proof.
  #+end_box

  #+begin_box (3.34) Rabbit Avoidance ---Page 51
  A â€œrabbit pulled out of hatâ€ is a step in a proof that has little or no
  motivation; e.g., it introduces more structure and it's not clear why that is
  the case ---for instance, replacing $true$ with $p âˆ¨ p â‰¡ p âˆ¨ p$.

  Structure proofs to minimise the number of rabbits pulled out of a hat ---make
  each step seem obvious, based on the structure of the expression and the goal
  of the manipulation.

  E.g., when the driving goal of a proof is to simplify; then there should not
  be any rabbits,
  #+end_box

  Finally, (3.4) gives us the following doc:Metatheorem.

  \[\Law[(3.7)]{Metatheorem}{\text{Any two theorems are equivalent}}\]

  Indeed, if $P$ is a theorem and $Q$ is a theorem, then by (3.4) we have $P â‰¡
  \true$ and $\true = Q$ and so by transitivity of â‰¡, we have $P â‰¡ Q$.

  With true in-hand, one can now define false:
  \[\Law[3.10]{Definition of false}{\false â‰¡ Â¬ true}\]

  Since â€˜â‰¡â€™ = â€˜=â€™ on the Booleans, we can phrase this as $false â‰  true$,
  which is a useful thing to know.

  Moreover, we can then show that a Boolean expression not equal to true is
  equal to false: ~(p â‰¢ true) â‰¡ (p â‰¡ false)~.

** Double Negation Example.
   :PROPERTIES:
   :CUSTOM_ID: Double-Negation-Example
   :END:

 \[\Law[(3.12)]{Double negation}{Â¬ Â¬ p â‰¡ p}\]

 Double negation asserts that negation is its own inverse.

 Double negation is used in English occasionally.
 For example, one might say â€œThat was not done unintentionallyâ€
 instead of â€œThat was done intentionallyâ€.

** A remark on Axiom (3.9) â€œCommutativity of Â¬ with â‰¡â€: Â¬ (p â‰¡ q) â‰¡ (Â¬ p â‰¡ q)
   :PROPERTIES:
   :CUSTOM_ID: A-remark-on-Axiom-3-9-Commutativity-of-with-p-q-p-q
   :END:

   \[\Law[(3.9)]{Commutativity of Â¬ with â‰¡}{ Â¬ (p â‰¡ q) â‰¡ (Â¬ p â‰¡ q)}\]

   The left side says that /p/ and /q/ are different; but there are only two Boolean
   values and so for /p/ and /q/ to be different, one must be the â€˜flipâ€™ (negation)
   of the other.

   Moreover, this rule says â€œdiffers fromâ€ (â‰ ) on the Booleans can be expressed
   directly in terms of equality (=) instead of a negation of an equality
   ---which is the case in general.

   The following laws uniquely define negation.

   \[\Law[(3.8)]{Axiom, Definition of false}{\false â‰¡ Â¬ \true}\]
   \[\Law[(3.9)]{Axiom, Commutativity of Â¬ with â‰¡}{Â¬ (p â‰¡ q) â‰¡ Â¬ p â‰¡ q}\]

   Indeed, suppose $f : ğ”¹ â†’ ğ”¹$ also satisfies these laws, then we can
   show $f(p) â‰¡ Â¬ p$ ---in particular, $f(\true) = \false$ and $f(\false) = \true$.

   That is, of the 4 possibly unary functions on the Booleans, only negation
   satisfies these two properties.

** TODO COMMENT 3.21 on p47; 3.22&23 and preceeding paragraph on p48; and footnote 10 on p58
   :PROPERTIES:
   :CUSTOM_ID: COMMENT-3-21-on-p47-3-22-23-and-preceeding-paragraph-on-p48-and-footnote-10-on-p58
   :END:

** Alternative definitions of â‰¡ and â‰¢
   :PROPERTIES:
   :CUSTOM_ID: Alternative-definitions-of-and
   :END:

  The following theorems are sometimes used to define â‰¡ and â‰¢.
  The first theorem indicates that $p â‰¡ q$ holds exactly when $p$
  and $q$ are both /true/ or both /false/. The second theorem indicates that
  $p â‰¢ q$ holds exactly when one of them is /true/ and the other is /false/.

  \[\Law[(3.52)]{Definition of â‰¡}{p â‰¡ q â‰¡ (p âˆ§ q) âˆ¨ (Â¬ p âˆ§ Â¬ q)}\]
  \[\Law[(3.53)]{Exclusive or}{p â‰¢ q â‰¡ (Â¬ p âˆ§ q) âˆ¨ (p âˆ§ Â¬ q)}\]

# (3.53b) ?
#+begin_src math
Theorem â€œxorâ€ â€œâ‰¢ is one or the other, but not bothâ€: (p â‰¢ q) â‰¡ (p âˆ¨ q) âˆ§ Â¬ (p âˆ§ q)
Proof:
    (p âˆ¨ q) âˆ§ Â¬ (p âˆ§ q)
  =âŸ¨ â€œDe Morganâ€ âŸ©
    (p âˆ¨ q) âˆ§ (Â¬ p âˆ¨ Â¬ q)
  =âŸ¨ â€œDistributivity of âˆ§ over âˆ¨â€ âŸ©
     ((p âˆ¨ q) âˆ§ Â¬ p) âˆ¨ ((p âˆ¨ q) âˆ§ Â¬ q)
  =âŸ¨ â€œAbsorptionâ€ âŸ©
     (Â¬ p âˆ§ q) âˆ¨ (Â¬ q âˆ§ p)
  =âŸ¨ â€œAlternative definition of â‰¢â€ âŸ©
     p â‰¢ q
#+end_src

  In most propositional calculi equivalence is the last operator to be defined
  and is defined as â€œmutual implicationâ€.  Thus, (3.80) below typically is made
  an axiom.  We down-play implication in our calculus because, as an unsymmetric
  operator (by 3.72 and 3.73), it is harder to manipulate. Sometimes
  (3.80) would be read as â€œ(strong) antisymmetry of â‡’â€.

  \[\Law[(3.72)]{Right Zero of â‡’}{p â‡’ \true â‰¡ \true}\]
  \[\Law[(3.73)]{Left Identity of â‡’}{\true â‡’ p â‰¡ p}\]

  \[\Law[((3.80))]{(Mutual Implication)}{(p â‡’ q) âˆ§ (q â‡’ p) â‰¡ p â‰¡ q}\]

** Contextual Rules ---Leibniz and Substitution
   :PROPERTIES:
   :CUSTOM_ID: Contextual-Rules-Leibniz-and-Substitution
   :END:

   With the implication operator available, the Leibniz inference
   rule can be re-cast as an axiom.

   \[\Law{Abbreviation}{E^z_F \;=\; E[z â‰” F]}\]
   \[\Law[(3.83)]{Axiom, Leibniz}{(e = f) â‡’ E_e^z = E_f^z}\]
   \[\Law[(3.84a)]{Substitution}{(e = f) âˆ§ E_e^z \quadâ‰¡\quad (e = f) âˆ§ E_f^z}\]
   \[\Law[(4.84c)]{Substitution}{q âˆ§ (e = f) â‡’ E_e^z \quadâ‰¡\quad q âˆ§ (e = f) â‡’ E_f^z}\]

   Replacing variables by Boolean constants.

   \[\Law[(3.85)]{Replace by true}{q âˆ§ p â‡’ E^z_p \quadâ‰¡\quad q âˆ§ p â‡’ E^z_\true}\]
   \[\Law[(3.86)]{Replace by false}{E^z_p â‡’ p âˆ¨ q \quadâ‰¡\quad E^z_\false â‡’ p âˆ¨ q}\]
   \[\Law[(3.87)]{Replace by true}{p âˆ§ E_p^z \quadâ‰¡\quad p âˆ§ E_\true^z}\]
   \[\Law[(3.88)]{Replace by false}{p âˆ¨ E_p^z \quadâ‰¡\quad p âˆ¨ E_\false^z}\]
   \[\Law[(3.89)]{Shannon, Case analysis}{E_p^z \quadâ‰¡\quad (p âˆ§ E_\true^z) âˆ¨ (Â¬  p âˆ§ E_\false^z)}\]

** Disjunction
   :PROPERTIES:
   :CUSTOM_ID: Disjunction
   :END:

   The axioms (3.24)-(3.28) uniquely determine disjunction.

   That is, of the 16 possibly binary functions on the Booleans, only
   disjunction satisfies these properties.

* COMMENT Induction ---Chapter 12
  :PROPERTIES:
  :CUSTOM_ID: Induction
  :END:

\[Law[(12.4)]{Induction}{(âˆ€ n : â„• â€¢ (âˆ€ m : â„• â€¢ m < n â‡’ P\, m) â‡’ P\, n) \quadâ‰¡\quad (âˆ€ n : â„• â€¢ P\, n)}\]

This says that to prove $P\, n$ for all natural numbers $n$ (the right side), we
prove (the left side) that for any $n$, if $P\, 0, P\, 1, â€¦, P\, (n - 1)$ holds,
then so does $P\, n$. This is because, in principle ---given enough time and
space--- we can prove $P\, N$ for any given $N$ by proving, in turn, $P\, 0, P\,
1, â€¦,$ and finally $P\, N$:
+ When $n = 0$, the left side simplifies to $P\, 0$, and so we conclude $P\, 0$.
+ From $P\, 0$ and $P\,0 â‡’ P\, 1$ (the left side with $n â‰” 1$), by Modus ponens
  (3.77)
  we conclude $P\, 1$.
+ ...
+ From $P\, 0 âˆ§ â‹¯ âˆ§ P\, (N - 1)$ and $P\, 0 âˆ§ â‹¯ âˆ§ P\, (N - 1) â‡’ P\, N$ (the left
  side with $n â‰” N$) by Modus ponens (3.77) we conclude $P\, N$.


#+begin_box Exposing the Hypothesis Heuristic
In the proof of $P\, n$, one aims to â€˜split off a termâ€™
so as to expose $P\, i$, for $i < n$, and use that hypothesis
to make progress in the proof of $P\, n$.
#+end_box

* Â Program Correctness
   :PROPERTIES:
   :CUSTOM_ID: The-Assignment-Statement
   :END:

   #+begin_latex-definitions
   \def\If#1{\,\;â‡\!\![#1]\;\;\,} % goal â‡[command] provisos
   \def\Then#1{\,\;â‡’\!\![#1]\;\;\,} % provisos â‡’[command] goal
   #+end_latex-definitions

    # (load-file "~/Desktop/power-blocks.el")

    Textual substitution is inextricably intertwined with equality.

    Likewise, assignment statements in programming can be reasoned
      about using textual substitution.

      The coincidence of notations is deliberate.

      Rather than understanding how a program is /executed/,
      we can also understand a program in terms of /syntactic substitution/.

   #+begin_center
   Focus is on goal-directed and calculational construction of algorithms as
   opposed to the traditional guess-and-verify methodology.
   #+end_center

** From Comments to Hoare Triples
   :PROPERTIES:
   :CUSTOM_ID: More
   :END:

 #+begin_box Commenting Your Code :background-color cyan
 When writing computer programs, it is very good practice to comment them
 thoroughly in order to explain what is going on.  It helps the programmer to
 avoid errors by enforcing greater clarity, and it helps others who need to
 modify the program at a later date (including the one who wrote the program in
 the first place!)

 It is a good discipline, for example, to comment every variable declaration with
 a statement about the variable's function in the program.  This has the
 additional benefit of disciplining the programmer to use distinct variables for
 distinct functions, rather than overloading a variable with several different
 functions.

 Good comments supplement  the program text with explanations of the program's
 function and why the code that is used achieves that function.
 #+end_box

   The comments we write state formal properties of the program variables at a
   particular point in the execution of the program.

   Sometimes comments are written within braces, as in ~{ 0 < i } i := i - 1 { 0 â‰¤
   i }~ which documents that before the assignment we know $0 < i$ and after the
   assignment we know $0 â‰¤ i$. Such /machine checkable comments/ are also known as
   *assertions* and many languages have src_plantuml[:exports code]{assert }
   commands ---e.g., [[https://alhassy.github.io/PythonCheatSheet/CheatSheet.pdf][Python]] has them.

   An assertion is a claim that a particular property about the program variables
   is true at that point in the program's execution.

   An expression of the form ~{ R } C { G }~, where $R, G$ are properties of the program
   variables and $C$ is a program command, is called a <<<Hoare triple>>>.

   Such expressions are *commented programs*, but they are also *Boolean
     expressions*: Triples ~{ R } C { G }~ denote the claim that, if the program
     variables satisfy property $R$ before execution of command =C=, then execution
     of =C= is guaranteed to terminate and, afterwards, the program variables will
     satisfy property $G$. ( /Specifications are theorems!/ )

 What can be said of the following *very interesting* triples?
 | ~{ true  } i := 1 { i = 1 }~ |
 | ~{ i = 1 } i := 0 { true  }~ |
 | ~{ false } i := 1 { i = 0 }~ |
 #+begin_details Solutions

 The solutions below will generalise the exercises.

 #+begin_box No assumptions needed!

 | ~{ true } x := K { x = K }~  for a /constant/ $K$ |

 A $\true$ precondition describes all states of the program variables; the claim
 is thus that /whatever the initial value of the program variables/ (in particular
 the variable /x/) after execution of the assignment ~x := K~ the property $x = K$
 will hold ---you can /prove/ this using the assignment rule below.
 #+end_box

 #+begin_box Termination!

 | ~{ R } C { true }~        |

 The triple says nothing about the command because Since all states satisfy
 postcondition $\true$, the triple communicates that ~C~ /terminates/ ---see the
 informal definition of the Hoare triples.

 Compare this with the right-zero property of implication: $p â‡’ \true$; â€œtrue is
 always true, no matter what you have in handâ€. Also similar to $n â‰¤ +âˆ$.
 #+end_box

 #+begin_box Impossible Assumptions ---â€œThe Law of the Excluded Miracleâ€

 | ~{ false } C { G }~          |

 The claim is vacuously true because the assumption is that the execution of the
 assignment is begun in a state satisfying $\false$, which can never be the case.

 Compare this property with ex falso quodlibet: $\false â‡’ p$; â€œstarting from
 false, anything can be derivedâ€. Also similar to $-âˆ â‰¤ n$.
 #+end_box

 #+end_details

 Consider the swap program ~x, y := y, x~, it swaps the /values/ of the /program
 variables/. To formalise such a claim, we introduce variables $X$ and $Y$
 to denote the /values/ of the program variables ~x, y~. Then, we can
 formalise the claim as
 \[
 x = X \;âˆ§\; y = Y \;\Then{x,y := y, x}\; x = Y \;âˆ§\; y = Y
 \]
 We refer to variables that never appear in program text
 as _<<<ghost variables>>>_; their function is to relate the final values of the
 program variables to their initial values.

 #+begin_box Definition of Hoare Triples
 In general, an expression ~{ R } C { G }~, with $R, G$ predicates on a collection
 of program variables and ghost variables, means that, /forall possible values of
 the ghost variables,/ if the program variables satisfy property $R$ before
 execution of the command ~C~, execution of ~C~ is guaranteed to terminate, and,
 afterwards, the program variables will satisfy property $G$.
 #+end_box

 So the claim about swapping variables above is that, for all values of $X$ and
 $Y$, if $x = X \;âˆ§\; y = Y$, before executing the simultaneous assignment ~x,
 y := y, x~, then, afterwards, $x = Y \;âˆ§\; y = X$.
** â€˜Dynamic Logicâ€™ Notation
   :PROPERTIES:
   :CUSTOM_ID: Dynamic-Logic-Notation
   :END:
   States may be represented by predicates on variables and so *imperative
   commands are relations on predicates*: /Given two propositions $G, R$ making/
   /use of program variables, we write $R \Then{C} G$ to mean â€œin a state $R$, the
   execution of command $C$ terminates in state $G$â€/.
   ( This is also known as a /Hoare Triple/ and written ={R} C {G}=. )

   For  example,
   | ~v = 5 âˆ§ w = 4 âˆ§ x = 8 Â Â â‡’[ v := v + w ]Â Â  v = 9 âˆ§ w = 4 âˆ§ x = 8~ |
   |-----------------------------------------------------------------|
   | ~x = 0 Â Â â‡’[ x := x + 1 ]Â Â x > 0~                                  |
   | ~x > 5 Â Â â‡’[ x := x + 1]Â Â x > 0~                                   |
   |-----------------------------------------------------------------|
   | ~(x = 5 Â Â â‡’[ x := x + 1 ]Â Â x = 7) Â Â â‰¡Â Â  false~                    |
   |-----------------------------------------------------------------|
   | ~5 â‰  5 Â Â â‡’[ x := 5 ]Â Â  x â‰  5~                                     |

   In practice, one begins with *the goal G* (also known as the
   â€˜postconditionâ€™) and /forms/ a suitable *command C* that ensures $G$ but may
   require some provisos to be given, which are conjuctively known the *required R*
   (also known as the â€˜preconditionâ€™).

   | $R \Then{C} G$       | â‰ˆ | â€œGet goal $G$ using command $C$, by requiring $R$.â€ |

   This is only /reasonable/: We have some desired goal $G$ and we wish to form an
   imperative program =C= whose execution will ensure the goal $G$, but the
   construction of =C= may require some necessary provisos $R$.

** Example Specifications
   :PROPERTIES:
   :CUSTOM_ID: Hi
   :END:

 A /specification is an equation of a certain shape.
 /Programming/ is the activity of solving a specification
 for its unknown. Its unknown is called a /program/.

 One says â€œprogram =C= is specified by precondition $R$ and postcondition $G$â€
 whenever $R \Then{C} G$. One also says this is a /specification of =C=./

 #+begin_box C. A. R. Hoare
 Tony Hoare's 1969 landmark paper /An axiomatic basis for computer programming/
 proposed to define the meaning of programs by how they transform state
 (predicates on the program variables; i.e., stores). It defined $R \,\{C\}\, G$
 to mean /partial correctness/; whereas the modern notations $\{ R\}\, C\, \{G\}$
 and $R \Then{C} G$ denote /total correctness/, which has the additional
 requirement of the /termination/ of $C$.
 #+end_box

 *Programming* is solving the equation $R \Then{C} G$ in the unknown $C$;
 i.e., it is the activity of finding a â€˜recipeâ€™ that satisfies a given
 specification.  Sometimes we may write $R \Then{?} G$ and solve for â€˜?â€™.
 *Programming is a goal-directed activity: From a specification, a program is
 found by examining the /shape/ of its postcondition.*

 The notation $x : E$ is intended to communicate that we are looking at the
 expression $E$ with /unknown/ $x$ ---i.e., $x$ is the variable we are focusing on.
 For instance, is $xÂ² + b Â· x = 0$ a linear equation? Yes, if $b$ is the variable
 and $x$ is considered constant! In such a case, we are speaking of the equation
 $b : xÂ² + b Â· x = 0$. With this convention, the notation
 $R \Then{C} \vec{x} : G$ means that /only/ the names $\vec{x}$ should be
 considered â€˜program variablesâ€™ and all other variables should be treated
 as â€˜fixedâ€™ or â€˜constantâ€™ and so cannot appear in the program command $C$.

 However, this convention only reduces ambiguity about what variables can be
 meddled with; and so ghost variables are still required.  For instance, consider
 the example specification â€œset $z$ to its own absolute valueâ€, it is formalised
 with the help of a ghost variable: $z = Z \Then{?} z = |Z|$.

 #+begin_details Set q and r to the quotient and remainder of integer division of x â‰¥ 0 by y > 0
 \[ x â‰¥ 0 âˆ§ y > 0 \Then{?} q Â· y + r = x âˆ§ 0 â‡ r âˆ§ r < y \]

 The first conjunct states that the quotient times the denominator plus the
 remainder equals the numerator. The last two conjuncts bound the remainder: The
 remainder is at least 0 and is less than the denominator.
 #+end_details

 #+begin_details Set x to the integer square-root of N â‰¥ 0
 The integer square-root of $N$ is the largest integer whose square is at most
 $N$, and so the answer is \[ N â‰¥ 0 \Then{?} xÂ² â‰¤ N âˆ§ N < (x + 1)Â²\]

 Note that the postcondition is equivalent to $x â‰¤ \sqrt{N} < x + 1$.
 #+end_details

 #+begin_details Set x to the largest integer that is a power of 2 and is at most N
 \[N â‰¥ 0 \Then{?} (âˆƒ i : â„• â€¢ x = 2^i) âˆ§ x â‰¤ N âˆ§ N < 2 Â· x\]

 The first conjunct states that $x$ is a power of 2. The second states that $x4
 is at most $N4, while the third states that the next power of 2 exceeds $N$.

 This is /the integer-logarithm base-2/ problem; the postcondition is equivalent to
 $âˆƒ i : â„• \,â€¢\, i = \lg_2 x \,âˆ§\, i â‰¤ \lg_2 N < 1 + i$.
 #+end_details

** What is the definition of $R \Then{C} G$?
   :PROPERTIES:
   :CUSTOM_ID: What-is-the-definition-of-R-C-G
   :END:

   It is defined by the /shape/ of the possible commands =C=.

   For example, the *<<<sequential>>> command* =Câ‚ â® Câ‚‚= is executed by first
   executing =Câ‚= then by executing =Câ‚‚=. As such, to obtain a goal state $G$, we
   may construct a partial program $Câ‚‚$ which in-turn requires an intermediary
   state $I$; then to establish state $I$, we may construct a program =Câ‚= which
   requires a state $R$. This is *divide and conquer*.

   \[
   \Rule[Sequencing]{R \;â‡’[Câ‚]\;\; I \And I \;â‡’[Câ‚‚]\;\; G}{R \;â‡’[Câ‚â®Câ‚‚]\;\; G}
   \]

   Notice that this is similar to $\Rule{x â‰¤ y \And y â‰¤ z}{x â‰¤ z}$ but unlike
   inclusions â€˜â‰¤â€™ which are either true or false, the relationships â€˜$â‡’[C]$â€™ are
   /parameterised/ by commands $C$ and so it's important to *remember* which
   commands /witnessed/ the relationship. To see the similarity even closely, let
   us write $m â‰¤â‚–\, n \;â‰¡\; m + k = n$, so that we have â€œa witness to the
   inclusionâ€; then the â‰¤-transitivity becomes suspiciously similar to the
   sequencing rule â€¦  \[ \Rule[Addition]{x â‰¤_a y \And y â‰¤_b z}{x â‰¤_{a+b} z} \]
   The reason that we usually use â€˜â‰¤â€™ instead of â€˜â‰¤â‚–â€™, because if $x â‰¤â‚–\, y$
   then there can only be one such $k$, namely $k = y - x$, and so the simpler
   â€˜â‰¤â€™ simply marks whether such a (unique) $k$ exists or not. In contrast,
   infinitely many programs =C= can be used to establish relationships $R â‡’[C] G$
   and this is what makes programming interesting!

** We define assignment using textual substitution
   :PROPERTIES:
   :CUSTOM_ID: We-define-assignment-using-textual-substitution
   :END:

   The <<<assignment statement>>>
   ~x := E~ evaluates expression =E= (which we assume â€œnever crashesâ€)
   and stores the result in variable =x=.
   The statement is read â€œ ~x~ becomes ~E~ â€.

   When your goal $G$ mentions a variable $x$,
   then your goal state $G$ could be established
   by using the command ~x := E~ for some choice of expression $E$
   provided you know that $G[x â‰” E]$ is true to begin with.

   For example, suppose we want to get the goal $i < 10$ /after/ executing ~i := 2
   Â· i~. Then, this could only happen if /beforehand/ we had $(i < 10)[i â‰” 2 Â· i]$;
   i.e., $2 Â· i < 10$; i.e., $i < 5$. Hence, starting in a state in which $i <
   5$, the execution of ~i := 2 Â· i~ is guaranteed to terminate in a state with $i
   < 10$.

   This is summarised as follows.
   \[
   \mathsf{Definition\, of\, Assignment:}\quad G[x:= E] \Then{x:= E} G
   \]

   The â€˜backwardsâ€™ nature of this rule ---working backward from the
   postcondition--- is most easily understood via examples of the video lectures
   that start with the goal and work back to the precondition.  See the first 5
   minutes of this [[https://youtu.be/JxRZC2UMJb0][this video lecture]] for a taste of â€˜working backwardsâ€™.  The
   examples below also aim to demonstrate this idea.

   This definition is also used when we allow /multiple assignments/
   ~xâ‚, xâ‚‚, â€¦, xâ‚™ := Eâ‚, Eâ‚‚, â€¦, Eâ‚™~ ---which, for distinct variables $xáµ¢$,
   is executed by first evlauating all the expressions $Eáµ¢$
   to yield values, say, $váµ¢$; then assigning $vâ‚$ to $xâ‚$, â€¦, $vâ‚™$ to $xâ‚™$.
   - Note that all expressions are evaluated before any assignments are
     performed. E.g., ~x, y := y, x~ is a program that *swaps* the values of two
     variables, ~x~ and ~y~:
     | ~x = X âˆ§ y = Y Â Â â‡’[x, y := y, x]Â Â x = Y âˆ§ y = X~ |

   - Python, for example, allows multiple assignments.

** Let's show a calculation!
   :PROPERTIES:
   :CUSTOM_ID: Let's-show-a-calculation
   :END:

   First, since *programming is a goal-directed activity*, let us begin with the
   goal and use that to arrive at a required precondition.
   How? Just as $y â‰¥ x \;â‰¡\; x â‰¤ y$, we define
   \[G \If{C} R \quadâ‰¡\quad R \Then{C} G \]
   Using this â€˜turned aroundâ€™ (/converse/) notation, we may begin with the goal.

   :Swap:
   Here's a common problem: We want to have $x = Y âˆ§ y = X$, how do we achieve this?
   \begin{calc}
   x = Y âˆ§ y = X
   \step[\If{x, y := y, x}]{ Assignment rule }
   (x = Y âˆ§ y = X)[x, y â‰” y, x]
   \step[=\quad\qquad\qquad\;\;]{ Substitution }
    y = Y âˆ§ x = X
   \end{calc}
   :End:

   # LADM exercise 1.11.b
   Let's use the assignment rule to *find a necessary precondition*:
   When will the assignment ~x := x - 1~ ensure the goal state $xÂ² + 2 Â· x = 3$?
   \begin{calc}
     xÂ² + 2 Â· x = 3
   \step[\If{x := x - 1}]{ Assignment rule }
     (xÂ² + 2 Â· x = 3)[x â‰” x - 1]
   \step[=\qquad\qquad\quad]{ Substitution }
     (x - 1)Â² + 2 Â· (x - 1) = 3
   \step[=\qquad\qquad\quad]{ Arithmetic: Perform multiplications }
     (xÂ² - 2 Â· x + 1) + (2 Â· x - 2) = 3
   \step[=\qquad\qquad\quad]{ Arithmetic: Collect like terms }
     xÂ² - 1 = 3
   \step[=\qquad\qquad\quad]{ Arithmetic: Square roots }
     x = Â±2
   \end{calc}
   Hence, if $x$ is positive or negative 2, then the assignment
   ~x := x - 1~ will ensure that $x$ satisfies the predicate state $xÂ² + 2 Â· x =
   3$
   ---incidentally, after the assignment $x$ will have its value being
   positive 1 or negative 3, which are both solutions to the equation
    $xÂ² + 2 Â· x = 3$.

    kbd:Begin_Warning [[color:red][*At the moment, in CalcCheck notebooks, to keep things
   clear, we are using =R â‡’[C] G= and /not (yet)/ using ~G â‡[C] R~.*]] See the first 5
   minutes of this [[https://youtu.be/JxRZC2UMJb0][this video lecture]] for a taste of â€˜working backwardsâ€™.
   kbd:End_Warning

 When $I \;â‡’[C]\;\; I$, one says that â€œstate $I$ is *<<<invariant>>>* for
 command $C$â€ or that â€œ$C$ /maintains/ $I$â€.

** Calculating Assignments
   :PROPERTIES:
   :CUSTOM_ID: Calculating-Assignments
   :END:

   It is /often/ possible to *calculate* an assignment statement that satisfies a
   given precondition-postcondition specification.
   Many examples involve a property that is to be maintained invariant whilst
   progress is made by incrementing (or decrementing) a counter.

   E.g., suppose we want to increment =k= while maintain the value of the sum =j +
   k=. How should we alter =j=? Rather than guess and check, we can /calculate/! Let
   =S= be the value of the sum (whatever it may) and let =ğ’³= be the unknown
   assignment to =j=; then our goal is to â€œsolve for ğ’³â€ in
   \[j + k = S \Then{j,k := ğ’³, k+1} j+k = S\] Let's begin calculating!
   \begin{calc}
       j + k = S
   \step[\If{j, k := ğ’³, k + 1}]{Assignment rule}
       (j + k = S)[j, k â‰” ğ’³, k + 1]
   \step[=\hspace{7em}]{ Substitution }
       ğ’³ + k + 1 = S
   \step[â‡\hspace{7em}]{ Strengthening }
       ğ’³ + k + 1 = S = j + k
   \step[=\hspace{7em}]{ Arithmetic: Substitution and +-cancellation }
       ğ’³ + 1 = j \;âˆ§\; j + k = S
   \step[=\hspace{7em}]{ Subtraction }
       ğ’³ = j - 1 \;âˆ§\; j + k = S
   \end{calc}
   Hence, we have found a solution for the unknown assignment ğ’³,
   and so have /calculated/:
   \[ j + k = S \Then{j,k := j - 1, k+1} j+k = S \]

   Within the above calculation, we have /silently/ used the following law:
   \[
   \Rule[Strengthening]{Râ€² â‡’ R \And R \Then{C} G}{Râ€² \Then{C} G}
   \]

   Here is a more complicated example: Suppose we are computing the square $s$
   of a number $n$ *without* using multiplication or squaring ---only using
   addition!---, and we have just incremented $n$, how should we alter $s$ so as
   to /maintain/ its relationship to $n$? Exercise: Solve for the unknown
   assignment ğ’³ in \[ s = nÂ² \Then{s,n := ğ’³, n + 1} s = nÂ² \]
   # Solution: ğ’³ = s + n + n + 1

** Calculating expressions in assignments :ignore:
   :PROPERTIES:
   :CUSTOM_ID: Calculating-expressions-in-assignments
   :END:

 + Exercise: Solve for ğ’³ in $\true \Then{x := ğ’³} x = 4$.
   - The answer is â€˜obviousâ€™, but actually do the calculation
     to see that unknown expressions in an assignment can be found by
     calculation.
 + Exercise: Solve for ğ’³ in $0 â‰¤ x âˆ§ 0 < y \Then{q, r := ğ’³, x} 0 â‰¤ r âˆ§ q Â· y + r
   = x$
 + Exercise: Solve for ğ’³ in $q = a Â· c âˆ§ w  = cÂ² \Then{a, q := a + c, ğ’³} q = a *
   c$

 #+begin_box  Heuristic
 To determine an unknown expression in an assignment, calculate.
 #+end_box
** The Shapes of Programs
   :PROPERTIES:
   :CUSTOM_ID: The-Shapes-of-Programs
   :END:

 The activity of programming is the activity of solving a specification for its
 unknown, where this unknown is called a /program/. Programs are
 formulae of a certain shape.

 The simplest program is called =skip=, then we may â€˜sequentially composeâ€™ two
 programs =Câ‚= and =Câ‚‚= to obtain the program =Câ‚â® Câ‚‚=, and finally we have the
 /(multiple) assignment/ program ~x := E~ consisting of a list of distinct variables
 =x= and a corresponding list of expressions =E=. There are other shapes of programs
 and we will get to those in due time.

 #+begin_src math
Program ::=  skip
         |   Câ‚ â® Câ‚‚
         |   x := E
 #+end_src

 :Hide:
 Thus far we have defined $R \Then{C} G$ informally, we now turn to defining
 it /algebraically/ thereby placing the notion of program correctness firmly
 within predicate calculus: We shall have a formal method to calculate
 whether a program satisfies its specification.
 :End:

 #+begin_box Program Correctness Laws
 $$\begin{align*}
 \mathsf{Axiom,\; The\; Law\; of\; the\; Exluded\; Miracle} & & R \Then{C} \false \quadâ‰¡\quad Â¬ R \\
 \mathsf{Axiom,\; Conjunctivity} && R \Then{C} (Gâ‚ âˆ§ Gâ‚‚) \quadâ‰¡\quad \big(R \Then{C} Gâ‚\big) âˆ§ \big(R \Then{C} Gâ‚‚\big) \\
 \mathsf{Axiom,\; Skip\; Rule} && R \Then{\mathsf{skip}} G \quadâ‰¡\quad R â‡’ G \\
 \mathsf{Axiom,\; Sequence\; Rule} && R \Then{Câ‚} I \Then{Câ‚‚} G \quadâ‡’\quad R \Then{Câ‚â®Câ‚‚} G \\
 \mathsf{Axiom,\; Assignment\; Rule} && R \Then{x := E} G \quadâ‰¡\quad R[x â‰” E] â‡’ G \\ \hdashline
 \mathsf{Postcondition\; Weakening} && \big(R \Then{C} G) âˆ§ (G â‡’ Gâ€²) \quadâ‡’\quad R \Then{C} Gâ€² \\
 \mathsf{Precondition\; Strengthening} && (Râ€² â‡’ R) âˆ§ \big(R \Then{C} G) \quadâ‡’\quad Râ€² \Then{C} G \\  \hdashline
 \mathsf{Conjunction} && (Râ‚ \Then{C} Gâ‚) âˆ§ (Râ‚‚ \Then{C} Gâ‚‚) \quadâ‡’\quad (Râ‚ âˆ§ Râ‚‚) \Then{C} (Gâ‚ âˆ§ Gâ‚‚) \\
 \mathsf{Case\; Analysis} && (Râ‚ \Then{C} Gâ‚) âˆ§ (Râ‚‚ \Then{C} Gâ‚‚) \quadâ‡’\quad (Râ‚ âˆ¨ Râ‚‚) \Then{C} (Gâ‚ âˆ¨ Gâ‚‚) \\
 \end{align*}$$
#+end_box

 The skip rule tells us that the =skip= command is executed by doing nothing: When
 $R â‡’ G$, and we are in state $R$, then we get to state $G$ by doing nothing.

 Note: One then defines the equivalence of programs â€œup to specificationâ€:
 \[C = Câ€² \qquadâ‰¡\qquad \big(R \Then{C} G \quadâ‰¡\quad R \Then{Câ€²} G\big) \text{ for
 all } R, G \]
 Using this, one can show that =skip= is the unit of sequencing,
 and that it can be defined in terms of assignments.
 | ~C â® skip = C = skip â® C~ |
 | ~skip = (x := x)~         |

* Induction
  :PROPERTIES:
  :CUSTOM_ID: Induction
  :END:

How we prove a theorem $P\, n$ ranging over natural numbers $n$?

For instance, suppose the property $P$ is that using only 3 and 5 dollar bills,
any amount of money that is at-least 8 dollars can be formed.

Since there are an infinite number of natural numbers, it is not possibly to
verify $P\, n$ is true by /evaluating/ $P\, n$ at each natural number $n$.

#+begin_box Knocking over dominos is induction
The natural numbers are like an infinite number of dominoes ---i.e., standing
tiles one after the other, in any arrangement. Can all dominoes be knocked over?
That is, if we construe $P\, n$ to mean â€œthe /n/-th domino can be knocked overâ€,
then the question is â€œis $âˆ€ n â€¢ P\, n$ trueâ€. Then, clearly if we can knock over
the first domino, $P\, 0$, and if when a domino is knocked over then it also
knocks over the next domino, $P\, n â‡’ P\, (n + 1)$, then â€˜clearlyâ€™ all dominoes
will be knocked over. This â€˜basic observationâ€™ is known as /induction/.
#+end_box

#+begin_box Climbing a ladder is induction
The natural numbers are like an infinite ladder ascending to heaven.  Can we
reach every step, rung, on the ladder?  That is, if we construe $P\, n$ to mean
â€œthe /n/-th rung is reachableâ€, then the question is â€œis $âˆ€ n â€¢ P\, n$
trueâ€. Then, clearly if we can reach the first rung, $P\, 0$, and whenever we
climb to a rung then we can reach up and grab the next rung, $P\, n â‡’ P\, (n +
1)$, then â€˜clearlyâ€™ all rungs of the ladder can be reached. This â€˜basic
observationâ€™ is known as /induction/.
#+end_box

#+begin_box Constant functions are induction
A predicate $P : â„• â†’ ğ”¹$ is a function. When is such a function constantly the
value $\true$? That is, when is $âˆ€ n â€¢ P\, n = \true$?  Clearly, if $P$ starts
off being $\true$ ---i.e., /P 0/--- and it preserves truth at every step ---i.e.,
/P n â‡’ P (n + 1)/--- then /P n/ will be true for any choice of $n$.

That is, if we consider $(â„•, â‰¤)$ and $(ğ”¹, â‡’)$ as ordered sets and $P$ starts at
the â€˜topâ€™ of ğ”¹ ---i.e., /P 0 = true/--- and it is ascending ---i.e., /P n â‡’ P (n +
1)/--- and so â€˜never goes downâ€™, then clearly it must stay constantly at the top
value of ğ”¹. This â€˜basic observationâ€™ is known as /induction/.
#+end_box

For the money problem, we need to start somewhere else besides 0.

#+begin_box Principle of (â€œWeakâ€) Mathematical Induction
To show that a property $P$ is true for all natural numbers starting with some
number $n_0$, show the following two properties:
+ Base case :: Show that $P\, nâ‚€$ is true.
+ Inductive Step :: Show that whenever (the *inductive hypothesis*) $n$ is a
  natural number that such that $n â‰¥ nâ‚€$ and $P\, n$ is true, then $P\, (n + 1)$
  is also true.
#+end_box
For the money problem, we need to be able to use the fact that to prove $P\,
(n + 1)$ we must have already proven $P$ for all smaller values.

#+begin_box Principle of (â€œStrongâ€) Mathematical Induction
To show that a property $P$ is true for all natural numbers starting with some
number $n_0$, show the following two properties:
+ Base case :: Show that $P\, nâ‚€$ is true.
+ Inductive Step :: Show that whenever (the *inductive hypothesis*) $n$ is a
  natural number that such that $n â‰¥ nâ‚€$ and $P\, n_0, P\, (n_0 + 1), P\, (n_0 +
  2), â€¦, P\, n$ are true, then $P\, (n + 1)$ is also true.
#+end_box

These â€˜strengthâ€™ of these principles refers to the strength of the inductive
hypothesis. The principles are provably equivalent.

# (It is also a way to say that â„• has non-empty meets.)
#+begin_box The Least Number Principle ---Another way to see induction
Every non-empty subset of the natural numbers must have a least element,
â€˜obviouslyâ€™. This is (strong) induction.
# Possibly infinite!

#+begin_details Induction â‡’ LNP
\begin{calc}
(âˆƒ x â€¢ x âˆˆ S)
\step{ Double negation }
Â¬ Â¬ (âˆƒ x â€¢ x âˆˆ S)
\step{ De Morgan }
Â¬ (âˆ€ x â€¢ x âˆ‰ S)
\step{ Strong induction }
Â¬ (âˆ€ n âˆ£ (âˆ€ m : â„• âˆ£ m < n â€¢ m âˆ‰ S) â€¢ n âˆ‰ S)
\step{ De Morgan and double negation }
(âˆƒ n âˆ£ (âˆ€ m : â„• âˆ£ m < n â€¢ m âˆ‰ S) â€¢ n âˆˆ S)
\step{ Definition of â€˜leastâ€™ }
\text{S has a least element}
\end{calc}

# neato observation
#
# 0 âˆˆ S âˆ¨ (âˆƒ n âˆ£ (âˆ€ m : â„• âˆ£ m < n â€¢ m âˆ‰ S) â€¢ n âˆˆ S))
# \stepmany{ \line{Since 0 is the least number, if 0 âˆˆ S then S would have a least
# element.}
# \line{The right disjunct says n âˆˆ S is a least element.}
# \line{(â‡’) In both cases, S has a least element.}
# \line{Conversely, (â‡), if S has a least element,}
# \line{then either it is 0 or the right disjunct is satisfied. }
# }
# \text{S has a least element}
#
#+end_details

#+begin_details LNP â‡’ Induction
\begin{calc}
  (âˆ€ n â€¢ P\, n)
\step{ Double negation }
  Â¬ Â¬ (âˆ€ n â€¢ P\, n)
\step{ De Morgan }
  Â¬ (âˆƒ n â€¢ Â¬ P\, n)
\step{ Negation applied to both sides of LNP }
Â¬ (\text{there is a least n with } Â¬ P\, n)
\step{ Formalise â€˜leastâ€™ }
Â¬ (âˆƒ n âˆ£ (âˆ€ m : â„• âˆ£ m < n â€¢ Â¬ (Â¬ P\, n)) â€¢ Â¬ P\, n)
\step{ De Morgan and double negation }
(âˆ€ n âˆ£ (âˆ€ m : â„• âˆ£ m < n â€¢ P\, n) â€¢ P\, n)
\end{calc}
#+end_details

*Application of LNP to showing that algorithms terminate*:
In particular, every decreasing non-negative sequence of integers
$râ‚€ > râ‚ > râ‚‚ > â‹¯$ must terminate.
#+end_box


:Hide:
The money problem can now be proven by showing
$P\,8, P\, 9\, P\, 10\, P\, 11, P\, 12$ are all true,
then showing $P\, n$ for $n â‰¥ 12$ is true by using the inductive hypothesis
on $n - 3 â‰¥ 12 - 3 = 8$ and so $n = (n - 3) + 3$ and so $n$ can be formed
using 3s and 5s.

#+begin_details Exercises
1. the sum of the first $n$ natural numbers is $n Â· (n + 1) / 2$
2. using only 3 and 5 dollar bills, any amount of money that is at-least 8
   can be formed
3. $7^n + 5$ is divisible by 3
#+end_details
:End:


* Number Theory
  :PROPERTIES:
  :CUSTOM_ID: Number-Theory
  :END:

   #+begin_latex-definitions
\newcommand{Law}[3][]{ #1\;\;\textbf{#2:}\quad #3 }
\def\abs{\mathsf{abs}\,}
   #+end_latex-definitions

  Number Theory is concerned with the properties of whole numbers, such as 0,
  42, and 1927 rather than fractional numbers such as 0.3, Ï€, or 1/3.

  Division is one of the most important concepts of number theory.  It is a
  partial order on the naturals and its infimum, meet, is formed constructively
  using Euclid's Greatest Common Divisor algorithm.

  # Algorithms may provide constructive solutions to problems but they can also be
  # used to /reason/ about the resulting constructions.

  --------------------------------------------------------------------------------

** COMMENT Integer Division
   :PROPERTIES:
   :CUSTOM_ID: Integer-Division
   :END:

  The integer division of $P$ by $Q$, denoted $P Ã· Q$, is specified by
  the following Galois connection:
  \[ k Ã— Q â‰¤ P \quadâ‰¡\quad k â‰¤ P Ã· Q \]

  Here, $Q$ is a natural number (otherwise, the inclusion might need to change)
  and $P$ is any integer.

  Replacing $k$ by $P Ã· Q$ yields,
  \[ (P Ã· Q) Ã— Q â‰¤ P \]

  Replacing $k$ by 0 yields,
  \[ 0 â‰¤ P \quadâ‰¡\quad 0 â‰¤ P Ã· Q \]

  Using the law of indirect equality, one can show
  \[ (a Ã· b) Ã· c \;=\; a Ã· (c Ã— b) \]

** COMMENT Deriving an imperative algorithm
   :PROPERTIES:
   :CUSTOM_ID: Deriving-an-imperative-algorithm
   :END:

  The characterisation of integer division
  could also serve as a specification for an algorithm
  that actually computes $x = P Ã· Q$.

  \[ k Ã— Q â‰¤ P \quadâ‰¡\quad k â‰¤ x \]

  If a solution $x$ exists to this equation, then it is the largest integer with
  $x Ã— Q â‰¤ P$ (just take $k â‰” x$) and so is unique.
  Moreover, taking $k â‰” x + 1$ yields $Â¬ (x + 1) Ã— Q â‰¤ P$. Hence, we have the
  problem:
  \[ ? \Then{?} x Ã— Q â‰¤ P \;âˆ§\; Â¬ (x + 1) Ã— Q â‰¤ P\]

  Since the first conjunct is easily truthified (by $x â‰” 0$ and additionally
  requiring $0 â‰¤ P$), we take it to be the invariant and take the negation of
  the second conjunct as the loop guard, thereby obtaining.

  #+begin_latex-definitions
\def\While#1{ \mathsf{while}\,#1\,\textbf{:}\, }
  #+end_latex-definitions

  \[ 0 â‰¤ P \Then{x := 0â® \While{(x + 1) Ã— Q â‰¤ P} ?} x Ã— Q â‰¤ P \;âˆ§\; Â¬ (x + 1) Ã— Q â‰¤ P\]

  Now ~?~ could only be an assignment to ~x~, and so we calculate such an assignment
  /with the aim/ of preserving the invariant, wherein we know the loop guard also
  holds.  That is, we want to solve for $A$ in the following equation.

  \[ (x + 1) Ã— Q â‰¤ P \;âˆ§\; x Ã— Q â‰¤ P \Then{x := A} x Ã— Q â‰¤ P \]

  By the assignment rule, it is clear that $A = x + 1$ works.

  Hence, we have
 \[ 0 â‰¤ P \Then{x := 0â® \While{(x + 1) Ã— Q â‰¤ P} x := x + 1} x Ã— Q â‰¤ P \;âˆ§\; Â¬ (x + 1) Ã— Q â‰¤ P\]

 To prove that the algorithm terminates, we have to define a /bound function/,
 which is a natural-number-valued function of the program variables that
 measures the size of the problem to be solved.  A guarantee that the value of
 such a bound function is always decreased at each iteration is a guarantee that
 the number of times the loop body is executed is at most the initial value of
 the bound function.

 It seems we can take $P - x$ to be the bound function.
 However, for this function to be bounded below, we require
 $0 < Q$.

 In summary,
 \[ 0 â‰¤ P \;âˆ§\; 0 < Q \Then{x := 0â® \While{(x + 1) Ã— Q â‰¤ P} x := x + 1} x Ã— Q â‰¤ P \;âˆ§\; Â¬ (x + 1) Ã— Q â‰¤ P\]

 Notice that additional assumptions on $P$ and $Q$ emerged in the process of
 constructing a correct program.

 *Improvement:* As it stands, the algorithm computes $x + 1$ twice, let us /aim to
 simplify the loop guard, possibly by introducing a new variable/.  Since
 $(x + 1) Ã— Q â‰¤ P \quadâ‰¡\quad Q â‰¤ P - x Ã— Q$ we may introduce a variable $r$
 whose purpose is to maintain the value $P - x Ã— Q$; it's purpose is guaranteed
 if we make it part of the invariant. This results in:
 \[ 0 â‰¤ P \;âˆ§\; 0 < Q \Then{x, r := 0, Pâ® \While{(x + 1) Ã— Q â‰¤ P} x, r := x + 1,
 B} x Ã— Q â‰¤
 P \;âˆ§\; Â¬ (x + 1) Ã— Q â‰¤ P \;âˆ§\; r = P - x Ã— Q \]

 To satisfy the invariant, $x Ã— Q â‰¤ P \;âˆ§\; r = P - x Ã— Q$, we are forced to
 initially set $r$ to $P$. Then we update $r$ via the yet unkwon $B$
 so that the invariant is preserved:
 \[ (x + 1) Ã— Q â‰¤ P \;âˆ§\; x Ã— Q â‰¤ P \;âˆ§\; r = P - x Ã— Q \Then{x, r := x + 1, B}
 x Ã— Q â‰¤ P \;âˆ§\; r = P - x Ã— Q \]

 The assignment rule quickly shows that $B = r - Q$.

 In summary, \[ 0 â‰¤ P \;âˆ§\; 0 < Q \Then{x, r := 0, Pâ® \While{(x + 1) Ã— Q â‰¤ P} x,
 r := x + 1, r - Q} x Ã— Q â‰¤ P \;âˆ§\; Â¬ (x + 1) Ã— Q â‰¤ P \;âˆ§\; r = P - x Ã— Q \]

** COMMENT Deriving a recursive algorithm
   :PROPERTIES:
   :CUSTOM_ID: Deriving-a-recursive-algorithm
   :END:

   The characterisation of integer division as a Galois connection is so
   versatile that it can also be used directly to obtain a recursive algorithm
   for the definition of the new operation.

   The first step is to express the new operation in terms
   of itself but with an argument reduced.

   \begin{calc}
   k â‰¤ P Ã· Q
   \step{ definition of Ã·, using proviso $Q > 0$ }
   k Ã— Q â‰¤ P
   \step{ cancellation; aiming to reduce an argument }
   k Ã— Q - Q â‰¤ P - Q
   \step{ distributivity }
   (k - 1) Ã— Q â‰¤ P - Q
   \step{ definition of Ã·, using proviso $P - Q > 0$; i.e., $P â‰¥ Q > 0$ }
    k - 1 â‰¤ (P - Q) Ã· Q
   \step{ arithmetic }
    k â‰¤ (P - Q) Ã· Q + 1
   \end{calc}

   Hence, by indirect equality, we have shown
   \[ P Ã· Q \;=\; (P - Q) Ã· Q + 1  \quadâ‡\quad  Q â‰¤ P \]

   We have found a reduction in an argument, but it is conditional.  So let us
   consider the negation of this condition so as to cover all possible cases.
   Supposing, $P < Q$, we calculate:
   \begin{calc}
   k â‰¤ P Ã· Q
   \step{ definition of Ã· }
   k Ã— Q â‰¤ P
   \step{ transitivity using assumption }
   k Ã— Q < Q
   \step{ cancellation using $0 < Q$ }
   k < 1
   \step{ integers }
   k â‰¤ 0
   \end{calc}

  Hence, by indirect equality, we have shown
  \[ P Ã· Q \;=\; 0  \quadâ‡\quad  P < Q \]

  Putting these two cases together, we have the following
  recursive definition for natural $P$ and positive $Q$:
  #+begin_src math
P Ã· Q  |  P < Q  =  0
P Ã· Q  |  Q â‰¤ P  =  (P - Q) Ã· Q + 1
  #+end_src

  This function clearly terminates since an argument, $P$, is being reduced at
  each recursive call and stops when $P < Q$.

** Division Algorithm :ignore:
   :PROPERTIES:
   :CUSTOM_ID: Division-Algorithm
   :END:

#+begin_box The Division Algorithm
For any integers $a$ and $b â‰  0$, there are unique
integers $q, r$ with $a = b Â· q + r$ and $0 â‰¤ r < \abs b$.

Here is the algorithm for the positive /b/ case.
#+begin_src math
{ a â‰¥ 0  âˆ§  b > 0 }
q, r := b, 0;
while r â‰¥ m:  q, r := r - a, q + 1
{ a = b Â· q + r  âˆ§  0 â‰¤ r < b }
#+end_src

We write $q, r$ as $a Ã· b$ and /a mod b/.

+ /a Ã· b/ : The quotient of /a/ and /b/ on dividing /a/ by /b/.
+ /a mod b/: The remainder of /a/ and /b/ on dividing /a/ by /b/.
  - When /a mod b = 0/, one says â€œb divides aâ€ or â€œa is a multiple of bâ€.
    Equivalently, /b divides a â‰¡ (âˆƒ c â€¢ a = b Â· c)/.
#+end_box

For instance, if we take $b = 3$, then the theorem says every integer is of the
form $3 Â· q, 3 Â· q + 1$, or $3 Â· q + 2$.

If today is Thursday, what weekday is 90 weekdays from today?  A week has 7 days
and so by the division algorithm we have /90 = 12 Â· 7 + 6/. Since a weekly cycle
(each 7 days) leaves us on the same weekday (today, Thursday), after 12 Â· 7 days
it will again be Thursday, and so the weekday in 90 days is the same as the day
6 days from now; which happens to be Wednesday.

:More:
+ a â•² b â‡’ (c Ã— a) â•² (c Ã— b)  â”€if b is a multiple of a, then cÃ—b is a multiple of cÃ—a
+ a â•² b  âˆ§  a â•² c  â‡’  a â•² (b Ã— x + c Ã— y) â”€if b and c are multiples of a, then
  so is their linear combination.
+ a â•² b  âˆ§  a â•² (b + c)  â‡’  a â•² c â”€if b is a multiple of a, then for b+c to also
  be a multiple, we need c to be a multiple of a; See â€œdivisibility preserves
  semi-linear combinationsâ€.
:End:

** The Division Relation :ignore:
   :PROPERTIES:
   :CUSTOM_ID: The-Division-Relation
   :END:

   The division relation is the relation on integers defined to be the converse
   of the â€œis-a-multiple-ofâ€ relation:
   \[\Law{Definition of Divisibility}{m â•² n  \,\;â‰¡\;\, (âˆƒ k : â„¤ â€¢ n = k Ã— m)}  \]

   Although, the notation $m âˆ£ n$ is more common, the asymmetric â€˜â•²â€™ symbol is
   more suggestive.  In /Concrete Mathematics/, p102, it is pointed out that
   vertical bars are overused and $m â•² n$ (read â€œm under nâ€) gives an impression
   that $m$ is the denominator of the implied ratio ($k = {n \over m}$), and in
   the order sense it implies that, well, $m$ is â€˜underâ€™ (or â€œat mostâ€) $n$.

   Since multiplication distributes over addition, divisibility is (almost)
   preserved by linear combination.

   \[ \Law{Divisibility preserves semi-linear
   combinations}{k â•² x \;âˆ§\; k â•² y \quadâ‰¡\quad k â•² (x + a Ã— y) \;âˆ§\; k â•² y} \]

   That is,

   \[ \Law{Divisibility preserves semi-linear combinations}{
   k â•² y \quadâ‡’\quad \big(k â•² (x + a Ã— y) \;â‰¡\; k â•² x\big)}\]

   Often this theorem is
   used in the /simpler, less generic/, shape â€œsince /k/ divides /x/ and /y/, it also
   divides their sumâ€.

   Divisibility is invariant under additive inverse,
   \[\Law{Divisibility is invariant under additive inverse}{ m â•² n \quadâ‰¡\quad m â•² (-n)} \]

   Since multiplication has a unit and is associative, divisibility is reflexive
   and transitive. That is, divisibility is a /preorder/ on the integers,
   and an /order/ on the naturals; moreover, it has 0 as the greatest element and 1 as the smallest.
   \[ \Law{Reflexivity of divisibility}{ m â•² m} \]
   \[ \Law{Transitivity of divisibility}{m â•² n â•² k \quadâ‡’\quad m â•² k} \]
   \[ \Law{Quasi-antisymmetry of divisibility}{m â•² n \;âˆ§\; n â•² m \quadâ‰¡\quad \abs n = \abs m} \]
   \[ \Law{Top of divisibility}{m â•² 0} \]
   \[ \Law{Bottom of divisibility}{1 â•² m} \]

   Multiplication interacts nicely with divisibility:
   \[ \Law{Divisibility of multiples}{ m â•² (c Ã— m)} \]

  ( Notice that â€˜â•²â€™ is very similar to â€˜â‡’â€™; e.g., $m â•² 0$ is akin to $p â‡’
   \true$, and $m â•² m$ is akin to $p â‡’ p$.  Indeed, the similarly is due to the
   fact that implication â€˜â‡’â€™ is also an order with least and greatest
   elements. )

   --------------------------------------------------------------------------------

   Let's consider â„•atural numbers only â€¦

   The infimum (greatest lower bound) in the division ordering is denoted by â€˜âˆ‡â€™
   and characterised as follows.

   \[\Law{Characterisation of gcd}{ k â•² m \;âˆ§\; k â•² n \quadâ‰¡\quad k â•² (m âˆ‡ n)} \]

   Taking $n â‰” m$ and $n â‰” 0$ gives us
   \[ \Law{Idempotence of gcd}{m âˆ‡ m = m} \]
   \[ \Law{Identity of gcd}{m âˆ‡ 0 = m} \]

   Taking $k â‰” m âˆ‡ n$ yields,
   \[\Law{Gcd is a divisor of its arguments}{(m âˆ‡ n)â•²m \quadâˆ§\quad (m âˆ‡ n)â•²n}\]

   Of-course, the divisibility order can be â€˜definedâ€™ using meet (just take $k â‰”
   m$ and apply antisymmetry of divisibility):
   \[ \Law{Definition of divisibility via gcd}{ m â•² n \quadâ‰¡\quad m âˆ‡ n = m } \]

   Since conjunction is symmetric and doc-associative, so is the meet.
   \[ \Law{Symmetry of gcd}{ m âˆ‡ n = n âˆ‡ m} \]
   \[ \Law{Associative of gcd}{(m âˆ‡ n) âˆ‡ p = m âˆ‡ (n âˆ‡ p)} \]

   Since divisibility is invariant under additive inverse,
   \[ \Law{Gcd is invariant under additive inverse}{ (-m) âˆ‡ n \;=\; m âˆ‡ n} \]

   Since divisibility is almost preserved by linear combinations, meets are
   invariant under them.

   \[ \Law{Gcd is invariant under semi-linear combinations}{(m + a Ã— n) âˆ‡ n \;=\; m âˆ‡ n} \]

   Taking $m â‰” 0$ yields,
   \[ \Law{Gcd of multiples}{ (a Ã— n) âˆ‡ n \;=\; n} \]

   #+begin_box â€œGreatestâ€
   For positive integers, the â€œlargest number wrt to the â•²-ordering that divides
   both of the numbersâ€ is the same as â€œthe largest number wrt to the size
   â‰¤-ordering that divides both of the numbersâ€.  Hence, we may refer to $m âˆ‡ n$
   as the greatest common divisor ---and the word â€˜greatestâ€™ causes no problems
   in ambiguity, when the numbers are positive.

   \[\Law{Inclusion of Positives}{ 0 < m \;âˆ§\; 0 < n \quadâ‡’\quad m â•² n \;â‡’\; m â‰¤
   n} \]
   #+end_box

   Just as â€˜â†“â€™ is used for minima, we use â€˜âˆ‡â€™ for gcd.
   Likewise, â€˜â†‘â€™ for maxima and â€˜Î”â€™ for lcm.

   Multiplication interacts nicely with gcd:
   \[ k â•² (c Ã— m) \;âˆ§\; k â•² (c Ã— n) \quadâ‰¡\quad k â•² (c Ã— (m âˆ‡ n)) \]

   Taking $k â‰” m$ yields, \[ m â•² (c Ã— n) \quadâ‰¡\quad m â•² (c Ã— (m âˆ‡ n))\] \[
   \Law{Relative-Prime Cancellation for divisibility}{\big(m â•² (c Ã— n) \quadâ‰¡\quad m â•²
   c\big) \quadâ‡\quad m âˆ‡ n = 1} \]
   ( The second one above is also known as
   â€œEuclid's Lemmaâ€ when the consequences is presented as an implication instead
   of an equivalence. )

   In particular, we can show that multiplication distributes over gcd:
   For $c : â„•$,
   \[\Law{Multiplication distributes over gcd}{c Ã— (m âˆ‡ n) \quadâ‰¡\quad (c Ã— m) âˆ‡ (c Ã— n)} \]

   Consequently,
   \[ \Law{Relative-Prime Cancellation for gcd}{ (m Ã— p) âˆ‡ n = m âˆ‡ n \quadâ‡\quad p âˆ‡ n = 1} \]
   # Super tiny proof: Start with RHS and replace m by m Ã— 1 ;-)

** COMMENT Deriving Euclid's Algorithm
   :PROPERTIES:
   :CUSTOM_ID: Deriving-Euclid's-Algorithm
   :END:

   The characterisation of the meet could act as a specification for an
   imperative algorithm computing $x = m âˆ‡ n$.

   \[ (âˆ€ k â€¢ k â•² m \;âˆ§\; k â•² n \quadâ‰¡\quad k â•² x) \]

   Unfortunately, these is no explicit conjunction that we could truthify one
   part easily and so take it as invariant, and leave the other conjunct for the
   loop guard. For instance, the only variable we have is $x$ and the only
   reasonable choices for it are $0, 1, m, n$ but none truthify anything.
   So we add â€˜another degree of freedomâ€™: The following is an equivalent
   re-statement, but now we have two variables to work with, $x$ and $y$.

   \[ x = y \;âˆ§\; (âˆ€ k â€¢ k â•² m \;âˆ§\; k â•² n \quadâ‰¡\quad k â•² x \;âˆ§\; k â•² y) \]

   Now an (âˆ€)quantification is not something we want to consider for loop guard
   and so we consider it for invariant. Indeed, taking $x,y â‰” m, n$ immediately
   truthifies the right conjunct, and so the right conjunct is a good candidate
   for invariant. So, we take $x â‰  y$ as loop guard.

   In summary,
   \[ 0 < m \,âˆ§\, 0 < n \Then{x,y â‰” m, n â® \While{x â‰  y} ?} x = y \,âˆ§\, (âˆ€ k â€¢ k â•² m \;âˆ§\; k â•² n \quadâ‰¡\quad
   k â•² x \;âˆ§\; k â•² y) \]

   We now need to alter $x$ and $y$ in such a way as to maintain the invariant
   whilst making progress to a state satisfying $x = y$.  Similar considerations
   as was done for the division algorithm lead us to the following algorithm
   ---where conditionals have been absorbed into the top-level scope: $x â‰  y â‰¡ x
   < y âˆ¨ y < x$.

   #+begin_src math
{ 0 < m  âˆ§ 0 < n }
x, y â‰” m, n â®
{ Invariant: 0 < x  âˆ§  0 < y  âˆ§  (âˆ€ k â€¢ k â•² m  âˆ§  k â•² n  â‰¡  k â•² x  âˆ§  k â•² y)
  Bound function: x + y
}
do y < x âŸ¶ x := x - y
â™  x < y âŸ¶ x := y - x
od
{ Invariant: 0 < x  âˆ§  0 < y  âˆ§  x = y  âˆ§  (âˆ€ k â€¢ k â•² m  âˆ§  k â•² n  â‰¡  k â•² x  âˆ§  k â•² y) }
   #+end_src
* TODO COMMENT Quantification and Predicate Logic
  :PROPERTIES:
  :CUSTOM_ID: Quantification-and-Predicate-Logic
  :END:
* TODO COMMENT Sets
  :PROPERTIES:
  :CUSTOM_ID: Sets
  :END:
* TODO COMMENT Relations and Functions
  :PROPERTIES:
  :CUSTOM_ID: Relations-and-Functions
  :END:
* TODO COMMENT Induction and Sequences
  :PROPERTIES:
  :CUSTOM_ID: Induction-and-Sequences
  :END:
* TODO COMMENT Graphs and Counting
  :PROPERTIES:
  :CUSTOM_ID: Graphs-and-Counting
  :END:
